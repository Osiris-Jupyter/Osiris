"fig, (ax1, ax2) = plot_td_and_fd(t, nr, frequencies, nrtilde);

ax1.set_xlim(xmax=18.0)  # Zoom in a little to see mostly signal
ax1.set_title('')  # Remove the 'Time domain' title, because it gets in the way

def nr_frequency_string(time, position):  # Format the frequency as a string in the plot ticks
    if time>16.45:
        return ''
    return '{0:.1f}'.format(abs(nr_frequency_interpolator([time])[0]))
ax3 = ax1.twiny()  # Make a copy of the first plot, but use a different x axis
ax3.xaxis.set_major_formatter(FuncFormatter(nr_frequency_string))  # Use our tick formatter
ax3.plot([t[0], t[-1]], [0, 0], 'x', alpha=0.0)  # Plot some data occupying same span of time
ax3.set_xlabel('GW Frequency (Hertz)')  # Set the upper x label
ax3.set_xlim(ax1.get_xlim());  # Make sure we have the same zoom"
"fig, (ax1, ax2) = plot_td_and_fd(t, h, frequencies, htilde);"
"# Set up the frequency bins we'll use for our graphic equalizer
log2_sampling_rate = int(np.log2(sampling_rate/2))
frequency_bin_upper_ends = np.logspace(3, log2_sampling_rate,
                                       num=2*(log2_sampling_rate-3)+1, base=2)

# This is some code to set up some interactive sliders for our graphic equalizer
gap_filler = widgets.Label("""", layout=widgets.Layout(flex='1 1 auto'))
separator = widgets.VBox([], border='1px solid #ccc', width='0px')
use_equalizer = widgets.Checkbox(description='Use equalizer', value=True)
labels = widgets.VBox([widgets.Label(""Frequency (Hz)""), gap_filler,
                       use_equalizer, gap_filler,
                       widgets.Label(""Amplitude (dB)"")], width='110px')
sliders = [widgets.FloatSlider(min=-200.0, max=200.0, step=0.5, value=0.0,
                               height='auto', width='45px', padding=6,
                               readout_format='+.1f', orientation='vertical',
                               description=str(int(freq)))
           for freq in frequency_bin_upper_ends]
equalizer = widgets.HBox(children=[labels, separator]+sliders+[separator], height='200px')
display(equalizer)

# We might also want to add specific little ""notch"" filters to get rid of
# noise sources like the 60 Hz hum from the power lines
notch_filters = widgets.VBox([], width='450px')
add_notch_filters = widgets.Button(description=""Add notch filter"")
add_notch_filters.on_click(lambda change: add_notch_filter(notch_filters, gap_filler))
display(add_notch_filters)
display(notch_filters)

# It sometimes gets confusing to see the original data plotted behind the filtered data,
# so add a checkmark to remove it
hide_original_time_domain = widgets.Checkbox(description='Hide raw t. d. data', value=False)

# Add a button to re-make the plot and audio for the current settings
recalculate_button = widgets.Button(description='Recalculate')
def recalculate_button_on_click(change):
    clear_output(wait=True)
    display(equalizer)
    display(add_notch_filters)
    display(notch_filters)
    display(recalculate)
    filter_and_plot(h, t, htilde, sampling_rate, sliders, notch_filters, use_equalizer,
                    frequencies, frequency_bin_upper_ends, hide_original_time_domain)
recalculate_button.on_click(recalculate_button_on_click)
recalculate = widgets.HBox([recalculate_button, hide_original_time_domain])
display(recalculate)

filter_and_plot(h, t, htilde, sampling_rate, sliders, notch_filters, use_equalizer, frequencies,
                frequency_bin_upper_ends, hide_original_time_domain)"
"plt.close()
plt.loglog(frequencies, np.abs(htilde), label='Raw data')
plt.loglog(f_noise, noise_spectral_density, label='Noise estimate')
plt.xlim(1, 0.6*sampling_rate)
plt.xlabel('Frequency (Hertz)')
plt.ylabel('Noise spectrum and strain Fourier transform (seconds)')
plt.grid()
plt.legend();"
"# We simply divide htilde by the noise estimate to ""equalize"" the data in the frequency domain
htilde_equalized = htilde / noise_spectral_density_interpolator(frequencies)

# Now we transform back to the time domain, and smoothly fade on and off to get rid of loud clicks
h_equalized = fade(sampling_rate * np.fft.irfft(htilde_equalized))

# Finally we can plot and listen to the data
plot_td_and_fd(t, h_equalized, frequencies, htilde_equalized, h=h, htilde=1e21*htilde);"
"notch_locations_and_sizes = [
    (35.1, 37.1, 0.25), (35.7, 36., 0.8), (36.6, 36.8, 1.3), (40.9, 41.0, 2), # Gremlins
    (59.9, 60.1, 4), (119.9, 120.1, 3), (179.9, 180.1, 4),  # Line noise
    (299.47, 299.7, 2), (303.2, 303.35, 2), (310., 327., 0.25), (331.7, 332.2, 2), # Violin modes
    (500.5, 503.1, 1.0), (504.7, 504.9, 1.5), (507.0, 508.7, 1.5), # Violin modes
    (991.2, 992.8, 4), (993.7, 997.0, 4), (997.5, 999.5, 4), (1003.7, 1005.4, 4)]  # Harmonics

h_notched = fade(notch_data(h, sampling_rate, notch_locations_and_sizes))
htilde_notched = dt * np.fft.rfft(h_notched)

# Finally we can plot and listen to the notched data
plot_td_and_fd(t, h_notched, frequencies, htilde_notched, htilde=dt*np.fft.rfft(fade(h)));"
"# Now we'll estimate the noise spectrum again
number_of_chunks = 8
points_per_chunk = 2**int(np.log2(len(h_notched)/number_of_chunks))
f_noise, noise_welch = scipy.signal.welch(h_notched, sampling_rate, nperseg=points_per_chunk)
noise_spectral_density = np.sqrt(2 * len(h_notched) * noise_welch / sampling_rate)
noise_spectral_density_interpolator = InterpolatedUnivariateSpline(f_noise,noise_spectral_density)

# And we'll filter the data using this new estimate
htilde_notched_equalized = htilde_notched / noise_spectral_density_interpolator(frequencies)
h_notched_equalized = fade(sampling_rate * np.fft.irfft(htilde_notched_equalized))

plot_td_and_fd(t, h_notched_equalized, frequencies, htilde_notched_equalized, htilde=1e21*htilde);"
"h_filtered = bandpass(h_notched_equalized, sampling_rate, lower_end=35.0, upper_end=265.0)
h_filtered_tilde = dt * np.fft.rfft(h_filtered)

plot_td_and_fd(t, h_filtered, frequencies, h_filtered_tilde, htilde=1e22*htilde);"
"# Load the raw data from LIGO Livingston
with h5py.File('Data/L-L1_LOSC_4_V1-1126259446-32.hdf5') as f:
    l = f['strain/Strain'][:]  # Time of event is 16.4

# Now Fourier transform the raw data
ltilde = dt * np.fft.rfft(l)

# Filter it using our function
l_filtered = filter_signal(l, sampling_rate)

# Now Fourier transform the filtered data
l_filtered_tilde = dt * np.fft.rfft(l_filtered)

# Finally plot and play it
plot_td_and_fd(t, l_filtered, frequencies, l_filtered_tilde, h=l, htilde=1e22*ltilde);"
"plt.close()
plt.plot(t+0.0072, -l_filtered, label='Livingston')
plt.plot(t, h_filtered, label='Hanford')
plt.xlim(16.2, 16.5)
plt.xlabel('Time (seconds)')
plt.ylabel('Detector strain $h$ (dimensionless)')
plt.title('Filtered detector data from the GW150914 event')
plt.grid()
plt.legend();

# Here, we'll play the sounds with Livingston in the left ear and Hanford in the right
display(Audio(data=np.vstack((np.roll(-l_filtered, int(0.0072*sampling_rate)), h_filtered)),
              rate=sampling_rate))"
"plt.close()
plt.plot(t+0.0072, -l_filtered, label='Livingston')
plt.plot(t, h_filtered, label='Hanford')
plt.plot(t-0.002, 8e21*nr, label='Simulated')
plt.xlim(16.0, 16.5)
plt.xlabel('Time (seconds)')
plt.ylabel('Detector strain $h$ (dimensionless)')
plt.title('Filtered detector data and simulated signal')
plt.grid()
plt.legend(loc='upper left');

# Here, we'll play the sounds with Livingston in the left ear, Hanford in the right,
# and NR in the center.  We ""roll"" the Livingston and simulated data as a simple way
# of shifting them in time.
display(Audio(data=np.vstack((np.roll(-l_filtered, int(0.0072*sampling_rate)),
                              h_filtered,
                              np.roll(8e21*nr, int(-0.002*sampling_rate)))),
              rate=sampling_rate))"
"# Filter and rescale arbitrarily to match amplitude of data
nr_filtered_h = 0.55 * filter_signal(nr, sampling_rate, noisy_signal=h)
nr_filtered_h_tilde = dt * np.fft.rfft(nr_filtered_h)
nr_filtered_l = 0.55 * filter_signal(nr, sampling_rate, noisy_signal=l)
nr_filtered_l_tilde = dt * np.fft.rfft(nr_filtered_l)

# Now, plot and play the *filtered* NR data on top of the filtered detector data
plt.close()
plt.plot(t+0.0072, -l_filtered, label='Livingston')
plt.plot(t, h_filtered, label='Hanford')
plt.plot(t-0.002, nr_filtered_h, label='Simulated and filtered')
plt.xlim(16.1, 16.5)
plt.xlabel('Time (seconds)')
plt.ylabel('Detector strain $h$ (dimensionless)')
plt.title('Filtered detector data and simulated signal')
plt.grid()
plt.legend(loc='upper left');
display(Audio(data=np.vstack((np.roll(-l_filtered, int(0.0072*sampling_rate)),
                              h_filtered,
                              np.roll(nr_filtered_h, int(-0.002*sampling_rate)))),
              rate=sampling_rate))"
"# Add small time offsets to the filtered detector data, to align the measured data to the model,
# and evaluate the correlation.  (The precise time offsets will be derived below.)
l_correlation = np.roll(-l_filtered, 37) * nr_filtered_h
h_correlation = np.roll(+h_filtered,  7) * nr_filtered_l

# Integrate the correlation functions over time
c_l = scipy.integrate.simps(l_correlation, t)
c_h = scipy.integrate.simps(h_correlation, t)
display(Latex(r'$c_{{\mathrm{{Livingston}}}} = {0:.4f}$'.format(c_l)))
display(Latex(r'$c_{{\mathrm{{Hanford}}}} = {0:.4f}$'.format(c_h)))

# Plot the correlation functions as functions of time
plt.close()
plt.plot(t, l_correlation, label='Livingston')
plt.plot(t, h_correlation, label='Hanford')
plt.grid()
plt.xlim(16.25, 16.5)
plt.xlabel('Time (seconds)')
plt.ylabel('Correlation between data and simulated signal')
plt.legend();"
"import warnings
warnings.filterwarnings('ignore')
import numpy as np
import matplotlib.pyplot as plt
plt.style.use('bmh')
%matplotlib inline

plt.figure(figsize = (12, 6))
for i in range(10):
    x = np.arange(i * 10, i * 10 + 10)
    y_var1 = np.random.randint(1, 5, 10)
    y_var2 = np.random.randint(5, 8, 10)
    plt.plot(x, y_var1, color = 'k', label = 'variable1')
    plt.plot(x, y_var2, color = 'g', label = 'variable2')
    plt.legend()
    plt.ylim(0, 9)"
"plt.figure(figsize = (12, 6))
for i in range(10):
    x = np.arange(i * 10, i * 10 + 10)
    y_var1 = np.random.randint(1, 5, 10)
    y_var2 = np.random.randint(5, 8, 10)
    plt.plot(x, y_var1, color = 'k', label = 'variable1' if i == 0 else ""_esto_no_se_pintará"")
    plt.plot(x, y_var2, color = 'g', label = 'variable2' if i == 0 else ""_esto_tampoco"")
    plt.legend()
    plt.ylim(0, 9)"
"## Dictionary
example_dictionary = {""A"": ""Adenine"", 
                      ""C"": ""Cytosine"", 
                      ""G"": ""Guanine"", 
                      ""T"": ""Thymine""}
print('A dictionary:', example_dictionary)
print('Value associated to key C:', example_dictionary['C'])"
"def PR(nd, na):
    return na/(na + nd)

PR(nd, na)"
"def FRET(nd, na, gamma):
    return na / (na + gamma*nd)

FRET(nd, na, gamma)"
"ns_a_as_func_na = na + Lk*nd + n_dir
ns_a_as_func_na"
"Er_sym = sympy.factor(PR(nd, ns_a).subs(ns_a, ns_a_as_func_na).subs(n_dir, d_exT*(nd*gamma + na)))
Er_sym"
"E_func_Er = sympy.factor(FRET(nd, na, gamma).subs(nd, solve(Er_sym - Er, nd)[0])).collect(Er)
E_func_Er"
E_func_Er
"E_func_Er.subs(Lk, 0).subs(d_exT, 0)"
"sympy.collect(E_func_Er.subs(gamma, 1).subs(d_exT, 0), Er)"
"sympy.collect(E_func_Er.subs(gamma, 1).subs(Lk, 0), Er)"
"sympy.collect(Er_func_E, E)"
"Er_func_E.subs(Lk, 0).subs(d_exT, 0)"
"sympy.collect(Er_func_E.subs(gamma, 1).subs(d_exT, 0), Er)"
"sympy.collect(Er_func_E.subs(gamma, 1).subs(Lk, 0), Er)"
"def StoichRaw(nd, na, naa):
    return (na + nd)/(na + nd + naa)

StoichRaw(nd, na, naa)"
"def Stoich(nd, na, naa, gamma):
    return (na + gamma*nd)/(na + gamma*nd + naa)

Stoich(nd, na, naa, gamma)"
"ns_a_as_func_na = na + Lk*nd + n_dir
ns_a_as_func_na"
"Sr_sym = sympy.factor(
    StoichRaw(nd, ns_a, naa).subs(ns_a, ns_a_as_func_na).subs(n_dir, d_exT*(na + gamma*nd))
    )
Sr_sym"
"S_sym = Stoich(nd, na, naa, gamma)
S_sym"
"S_func_Sr_nx = S_sym.subs(na, solve(Sr_sym - Sr, na)[0]).factor()
S_func_Sr_nx"
"E_sym = FRET(nd, na, gamma)
E_sym"
"na_func_Sr = solve(Sr_sym - Sr, na)[0]
na_func_Sr"
"E_func_Sr = E_sym.subs(na, na_func_Sr).factor()
E_func_Sr"
"nd_func_E_Sr = solve(E_func_Sr - E, nd)[0]
nd_func_E_Sr"
"S_func_E_Sr = S_func_Sr_nx.replace(nd, nd_func_E_Sr).factor()
S_func_E_Sr"
ns_a_as_func_na
"Er_sym = PR(nd, ns_a).subs(ns_a, ns_a_as_func_na).subs(n_dir, d_exT*(nd*gamma + na))
Er_sym"
"E_func_Er = FRET(nd, na, gamma).subs(nd, solve(Er_sym - Er, nd)[0]).factor()
E_func_Er"
"S_func_Er_Sr = S_func_E_Sr.replace(E, E_func_Er).factor()
S_func_Er_Sr "
"S_func_E_Spr = (S_func_E_Sr.replace(Lk, 0).replace(d_exT, 0)
                .replace(Sr, Spr).replace(Er, Epr))
S_func_E_Spr"
"S_func_Epr_Spr = (S_func_Er_Sr.replace(Lk, 0).replace(d_exT, 0)
                  .replace(Sr, Spr).replace(Er, Epr))
S_func_Epr_Spr"
S_func_Er_Sr
"Sr_func_Er_S = solve(S_func_Er_Sr - S, Sr)[0]
Sr_func_Er_S"
"plt.plot(S_corr[idx], 'o')"
"plt.hist(t.ravel(), bins=50);"
"plt.hist(d1.ravel(), bins=40, histtype='step');
plt.yscale('log')
plt.title('$\{d_i\}\quad$ Mean = %.3e  Std.Dev. = %.3e ' % (d1.mean(), d1.std()));"
"kws = dict(bins=np.arange(0, 4, 0.05), histtype='step', lw=1.5, normed=True)
print('{:>8} {:>6} {:>8} {:>8}  {:>8}  {:>8} {:>8}'
      .format('c', 'n', 'Mean', 'Std', 'MSE', 'Median', '% > λ'))
for c in (-1, 0, 1/3, 1, 2, 3):
    r_hc = (n - c)  / d1.sum(axis=1) / λ
    r_hc_err_rms = np.sqrt(np.mean(((r_hc - 1)**2)))
    plt.hist(r_hc, **kws, label = 'c = %.1f' % c);
    print('%8.2f %6d %8.5f %8.2f%% %8.3f%% %8.3f %8.2f%%' % 
          (c, n, r_hc.mean(), r_hc.std()*100, r_hc_err_rms*100, np.median(r_hc),
           (r_hc > 1).sum() * 100 / num_iter))
    
r_hm = 1/np.median(d1, axis=1) / λ
r_hm_err_rms = np.sqrt(np.mean(((r_hm - 1)**2)))
plt.hist(r_hm, **kws, label = 'median');
print('%8s %6d %8.5f %8.2f%% %8.3f%% %8.3f %8.2f%%' % 
      ('median', n, r_hm.mean(), r_hm.std()*100, r_hm_err_rms*100, np.median(r_hm),
       (r_hm > 1).sum() * 100 / num_iter))
plt.xlabel('Normalized Rate')
plt.axvline(1, color='k');
plt.legend()
plt.text(0.35, 0.75, r'$\Lambda_{n,c} = \frac{n - c}{T_n}$',
         fontsize=24, transform=plt.gcf().transFigure);"
"kws = dict(bins=np.arange(0, 4, 0.05), histtype='step', lw=1.5, normed=True)
print('{:>6} {:>8} {:>8}  {:>8}  {:>8} {:>8}'
      .format('n', 'Mean', 'Std', 'Err.RMS', 'Median', '% > 1/λ'))
d_h = np.mean(d1, axis=1) * λ
d_h_err_rms = np.sqrt(np.mean(((d_h - 1)**2)))
plt.hist(d_h, **kws, label = r'$\hat\tau$');
print('%6d %8.5f %8.2f%% %8.3f%% %8.3f %8.2f%%' % 
      (n, d_h.mean(), d_h.std()*100, d_h_err_rms*100, np.median(d_h),
      (d_h > 1).sum() * 100 / num_iter))
plt.legend(fontsize=18)
plt.xlabel('Normalized Delays')
plt.axvline(1, color='k');"
"rate_t = 1 / ((d1.mean(axis=1)).cumsum() / np.arange(1, num_iter+1)) / λ

plt.plot(rate_t[100:])
plt.ylim(0.98, 1.02)
plt.axhline(1, color='k');"
"rate_t2 = ((n - 1) / d1.sum(axis=1)).cumsum() / np.arange(1, num_iter+1) / λ

plt.plot(rate_t2[100:])
plt.ylim(0.98, 1.02)
plt.axhline(1, color='k');"
"def show_bar(P):
    plt.figure(figsize=(9,7));
    # Wall.
    plt.axvline(0, color='g', lw=13);
    # Distance matrix.
    D = dist(P)
    # We plot the springs.
    for i, j in zip(I, J):
        # The color depends on the spring tension, which
        # is proportional to the spring elongation.
        c = D[i,j] - L[i,j]
        plt.plot(P[[i,j],0], P[[i,j],1], 
                 lw=2, color=plt.cm.copper(c*150));
    # We plot the masses.
    plt.plot(P[[I,J],0], P[[I,J],1], 'ok',);
    # We configure the axes.
    plt.axis('equal');
    plt.xlim(P[:,0].min()-e/2, P[:,0].max()+e/2);
    plt.ylim(P[:,1].min()-e/2, P[:,1].max()+e/2);
    plt.xticks([]); plt.yticks([]);

show_bar(P0);
plt.title(""Initial configuration"");"
energy(P0.ravel())
"show_bar(P1);
plt.title(""Equilibrium configuration"");"
"import warnings
warnings.filterwarnings('ignore')
import random
z = random.randint(1,42)
print(z)
"
ziehung()
"import warnings
warnings.filterwarnings('ignore')
from math import *
from scipy import optimize
import matplotlib.pyplot as plt
%matplotlib inline 
import numpy as np

def func2d(x):
    f = cos(14.5 * x[0] - 0.3) + (x[1] + 0.2) * x[1] + (x[0] + 0.2) * x[0]
    df = np.zeros(2)
    df[0] = -14.5 * sin(14.5 * x[0] - 0.3) + 2. * x[0] + 0.2
    df[1] = 2. * x[1] + 0.2
    return f, df

delta = 0.01
x = np.arange(-3.0, 3.0, delta)
y = np.arange(-2.0, 2.0, delta)
X, Y = np.meshgrid(x, y)
dim = X.shape

X_ = X.reshape(dim[0]*dim[1],1)
Y_ = Y.reshape(dim[0]*dim[1],1)



Z_ = []
for i in range(len(X_)):
    v = [X_[i],Y_[i]]
    y,dy = func2d(v)
    Z_.append(y)

Z = np.array(Z_).reshape(dim)
plt.figure(figsize=(20,10)) 
CS = plt.contour(X, Y, Z)

"
"from scipy.optimize import basinhopping
minimizer_kwargs = {""method"":""L-BFGS-B"", ""jac"":True}
x0 = [1.0, 1.0]
ret = basinhopping(func2d, x0, minimizer_kwargs=minimizer_kwargs, niter=200)

print(""global minimum: x = [%.4f, %.4f], f(x0) = %.4f"" % (ret.x[0], ret.x[1], ret.fun))
"
""
dir(random)
"import numpy as np
import matplotlib.pyplot as plt
%matplotlib inline 

N = 2000000

X = np.random.random(N)
Y = np.random.random(N)
plt.figure(figsize=(9,9))
plt.ylim([0.1,0.2])
plt.xlim([0.1,0.2])
plt.plot(X,Y,'.')"
"import random

N = 2000000
X=[]
Y=[]
for i in range(N):
    X.append(random.random())
    Y.append(random.random())
    
plt.figure(figsize=(9,9))
plt.ylim([0.1,0.2])
plt.xlim([0.1,0.2])
plt.plot(X,Y,'.')
    "
"N = 20000
X=[]
Y=[]
for i in range(N):
    X.append(lcg())
    Y.append(lcg())
    
plt.figure(figsize=(9,9))
plt.ylim([0,1])
plt.xlim([0,1])
plt.plot(X,Y,'.')"
"from math import pi,sqrt,exp

N = 100

def f(x,sigma=0.15,mu=0.5):
    return 1/sigma/sqrt(2*pi)*exp(-1*(x-mu)**2/(2*sigma*sigma))

#print(1/sigma/sqrt(2*pi))
X = np.random.uniform(0.,1.,N)

Y = [] 
for i in range(len(X)):
    Y.append(f(X[i]))
plt.plot(X,Y,'.')"
"import random

N = 1000000

sigma = 0.15
X=[]

for i in range(N):
    x_ = random.random()
    scale = 1/sigma/sqrt(2*pi)
    s_ = scale*random.random()
    v = f(x_)
    if v > s_:
        X.append(x_)

    
    
plt.figure(figsize=(9,9))
#plt.ylim([0,1])
#plt.xlim([0,1])
plt.hist(X,100)"
"import warnings
warnings.filterwarnings('ignore')
from math import sin
import matplotlib.pyplot as plt
import numpy as np
%matplotlib inline

def f(x):
    return np.sin(x)-3*x**2+3.4*x**3-x**4

X = np.arange(0,1,0.02)
Y = f(X) 
plt.plot(X,Y)"
"np.random.seed(123)
n = 2000000
X = np.random.random(n)
Z = np.random.standard_normal(n)
Y = f(X)
res = sum(Y)/n
print('n:',n)
print('int:',res)


E = Y-res
E2 = E*E
sf = sum(E2)/(n-1)



print('sf:',sf)"
"plt.hist(Z,200)"
""
"import warnings
warnings.filterwarnings('ignore')
import numpy as np
import matplotlib.pyplot as plt
%matplotlib inline
#%matplotlib qt

from pylab import rcParams
rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots
rcParams['image.interpolation'] = 'nearest'
rcParams['image.cmap'] = 'gray'

%load_ext autoreload
%autoreload 2"
"plt.plot(spending)
months = np.arange(len(spending))
ps = np.polyfit(months, spending, 1)
plt.plot(months, np.sum([p*months**(1-i) for i,p in enumerate(ps)], axis=0))"
ps
"def generate_tiles(level, start=0, end=1024):
    n = 2 ** level
    width = 1024 // n
    height = width
    indices = np.linspace(start, end, num=n, endpoint=False, dtype=int)
    for i, x0 in enumerate(indices):
        for j, y0 in enumerate(indices):
            yield i, j, width, height, x0 + 300, y0
        
for level in [0, 1, 2]:
    for tile in generate_tiles(level):
        i, j, width, height, x0, y0 = tile
        print(tile)
        filename = 'cat_{level}_{i}_{j}.mp4'.format(**locals())
        crop = ""crop={width}:{height}:{x0}:{y0}"".format(**locals())
        ! ffmpeg -y -i 6FUmapkqPn0.mp4 -filter:v ""$crop, scale=256:256"" -b 100000  -ss 00:01:40 -t 00:00:30 -an $filename

    "
"v = 1
periods = 2
frames_per = 50
Npoints = 100

x = np.linspace(-1,1,Npoints)
y = np.linspace(-1,1,Npoints)
X,Y = np.meshgrid(x,y)

def k(m,n):
    # jn_zeros(n, nt): Compute nt zeros of the Bessel function Jn(x).
    return jn_zeros(n,m)[m-1] # m is 0-indexed here

def generate(X, Y, t, n, m, v, f1, f2):
    theta = arctan2(Y,X) # This does arctan(Y/X) but gets the sign right.
    R = sqrt(X**2 + Y**2)
    # We know z = J_n(k*r)*cos(n*theta)*cos(k*v*t)
    # 
    f1 = {'sin':sin,'cos':cos}[f1]
    f2 = {'sin':sin,'cos':cos}[f2]

    result = jn(n,k(m,n)*R)*f1(n*theta)*f2(k(m,n)*v*t)
    result[R>1] = 0  # we plot points from the square, but physically require this.
    return result


def plot_frame(n=0,m=1,t=0,
              f1='cos',f2='cos',
              elev=20,azim=60,
              alpha=0.9):
    fig = plt.figure()
    ax = fig.add_subplot(111, projection='3d')
    Z = generate(X,Y,t=t,n=n,m=m,v=v,f1=f1,f2=f2)
    #ax.plot_surface(X, Y, Z, rstride=4, cstride=4, alpha=0.3, cmap=cm.viridis, vmin=-1, vmax=1)
    ax.plot_surface(X, Y, Z, cmap=cm.viridis, vmin=-1, vmax=1, alpha=alpha)
    if n == 0:
        ax.set_zlim(-1,1)
    else:
        ax.set_zlim(-0.5,0.5)
    ax.view_init(elev=elev, azim=azim)
    plt.show()
_ = interact(plot_frame,t=(0,periods,periods/frames_per),
             n=(0,10),m=(1,10),
             f1=['sin','cos'],f2=['sin','cos'],
             elev=(-180,180,1),azim=(-180,180,1),
             alpha=(0,1,0.1),
            )"
"v = 1
periods = 2
frames_per = 50
Npoints = 100

x = np.linspace(-1,1,Npoints)
y = np.linspace(-1,1,Npoints)
X,Y = np.meshgrid(x,y)


def plot_frame2(t=0,nmax=1,mmax=2):
    # one period is 2*pi/jn_zeros(n,m)[m-1]
    #t = t*2*pi/jn_zeros(n,m)[m-1]
    ns = list(range(0,nmax))
    ms = list(range(1,mmax))
    fig = plt.figure(figsize=(6*len(ms),2*(len(ns))))
    
    axs = {}
    rows, cols = len(ns), 2*len(ms)

    idx = 1
    for m in ms:
        axs[m] = {}
        for n in ns:
            axs[m][n] = (fig.add_subplot(rows,cols,idx, projection='3d'),
                        fig.add_subplot(rows,cols,idx+1))
            idx += 2

    
    for m in ms:
        for n in ns:
            Z = generate(X, Y, t, n, m, v, 'cos', 'cos')
            axs[m][n][0].plot_surface(X, Y, Z, alpha=0.9, 
                                      cmap=cm.viridis, vmin=-0.7,vmax=0.7,)

            if n == 0:
                axs[m][n][0].set_zlim(-0.7,.7)
                axs[m][n][1].imshow(Z,vmin=-0.7,vmax=0.7, cmap=cm.viridis)
            else:
                axs[m][n][0].set_zlim(-0.5,0.5)
                axs[m][n][1].imshow(Z,vmin=-0.5,vmax=0.5, cmap=cm.viridis)
            # The funny business with levels here is because you won't
            # get a contour exactly at zero that necessarily tracks
            # around both sides of the circle due to the fact that
            # we've discretized things.
            levels = [-0.000000001,0.0,0.000000001]
            axs[m][n][1].contour(Z, levels, colors='k',
                                 linestyles='solid', linewidths=2)
    plt.show()
_ = interact(plot_frame2,t=(0,periods,periods/frames_per),
             nmax=(1,5),mmax=(2,5),
            )"
"import warnings
warnings.filterwarnings('ignore')
import numpy as np, matplotlib as mpl, pandas as pd, seaborn as sns
from matplotlib import pyplot as plt
%matplotlib inline"
"atmos, terrestrial, ocean = 750, 600, 1000
a_t, a_o, t_o, o_a = 110, 40, 110, 150

time = 0
dt = 0.01

def evolve_system(time,dt,atmos,terrestrial,ocean,a_t,a_o,t_o,o_a):
    atmos, terrestrial, ocean = atmos - a_t - a_o + o_a, terrestrial - t_o + a_t, ocean - o_a + a_o + t_o
    return atmos, terrestrial, ocean

system_state = []
for step in range(1000):
    time = time + dt
    atmos, terrestrial, ocean = evolve_system(time,dt,atmos,terrestrial,ocean,a_t, a_o, t_o, o_a)
    system_state.append((time,atmos,terrestrial,ocean))

time = [i[0] for i in system_state]
atmos = [i[1] for i in system_state]
terrestrial = [i[2] for i in system_state]
ocean = [i[3] for i in system_state]

plt.plot(time,atmos,label='atmospheric carbon')
plt.plot(time,terrestrial,label='terrestrial carbon')
plt.plot(time,ocean,label='oceanic carbon')
plt.ylim([500,1100])
plt.legend()"
"atmos, terrestrial, ocean = 750, 600, 1000
a_t_rate, a_o_rate, t_o_rate, o_a_rate = 110/750, 40/750, 110/600, 150/1000

time = 0
dt = 1 # This is in years, based on what I've done above. You want dt in something smaller than years. How do you handle that?

def evolve_system(dt,atmos,terrestrial,ocean,a_t_rate,a_o_rate,t_o_rate,o_a_rate):
    a_t = a_t_rate * atmos * dt
    a_o = a_o_rate * atmos * dt
    t_o = t_o_rate * terrestrial * dt
    o_a = o_a_rate * ocean * dt
    atmos, terrestrial, ocean = atmos - a_t - a_o + o_a, terrestrial - t_o + a_t, ocean - o_a + a_o + t_o
    return atmos, terrestrial, ocean

system_state = []
for step in range(30):
    time = time + dt
    atmos, terrestrial, ocean = evolve_system(dt,atmos,terrestrial,ocean,a_t, a_o, t_o, o_a)
    system_state.append((time,atmos,terrestrial,ocean))

time = [i[0] for i in system_state]
atmos = [i[1] for i in system_state]
terrestrial = [i[2] for i in system_state]
ocean = [i[3] for i in system_state]

plt.plot(time,atmos,label='atmospheric carbon')
plt.plot(time,terrestrial,label='terrestrial carbon')
plt.plot(time,ocean,label='oceanic carbon')
#plt.ylim([500,1100])
plt.legend()"
plotit(dt=.1)
plotit(dt=.01)
plotit(dt=.001)
"x = np.linspace(0,100,1000)
y1 = 0.2*x + np.random.rand(1000)
y2 = 0.4*x + np.random.rand(1000)
y3 = 500*x + np.random.rand(1000)
plt.plot(x,y1,label='y1')
plt.plot(x,y2,label='y2')
plt.plot(x,y3,label='y3')
plt.legend()"
"x = np.linspace(0,100,1000)
y1 = 0.2*x + np.random.rand(1000)
y2 = 0.4*x + np.random.rand(1000)
y3 = 500*x + np.random.rand(1000)

fig, (ax1, ax2) = plt.subplots(2, 1, sharex=True)

ax1.plot(x,y1,label='y1')
ax1.plot(x,y2,label='y2')
ax2.plot(x,y3,label='y3')
plt.legend()"
"x = np.linspace(0,100,1000)
y1 = 0.2*x + np.random.rand(1000)
y2 = 0.4*x + np.random.rand(1000)
y3 = 500*x + np.random.rand(1000)

fig, (ax1, ax2) = plt.subplots(2, 1, sharex=True)

ax2.plot(x,y1,label='y1',color='blue')
ax2.plot(x,y2,label='y2',color='green')
ax1.plot(x,y3,label='y3',color='#aaef12')
ax1.legend()
ax2.legend()"
"dt = 0.01
atmos0, terrestrial0, ocean0 = 750, 600, 1000
a_t_rate, a_o_rate, t_o_rate, o_a_rate = dt*0.147, dt*0.053, dt*0.183, dt*0.15

from scipy import integrate

def simplemodel():
    # We make a vector X containing [atmos, terrestrial, ocean]
    # Here are our initial values
    init = [atmos0,terrestrial0,ocean0]
    def derivX(X,t):
        # Note that we have to take in `t` as a parameter, even though we don't use it.
        atmos, terrestrial, ocean = X
        a_t = a_t_rate * atmos
        a_o = a_o_rate * atmos
        t_o = t_o_rate * terrestrial
        o_a = o_a_rate * ocean
        # Just the derivatives here. Compare to the previous code!
        datmos = - a_t - a_o + o_a
        dterrestrial = - t_o + a_t
        docean = - o_a + a_o + t_o
        return np.array([atmos, terrestrial, ocean])

    # Look up the difference between np.arange and np.linspace
    time = np.arange(0,1,dt) 

    # Let scipy.integrate.odeint do all the work for us!
    X = integrate.odeint(derivX,init,time)
    atmos, terrestrial, ocean = X[:,0], X[:,1], X[:,2]

    plt.plot(time,atmos,label='atmospheric carbon')
    plt.plot(time,terrestrial,label='terrestrial carbon')
    plt.plot(time,ocean,label='oceanic carbon')
    plt.legend()
simplemodel()"
"t = np.linspace(0,100,1000)
alpha = 0.2
x = alpha*t
plt.plot(t,x)"
"x = linspace(0,L,100)
plt.plot(x,f(x,1.0))
plt.show()"
"interact(plotspread,sigma=(0.01,100.1,1),L=(0.1,10,.1),kappa=(0,50,.1),h0=(0,10,.2))"
"plt.plot(x,xfact,label='our def')
plt.plot(x,sp.special.gamma(x+1),label='gamma')
plt.legend()"
"plt.plot(x,xfact,label='our def')
plt.plot(x,sp.special.gamma(x+1),label='gamma')
plt.legend()

plt.xlim((8.9,9.1))
plt.ylim((250000,500000))"
"plt.plot(x,xfact - sp.special.gamma(x+1),label='diff')
plt.legend()

#plt.xlim((9.5,10))"
"x = np.linspace(1,10,20000)
xfact = [fact(i) for i in x]
plt.plot(x,np.log(xfact),label='our def')
plt.plot(x,np.log(sp.special.gamma(x+1)),label='gamma')
plt.legend()"
"_ = interact(T,N=(1,20,1),x=(0,10,0.1),y=(0,30,0.1))"
"def plotX(N=0,y=0):
    x = np.linspace(0,10,100)
    plt.plot(T(N,x,y))
    plt.show()
_ = interact(plotX,N=(1,20,1),y=(0,30,0.1))"
"def plotY(N=0,x=0):
    y = np.linspace(0,30,300)
    plt.plot(T(N,x,y))
    plt.show()
_ = interact(plotY,N=(1,20,1),x=(0,10,0.1))"
"x = np.linspace(0,10,100)
y = np.linspace(0,30,300)
X,Y = np.meshgrid(x,y)
fig = plt.figure()
ax = fig.gca(projection='3d')
surf = ax.plot_surface(X,Y,T(10,X,Y))
plt.show()"
"degree_step = 1
def plotT3D(N=1,elev=0,ymax=30,azim=0):
    x = np.linspace(0,10,100)
    y = np.linspace(0,ymax,300)
    X,Y = np.meshgrid(x,y)
    fig = plt.figure()
    ax = fig.gca(projection='3d')
    surf = ax.plot_surface(X,Y,T(N,X,Y))
    ax.view_init(elev=elev,azim=azim)    
    plt.show()
_ = interact(plotT3D,N=(0,200,1),ymax=(10,30,1),azim=(0,360,degree_step),elev=(0,360,degree_step))"
"# Number of points
N = 600
# sample spacing
T = 1.0/800.0
# linspace gives N evenly spaced points between the start and stop values.
x = np.linspace(0.0,N*T,N)
# That returns a numpy array.
print(x[:10]) # here are the first 10 points"
"plt.plot(x,y)
plt.xlabel('$x$')
plt.ylabel('$\sin(50 \\times 2 \pi x) + 0.5\sin(80\\times 2\pi x)$')"
"# Now take the FFT
yf = fft(y)
# You should understand the spacing here. 
# As far as why you only want half of the points, see the documentation.
# No, really, see the documentation. Maybe even `fftshift`.
xf = np.linspace(0.0, 1.0/(2.0*T), N//2)
plt.plot(xf, 2.0/N * np.abs(yf[0:N//2]))
# Question for you: what should the axis labels be?"
print(data)
"# you'll have to read the docs to figure out what `rate` is, 
# but here's me plotting the signal directly.
plt.plot(data)"
"dt = 0.0005
t = np.arange(0.0, 20.0, dt)
s1 = np.sin(2*np.pi*100*t)
s2 = 2*np.sin(2*np.pi*400*t)

# create a transient ""chirp""
mask = np.where(np.logical_and(t > 10, t < 12), 1.0, 0.0)
s2 = s2 * mask

# add some noise into the mix
nse = 0.01*np.random.random(size=len(t))

x = s1 + s2 + nse  # the signal
NFFT = 1024       # the length of the windowing segments
Fs = int(1.0/dt)  # the sampling frequency

# Pxx is the segments x freqs array of instantaneous power, freqs is
# the frequency vector, bins are the centers of the time bins in which
# the power is computed, and im is the matplotlib.image.AxesImage
# instance

ax1 = plt.subplot(211)
plt.plot(t, x)
plt.subplot(212, sharex=ax1)
Pxx, freqs, bins, im = plt.specgram(x, NFFT=NFFT, Fs=Fs, noverlap=900,
                                cmap=plt.cm.gist_heat)
plt.grid(False) # Hard to interpret with the grid on. "
"dt = 0.0005
t = np.arange(0.0, len(data)*dt, dt)
s1 = np.sin(2*np.pi*100*t)
s2 = 2*np.sin(2*np.pi*400*t)

# create a transient ""chirp""
mask = np.where(np.logical_and(t > 10, t < 12), 1.0, 0.0)
s2 = s2 * mask

# add some noise into the mix
nse = 0.01*np.random.random(size=len(t))

x = s1 + s2 + nse  # the signal
NFFT = 1024       # the length of the windowing segments
Fs = int(1.0/dt)  # the sampling frequency

# Pxx is the segments x freqs array of instantaneous power, freqs is
# the frequency vector, bins are the centers of the time bins in which
# the power is computed, and im is the matplotlib.image.AxesImage
# instance

ax1 = plt.subplot(211)
plt.plot(t, data)
plt.subplot(212, sharex=ax1)
Pxx, freqs, bins, im = plt.specgram(data, NFFT=NFFT, Fs=Fs, noverlap=900,
                                cmap=plt.cm.gist_heat)
plt.grid(False) # Hard to interpret with the grid on. "
plt.plot(data[:1000])
"print('Exact',exact(1/2))
print('estimate',estimate(1/2))
print('error',error(1/2))"
"x = np.linspace(0,1,100)
plt.plot(x,error(x),label='actual error')
plt.plot(x,error_bound(x),label='14.3 bound')
plt.plot(x,error_bound2(x),label='14.4 bound')
plt.legend();"
"x = np.linspace(0,1/2,100)
plt.plot(x,error(x),label='actual error')
plt.plot(x,error_bound(x),label='14.3 bound')
plt.plot(x,error_bound2(x),label='14.4 bound')
plt.legend();"
"plt.plot(x,x,label='term 1')
plt.legend(fancybox=True);"
"plt.plot(x,x,label='term 1')
plt.plot(x,x-x**3/6,label='term 1 and 3')
plt.legend(fancybox=True);"
"plt.plot(x,x,label='term 1')
plt.plot(x,x-x**3/6,label='term 3')
plt.axis((-pi,pi,-4,4))
plt.legend(fancybox=True);"
"def showterms(n=0,xmax=4*pi):
    x = linspace(-xmax,xmax,2000)
    total = zeros_like(x)
    plt.clf()
    plt.plot(x,np.sin(x),'.-',linewidth=4,color='red',label='sin(x)')
    for i in range(n+1):
        this_term = ((-1)**i * x**(1+2*i))/factorial(1+2*i)
        plt.plot(x,this_term,label='Term {i}'.format(i=i))
        total = total + this_term
    plt.plot(x,total,'--',linewidth=2,color='black',label='Sum')
    plt.axis((-xmax,xmax,-4,4))
    plt.grid(True)
    plt.legend(fancybox=True,loc='upper left',framealpha=0.5)
interact(showterms,n=(0,8),xmax=fixed(2*pi));"
"x = np.linspace(0,2*np.pi,100)
xgas = np.random.rand(100,10)*2*np.pi

def plotit(t=1.5):
    k = 1
    w = 1
    displacement = np.sin(k*x)*np.cos(w*t)
    pressure = np.cos(k*x)*np.cos(w*t)
    plt.subplot(2,1,2)
    plt.ylim(-1.1,1.1)
    plt.plot(x,displacement,'k-',label='$\Delta x$')
    plt.plot(x,pressure,'r-',label='$\Delta p$')
    plt.legend()
    plt.subplot(2,1,1)
    ygas = np.linspace(-1,1,xgas.shape[-1]) * np.ones_like(xgas)
    displacement = (np.pi/2)*np.sin(k*xgas)*np.cos(w*t)
    plt.scatter(xgas,ygas,facecolors='none')
    plt.scatter(xgas + displacement,ygas)
    plt.xlim(-.5,1.1*2*np.pi)
interact(plotit,t=(0,10,0.1))"
"def plot_airy1(n=1):
    a0 = 1
    x = np.linspace(-12,5,10000)
    y = np.zeros_like(x)
    for i,coeff in enumerate(get_y1_coeffs(n)):
        y = y + (x**i)*coeff
    plt.plot(x,y,'k--',label=""First {n} terms"".format(n=n))
    #plt.plot(x,sp.special.airy(x)[2]+sp.special.airy(x)[3],label=""full solution (ish)"")
    #plt.plot(x,sp.special.airy(x)[0],label='0')
    #plt.plot(x,sp.special.airy(x)[1],label='1')
    #plt.plot(x,sp.special.airy(x)[2],label='2')
    #plt.plot(x,sp.special.airy(x)[3],label='3')
    plt.axis([-12,5,-3,3])
    
    plt.legend()

interact(plot_airy1,n=(0,100));"
"def plot_airy1(n=1):
    a0 = 1
    x = np.linspace(-12,5,10000)
    y = np.zeros_like(x)
    for i,coeff in enumerate(get_y1_coeffs(n)):
        y = y + (x**i)*coeff
    plt.plot(x,y,'k--',label=""First {n} terms"".format(n=n))
    #plt.plot(x,sp.special.airy(x)[2]+sp.special.airy(x)[3],label=""full solution (ish)"")
    #plt.plot(x,sp.special.airy(x)[0],label='0')
    #plt.plot(x,sp.special.airy(x)[1],label='1')
    #plt.plot(x,sp.special.airy(x)[2],label='2')
    #plt.plot(x,sp.special.airy(x)[3],label='3')
    plt.axis([-12,5,-3,3])
    
    plt.legend()

interact(plot_airy1,n=(0,100))"
"nep = 1.21e-12
BW = 260e3
gain = 2.38e4
responsivity = 0.5
pmin = nep * np.sqrt(BW)
volts_min = pmin * responsivity * gain
print(volts_min)"
"# resolvable power:
pmin * scope_floor_factor"
"nep = 0.2e-12
BW = 50e6
gain = 50000
responsivity = 25
pmin = nep * np.sqrt(BW)
volts_min = pmin * responsivity * gain
print(volts_min)
scope_floor_factor = 0.010/volts_min

# resolvable power:
pmin * scope_floor_factor"
"# Enter the specs of the detector
nep = 2.34e-12  # in Watts per root hz
BW = 10e6  # Bandwidth in Hz
gain = 0.75e4  # gain in V/A
responsivity = 0.5  # Amps per Watt (assume 800 nm)
pmin = nep * np.sqrt(BW)
volts_min = pmin * responsivity * gain
print(""voltage generated by p_min:"",volts_min)"
"# the power has to be scope_floor_factor times larger in order to generate 10mV:
pmin * scope_floor_factor"
"import warnings
warnings.filterwarnings('ignore')
%matplotlib notebook
import pandas as pd  
import matplotlib.pyplot as plt  
from ipywidgets import *  
from IPython.display import display  
import ipywidgets  
plt.style.use('ggplot')

NUMBER_OF_PINGS = 4

#displaying the text widget
text = widgets.Text(description=""Domain to ping"", width=200)  
display(text)

#preparing the plot 
data = pd.DataFrame()  
x = range(1,NUMBER_OF_PINGS+1)  
plots = dict()  
fig, ax = plt.subplots()  
plt.xlabel('iterations')  
plt.ylabel('ms')  
plt.xticks(x)  
plt.show()

#preparing a container to put in created checkbox per domain
checkboxes = []  
cb_container = widgets.HBox()  
display(cb_container)

#add button that updates the graph based on the checkboxes
button = widgets.Button(description=""Update the graph"")

#function to deal with the added domain name
def handle_submit(sender):  
    #a part of the magic inside python : pinging
    res = !ping -c {NUMBER_OF_PINGS} {text.value}
    print(res)
    hits = res.grep('64 bytes').fields(-2).s.replace(""time="","""").split()
    if len(hits) == 0:
        print(""Domain gave error on pinging"")
    else:
         #rebuild plot based on ping result
        data[text.value] = hits
        data[text.value] = data[text.value].astype(float)
        plots[text.value], = ax.plot(x, data[text.value], label=text.value)
        plt.legend()
        plt.draw()
        #add a new checkbox for the new domain
        checkboxes.append(widgets.Checkbox(description = text.value, value=True, width=90))
        cb_container.children=[i for i in checkboxes]
        if len(checkboxes) == 1:
            display(button)

#function to deal with the checkbox update button       
def on_button_clicked(b):  
    for c in cb_container.children:
        if not c.value:
            plots[c.description].set_visible(False)
        else:
            plots[c.description].set_visible(True)
    plt.legend()
    plt.draw()

button.on_click(on_button_clicked)  
text.on_submit(handle_submit)  
plt.show()  "
"import numpy as np
from scipy.special import gamma
import matplotlib.pyplot as plt
%matplotlib inline
plt.rc('font', **{'size' : 22})

x = 1
ns = []; errs = [];
for n in range(30):
    ns.append(n)
    errs.append(abs(np.exp(x) - myexp(x, n)))
ns = np.array(ns)

plt.figure(figsize=(18, 10))
plt.semilogy(ns, errs, 'k.', ms=16)
plt.semilogy(ns, max(1, np.exp(x)) * x**ns / gamma(ns+1), 'r', lw=3)
plt.ylim([1e-18, 1])
plt.xlabel('$n$')
plt.ylabel('$\\varepsilon$')
plt.title('Error for $e^{%g}$' % x)
plt.show()"
"from scipy.special import loggamma
ns = []; errs = [];
x = -30
for n in range(0, 150, 5):
    ns.append(n)
    errs.append(abs(np.exp(x) - myexp(x, n)))
ns = np.array(ns)
plt.figure(figsize=(18, 10))
plt.semilogy(ns, errs, 'k.', ms=16)
plt.semilogy(ns, max(1, np.exp(x)) * np.exp(ns * np.log(abs(x)) \
                            - np.real(loggamma(ns+1))), 'r-', lw=3)
plt.ylim([1e-16, 1e12])
plt.title('Error for $e^{%g}$' % x)
plt.xlabel('$n$')
plt.ylabel('$\\varepsilon$')
plt.show()"
"hs = np.logspace(-16, 0, num=50) # h = 1e-16 ... 1
errs1 = []
errs2 = []
for h in hs:
    errs1.append(abs(diff1(np.sin, 1, h) - np.cos(1)))
    errs2.append(abs(diff2(np.sin, 1, h) - np.cos(1)))

M2 = M3 = 1
plt.figure(figsize=(18, 10))
plt.loglog(hs, errs1, 'k.', ms=16)
plt.loglog(hs, errs2, 'r.', ms=16)
plt.loglog(hs, M2 * hs / 2, 'k-', label='First order', lw=3)
plt.loglog(hs, M3 * hs**2 / 6, 'r-', label='Second order', lw=3)
plt.ylim(1e-14, 1)
plt.xlabel('$h$')
plt.ylabel('$\\varepsilon$')
plt.legend(loc='upper center')
plt.show()"
"import warnings
warnings.filterwarnings('ignore')
import numpy as np
from numpy.linalg import norm, inv

def nu(A, b, kind = np.inf):
    return norm(inv(A), kind) * norm(b, kind) / norm(inv(A).dot(b), kind)

eps = 0.0001
A = np.array([[1, 2], [2, 4 + eps]])
b = np.array([1, 2])
print('nu_inf(A, b) =', nu(A, b, np.inf))
print('nu_1(A, b) =', nu(A, b, 1))
print('nu_E(A, b) =', nu(A, b, 2))"
"def mu(A, kind = np.inf):
    return norm(inv(A), kind) * norm(A, kind)

eps = 0.0001
A = np.array([[1, 2], [2, 4 + eps]])
print('mu_inf(A) =', mu(A, np.inf))
print('mu_1(A) =', mu(A, 1))
print('mu_E(A) =', mu(A, 2))"
"from scipy.linalg import solve_banded

a =  np.array([0, 1, 1, 1, 1, 1, 1, 1])
b = -np.array([2, 2, 2, 2, 2, 2, 2, 2])
c =  np.array([1, 1, 1, 1, 1, 1, 1, 0])
f = -np.array([1, 1, 1, 1, 1, 1, 1, 1])
x = solve_tdm(a, b, c, f)
print('TDM solver: x=', x)

G = np.zeros((3, len(a)))
G[0, 1:]  = c[:-1]
G[1, :]   = b
G[2, :-1] = a[1:]
x_scipy = solve_banded((1, 1), G, f)
print('SciPy banded solver: x=', x_scipy)"
"from scipy.linalg import lu, solve_triangular
def lu_solve(A, f):
    P, L, U = lu(A) # A = P L U
    y = solve_triangular(L, P.T.dot(f), lower=True, unit_diagonal=True)
    x = solve_triangular(U, y, lower=False, unit_diagonal=False)
    return x

n = 4
x0 = np.ones(n)
A = np.random.rand(n, n)
f = A.dot(x0)
x = lu_solve(A, f)
print('x = ', x)
lu(A)"
"import matplotlib.pyplot as plt
%matplotlib inline
plt.rc('font', **{'size' : 22})

A = np.array([[1, 2, 3], [2, 5, 7], [3, 7, 15]])
xstar = np.array([1, 2, 3])
f = A.dot(xstar)
x0 = np.array([0, 0, 0])

print('lambda(A) =', np.linalg.eigvalsh(A))

tau = 0.12
print('tau =', tau)
x, res = simple_iteration(A, f, x0, tau)
plt.figure(figsize=(12,8))
plt.semilogy(res)
plt.xlabel('$k$')
plt.ylabel('$\|r_k\|$')
plt.show()"
"print('lambda(A) =', np.linalg.eigvalsh(A))

tau = 0.1
print('tau =', tau)
x, res = simple_iteration(A, f, x0, tau, maxit=10000)
plt.figure(figsize=(12,8))
plt.semilogy(res)
plt.xlabel('$k$')
plt.ylabel('$\|r_k\|$')
plt.show()"
"import warnings
warnings.filterwarnings('ignore')
import numpy as np
import matplotlib.pyplot as plt
%matplotlib inline
plt.rc('font', **{'size' : 18})

lmin = 1
lmax = 5
lamA = np.linspace(lmin, lmax, 20)
tau_opt = 2 / (lmin + lmax)
tau_max = 2 / lmax
taus = [tau_opt / 2, tau_opt, tau_max, 1.1 * tau_max]
q_opt = (lmax - lmin) / (lmax + lmin)
fig = plt.figure(figsize=(8, 6))
ax = fig.add_subplot(111)
for tau in taus:
    ax.plot([0, lmax+1], [1, 1 - tau * (lmax+1)], 'k')
    ax.plot(lamA, 1 - tau * lamA, lw=3, label = '$\\tau = %g$' % tau)
ax.plot([0, lmax+1], [1, 1], 'r-')
ax.plot([0, lmax+1], [-1, -1], 'r-')
ax.plot([lmin, lmin], [-1.1, 1.1], 'k--')
ax.plot([lmax, lmax], [-1.1, 1.1], 'k--')
ax.plot([0, lmax+1], [q_opt, q_opt], 'g-')
ax.plot([0, lmax+1], [-q_opt, -q_opt], 'g-')
ax.annotate('$q_\\operatorname{opt}$', (0.05, q_opt-0.1))
ax.annotate('$-q_\\operatorname{opt}$', (0.05, -q_opt-0.1))
ax.annotate('$\\lambda_\\operatorname{min}$', (lmin+0.05, -1.05))
ax.annotate('$\\lambda_\\operatorname{max}$', (lmax+0.05, -1.05))

ax.set_xlim([0, lmax+1])
ax.set_ylim([-1.1, 1.1])
ax.set_xlabel('$\\lambda(A)$')
ax.set_ylabel('$\\lambda(B)$')
ax.legend(bbox_to_anchor=(1.5, 1))
plt.show()"
"n = 10
A = np.random.rand(n, n) # берем случаную матрицу
A = A + np.diag(A.sum(axis=1)) # и делаем ей диагональное преобладание
x = np.ones(n) # точное решение
f = A.dot(x)
x0 = np.zeros_like(x)
xres, it = jacobi(A, f, x0)
print('Done in %d iterations' % it)
print('||x - x_res|| =', np.linalg.norm(xres - x))"
"n = 10
A = np.random.rand(n, n)
# Делаем случайную симметричную матрицу
A = A.T.dot(A) 
# Добавим немного единичной матрицы для лучшей обусловленности A
A = 0.1 * np.eye(n) + A

x = np.ones(n)
f = A.dot(x)
x0 = np.zeros_like(x)
xres, it = seidel(A, f, x0)
print('Done in %d iterations' % it)
print('||x - x_res|| =', np.linalg.norm(xres - x))"
"n = 30
A = np.diag(2.001 * np.ones(n)) - np.diag(np.ones(n-1), k=-1) - np.diag(np.ones(n-1), k=1)
D = np.diag(np.diag(A))
B_J = np.linalg.solve(D, A - D)
rho_B_J = np.abs(np.linalg.eigvals(B_J)).max()
omega = 2 / (1 + np.sqrt(1 - rho_B_J**2))

x = np.ones(n)
f = A.dot(x)
x0 = np.zeros_like(x)
xres, it = seidel(A, f, x0)
xres2, it2 = sor(A, f, x0, omega)
print('Seidel done in %d iterations' % it)
print('SOR done in %d iterations' % it2)
print('Seidel ||x - x_res|| =', np.linalg.norm(xres - x))
print('SOR ||x - x_res|| =', np.linalg.norm(xres2 - x))"
"import warnings
warnings.filterwarnings('ignore')
import numpy as np
import matplotlib.pyplot as plt
%matplotlib inline
plt.rc('font', **{'size' : 14})

x = np.linspace(0, 5, 21)
y = 3*x+5 + 0.5*np.random.randn(len(x))
A = np.zeros((len(x), 2))
A[:, 0] = x
A[:, 1] = 1
f = y
a,b = np.linalg.solve(A.T.dot(A), A.T.dot(f))
plt.plot(x, y, '.')
plt.plot(x, a*x+b, '-')
plt.title('$%g + %g x$' % (a,b))
plt.show()"
"n = 21
x = np.linspace(0, 5, n)
y = 2 + 3 * x + 0.5 * x**2 + 0.2 * np.random.randn(len(x))
A = np.zeros((n, 3))
A[:, 0] = 1
A[:, 1] = x
A[:, 2] = x**2
f = y
a,b,c = np.linalg.solve(A.T.dot(A), A.T.dot(f))
plt.plot(x, y, '.')
plt.plot(x, a+b*x+c*x**2, '-')
plt.title('$%g + %g x + %g x^2$' % (a,b,c))
plt.show()"
"from scipy.integrate import quad
a = 0; b = 4; n = 8
def func(x): return np.sin(np.pi * x)
G = np.zeros((n, n))
g = np.zeros(n)
for i in range(n):
    for k in range(n):
        G[i, k], _ = quad(lambda x: x**(i+k), a, b)
    g[i], _ = quad(lambda x: x**i * func(x), a, b)

c = np.linalg.solve(G, g)
X = np.linspace(a, b, 100)
fig = plt.figure(figsize=(12, 5))
ax = fig.add_subplot(121)
ax.plot(X, func(X), 'r', lw=2)
ax.plot(X, np.array([c[i]*X**i for i in range(n)]).sum(axis=0), 'b', lw=2)
ax.set_title('$f(x), P_{%d}(x)$' % (n-1))
ax = fig.add_subplot(122)
ax.plot(X, func(X) - np.array([c[i]*X**i for i in range(n)]).sum(axis=0), 'g', lw=3)
ax.set_title('$f(x) - P_{%d}(x)$' % (n-1))
plt.show()"
"from scipy.linalg import hilbert, svdvals
def cond2(A):
    s = svdvals(A);
    return s[0] / s[-1]
for n in range(5, 31, 5):
    H = hilbert(n)
    print('n = %d, mu_2(H_n) = %e' % (n, cond2(H)))"
"from scipy.special import legendre
x = np.linspace(-1, 1, 1000)
for k in range(6):
    plt.plot(x, legendre(k)(x), label='$P_{%d}(x)$' % k, lw=2)
plt.title('Legendre polynomials')
plt.ylim(-1.1, 1.1)
plt.legend(bbox_to_anchor=(1.4, 1.05))
plt.show()"
"a = -1; b = 1; n = 8
def func(x): return np.sin(2 * np.pi * x)
g = np.zeros(n)
A = np.zeros(n)
for k in range(n):
    g[k], _ = quad(lambda x: legendre(k)(x) * func(x), a, b)
    A[k] = 2 / (2*k+1)
c = g / A
X = np.linspace(a, b, 100)
fig = plt.figure(figsize=(12, 5))
ax = fig.add_subplot(121)
ax.plot(X, func(X), 'r', lw=2)
ax.plot(X, np.array([c[i]*legendre(i)(X) for i in range(n)]).sum(axis=0), 'b', lw=2)
ax.set_title('$f(x), P_{%d}(x)$' % (n-1))
ax = fig.add_subplot(122)
ax.plot(X, func(X) - np.array([c[i]*legendre(i)(X) for i in range(n)]).sum(axis=0), 'g', lw=3)
ax.set_title('$f(x) - P_{%d}(x)$' % (n-1))
plt.show()"
"from scipy.special import chebyt
x = np.linspace(-1, 1, 1000)
for k in range(5):
    plt.plot(x, chebyt(k)(x), label='$T_{%d}(x)$' % k, lw=2)
plt.title('Chebyshev polynomials')
plt.legend(bbox_to_anchor=(1.4, 1.05))
plt.ylim(-1.1, 1.1)
plt.show()"
"a = -1; b = 1; n = 8
def func(x): return np.sin(2 * np.pi * x)
g = np.zeros(n)
A = np.zeros(n)
for k in range(n):
    g[k], _ = quad(lambda x: chebyt(k)(x) * func(x) / np.sqrt(1 - x**2), a, b)
    A[k] = np.pi / 2 if k > 0 else np.pi
c = g / A
X = np.linspace(a, b, 100)
fig = plt.figure(figsize=(12, 5))
ax = fig.add_subplot(121)
ax.plot(X, func(X), 'r', lw=2)
ax.plot(X, np.array([c[i]*chebyt(i)(X) for i in range(n)]).sum(axis=0), 'b', lw=2)
ax.set_title('$f(x), P_{%d}(x)$' % (n-1))
ax = fig.add_subplot(122)
ax.plot(X, func(X) - np.array([c[i]*chebyt(i)(X) for i in range(n)]).sum(axis=0), 'g', lw=3)
ax.set_title('$f(x) - P_{%d}(x)$' % (n-1))
plt.show()"
"import matplotlib.pyplot as plt
%matplotlib inline
plt.rc('font', **{'size' : 14})
x = np.linspace(-2. + 0.001, 3, 1000);
plt.figure(figsize=(7,4))
plt.plot(x, x, 'r', label='$x$');
plt.plot(x, np.log(x + 2), 'g', label='$\ln(x+2)$');
plt.grid()
plt.legend(loc='upper left')
plt.ylim(-3, 3)
plt.show()"
"from scipy.optimize import fsolve

def phi(x):
    return np.log(x + 2)

x = 1.5
for i in range(10):
    x = phi(x)
    
[xtrue] = fsolve(lambda x: x - phi(x), 1.5, xtol=1e-20)
print('x =', x, ', x* =', xtrue)
print('x - x* =', x - xtrue)"
"x0 = -1.84140
x = x0
for i in range(10):
    x = phi(x)
    
[xtrue] = fsolve(lambda x: x - phi(x), x0, xtol=1e-20)
print('x =', x, ', x* =', xtrue)
print('x - x* =', x - xtrue)"
"def phi2(x):
    return np.exp(x) - 2

x0 = -1.45
x = x0
for i in range(10):
    x = phi2(x)
    
[xtrue] = fsolve(lambda x: x - phi2(x), x0, xtol=1e-20)
print('x =', x, ', x* =', xtrue)
print('x - x* =', x - xtrue)"
"def f(x):
    return (x-1)*(x+2)**2
def fprime(x):
    return 3 * x * (x+2)

x0 = 2.8
x = [x0]
n = 3
for i in range(n):
    x.append(x[-1] - f(x[-1]) / fprime(x[-1]))
    
X = np.linspace(0, 3, 1000)
plt.figure(figsize=(7, 4))
plt.plot(X, f(X), 'b')
plt.plot(X, 0*X, 'k')
for i in range(n):
    plt.plot([x[i], x[i]], [0, f(x[i])], 'g')
for i in range(n+1):    
    plt.annotate('$x_{%d}$' % i, xy=(x[i]-0.05, -3))
for i in range(n):
    plt.plot([x[i], x[i+1]], [f(x[i]), 0], 'r')

plt.grid()"
"def f(x):
    return (x-1)**2 * (x+2)
def fprime(x):
    return 3 * (x**2 - 1)

x0 = 2.8
x = [x0]
n = 4
for i in range(n):
    x.append(x[-1] - f(x[-1]) / fprime(x[-1]))
    
X = np.linspace(0, 3, 1000)
plt.figure(figsize=(7, 4))
plt.plot(X, f(X), 'b')
plt.plot(X, 0*X, 'k')
for i in range(n):
    plt.plot([x[i], x[i]], [0, f(x[i])], 'g')
for i in range(n+1):    
    plt.annotate('$x_{%d}$' % i, xy=(x[i]-0.05, -1))
for i in range(n):
    plt.plot([x[i], x[i+1]], [f(x[i]), 0], 'r')
plt.ylim(-3, 20)

plt.grid()"
"t = np.linspace(0, 1, 10)
plt.figure(figsize=(8, 6))
plt.plot(np.cos(t), t, 'r', label='$x = \cos(y)$', lw=2)
plt.plot(t, np.sin(t), 'b', label='$y = \sin(x)$', lw=2)
plt.xlabel('$x$')
plt.ylabel('$y$')
plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))
plt.axis('square')
plt.show()"
"def phi(x, y):
    return np.cos(y), np.sin(x)

x, y = 0.8, 0.8
for i in range(1, 91):
    x, y = phi(x, y)
    if i % 10 == 0: print('it =', i, 'x =', x, 'y =', y)"
"import warnings
warnings.filterwarnings('ignore')
import numpy as np
import matplotlib.pyplot as plt
%matplotlib inline
plt.rc('font', **{'size' : 14})

n = 9
h = 0.2 + np.random.rand(n)
x = np.cumsum(h)
x -= x[0]
x /= x[-1]
a = 2 * np.pi
x = a * (2 * x - 1)

def f(x):
    return np.sin(x)
fv = f(x)

W = np.empty((n, n))
for k in range(n):
    W[:, k] = x**k
c = np.linalg.solve(W, fv)

X = np.linspace(-a, a, 1000)
P = lambda x: np.polyval(list(reversed(c)), x)

plt.plot(X, f(X), 'r', label='$f(x)$')
plt.plot(X, P(X), 'b', label='$P_{%d}(x)$' % (n-1))
plt.plot(x, fv, 'g.', ms=10, label='$f(x_k)$')
plt.axis([-a, a, -1.2, 1.2])
plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))
plt.show()"
"def divided_differences(x, f):
    n = len(x);
    F = np.empty((n, n))
    F[:, 0] = f
    for k in range(1, n):
        F[0:n-k, k] = (F[1:n-k+1, k-1] - F[0:n-k, k-1]) / (x[k:] - x[:-k])
    return F # F[i, k] = f(x_i, x_{i+1}, ..., x_{i+k})

x = np.array([1, 2, 4, 5])
f = np.array([1, 3, 1, 3])
F = divided_differences(x, f)
print(F)"
"def evaluate(x, F, x0):
    n = len(x);
    P = 0;
    xprod = 1.0 # (x - x1) (x - x2) ... (x - xi)
    for i in range(n):
        P += F[0, i] * xprod
        xprod *= (x0 - x[i])
    return P

X = np.linspace(0.5, 5.5, 1000)
plt.plot(X, evaluate(x, F, X), 'r', lw=2, label='$P(x)$')
plt.plot(x, f, 'g.', ms=15, label='$f(x_i)$')
plt.legend(loc='center left', bbox_to_anchor=(1, .5)); plt.show()"
"n = 5
x = np.cumsum(0.5 + np.random.rand(n))
X = np.linspace(x[0], x[-1], 1000)
for i in range(n):
    v = np.eye(n)[i]
    F = divided_differences(x, v)
    plt.plot(X, evaluate(x, F, X), label='$\ell_{%d}(x)$' % (i+1), lw=2)
    plt.plot(x, v, 'k.', ms=10)
plt.legend(loc='center left', bbox_to_anchor=(1, .5)); plt.show()"
"def runge(x):
    return 1 / (1 + 25 * x**2)

X = np.linspace(-1, 1, 1000)

plt.figure(figsize=(10, 8))
plt.plot(X, runge(X), 'r', label='$f(x)$', lw=2)
n = 5
x = np.linspace(-1, 1, n)
F = divided_differences(x, runge(x))
plt.plot(X, evaluate(x, F, X), label='$P_{%d}(x)$' % (n-1), lw=2)
plt.axis([-1, 1, -1, 2]); 
plt.legend(loc='center left', bbox_to_anchor=(1, .5)); plt.show()"
"plt.figure(figsize=(10, 8))
plt.plot(X, runge(X), 'r', label='$f(x)$', lw=2)
n = 10
x = np.linspace(-1, 1, n)
F = divided_differences(x, runge(x))
plt.plot(X, evaluate(x, F, X), label='$P_{%d}(x)$' % (n-1), lw=2)
plt.axis([-1, 1, -1, 2]); 
plt.legend(loc='center left', bbox_to_anchor=(1, .5)); plt.show()"
"plt.figure(figsize=(10, 8))
plt.plot(X, runge(X), 'r', label='$f(x)$', lw=2)
n = 15
x = np.linspace(-1, 1, n)
F = divided_differences(x, runge(x))
plt.plot(X, evaluate(x, F, X), label='$P_{%d}(x)$' % (n-1), lw=2)
plt.axis([-1, 1, -1, 2]); 
plt.legend(loc='center left', bbox_to_anchor=(1, .5)); plt.show()"
"plt.figure(figsize=(10, 8))
plt.plot(X, runge(X), 'r', label='$f(x)$', lw=2)
n = 20
x = np.linspace(-1, 1, n)
F = divided_differences(x, runge(x))
plt.plot(X, evaluate(x, F, X), label='$P_{%d}(x)$' % (n-1), lw=2)
plt.axis([-1, 1, -1, 2]); 
plt.legend(loc='center left', bbox_to_anchor=(1, .5)); plt.show()"
"X = np.linspace(-1, 1, 1000)
x = np.linspace(-1, 1, 10)
omega = (X - x[0]);
for i in range(1, len(x)):
    omega *= (X - x[i])
plt.plot(X, omega, 'b', lw=2, label='$\omega(x)$')
plt.plot(X, 0*X, 'k')
plt.grid()
plt.legend(loc='center left', bbox_to_anchor=(1,.5)); plt.show()"
"X = np.linspace(-1, 1, 1000)
x = np.cos((2*np.arange(10)+1) / 20 * np.pi)
omega = (X - x[0]);
for i in range(1, len(x)):
    omega *= (X - x[i])
plt.plot(X, omega, 'b', lw=2, label='$\omega(x)$')
plt.plot(X, 0*X, 'k')
plt.grid()
plt.legend(loc='center left', bbox_to_anchor=(1,.5)); plt.show()"
"def runge(x):
    return 1 / (1 + 25 * x**2)

X = np.linspace(-1, 1, 1000)

plt.figure(figsize=(10, 8))
plt.plot(X, runge(X), 'r', label='$f(x)$', lw=2)
n = 5
x = np.cos((np.arange(n)*2+1) / (2*n) * np.pi)
F = divided_differences(x, runge(x))
plt.plot(X, evaluate(x, F, X), label='$P_{%d}(x)$' % (n-1), lw=2)
plt.axis([-1, 1, -1, 2]); 
plt.legend(loc='center left', bbox_to_anchor=(1, .5)); plt.show()"
"plt.figure(figsize=(10, 8))
plt.plot(X, runge(X), 'r', label='$f(x)$', lw=2)
n = 10
x = np.cos((np.arange(n)*2+1) / (2*n) * np.pi)
F = divided_differences(x, runge(x))
plt.plot(X, evaluate(x, F, X), label='$P_{%d}(x)$' % (n-1), lw=2)
plt.axis([-1, 1, -1, 2]); 
plt.legend(loc='center left', bbox_to_anchor=(1, .5)); plt.show()"
"plt.figure(figsize=(10, 8))
plt.plot(X, runge(X), 'r', label='$f(x)$', lw=2)
n = 15
x = np.cos((np.arange(n)*2+1) / (2*n) * np.pi)
F = divided_differences(x, runge(x))
plt.plot(X, evaluate(x, F, X), label='$P_{%d}(x)$' % (n-1), lw=2)
plt.axis([-1, 1, -1, 2]); 
plt.legend(loc='center left', bbox_to_anchor=(1, .5)); plt.show()"
"plt.figure(figsize=(10, 8))
plt.plot(X, runge(X), 'r', label='$f(x)$', lw=2)
n = 20
x = np.cos((np.arange(n)*2+1) / (2*n) * np.pi)
F = divided_differences(x, runge(x))
plt.plot(X, evaluate(x, F, X), label='$P_{%d}(x)$' % (n-1), lw=2)
plt.axis([-1, 1, -1, 2]); 
plt.legend(loc='center left', bbox_to_anchor=(1, .5)); plt.show()"
"def ell(i, x, xs):
    A = np.array([(x-xs[k])/(xs[i]-xs[k]) for k in range(len(xs)) if i != k])
    return np.prod(A, axis=0)

def L(x, xs):
    return np.sum([np.abs(ell(i, x, xs)) for i in range(len(xs))], axis=0)

X = np.linspace(-np.pi, np.pi, 1000)
x = np.linspace(-np.pi, np.pi, 9)

plt.figure(figsize=(10, 5))
plt.subplot(1, 2, 1);
plt.plot(X, L(X, x), lw=2)
plt.xlabel('$x$')
plt.ylabel('$L(x)$')
plt.xlim(-np.pi, np.pi)
plt.ylim(0, None)

fv = np.sin(x)
delta = 0.15
fvs = fv + delta * np.tanh(10 * np.random.rand(len(fv)) - 5)

plt.subplot(1, 2, 2);
F = divided_differences(x, fv)
Fs = divided_differences(x, fvs)
plt.plot(X, evaluate(x, F, X), 'b', label='$P(x)$', lw=2)
up = evaluate(x, F, X) + delta * L(X, x)
down = evaluate(x, F, X) - delta * L(X, x)
plt.fill_between(X, down, up, alpha = 0.5)
plt.plot(X, evaluate(x, Fs, X), 'r', label='$\\tilde P(x)$', lw=2)
plt.plot(x, fv, 'b.', label='$f(x_i)$', ms=10)
plt.plot(x, fvs, 'r.', label='$f(x_i) + \\Delta f(x_i)$', ms=10)
plt.xlim(-np.pi, np.pi)
plt.legend(loc='center left', bbox_to_anchor=(1, .5))

plt.show()"
"K = 20
x = np.linspace(0, 1, 1000)
plt.plot(x, ((1+K)**2*np.abs(x-1)*np.abs(x)+(K*np.abs(x-1)+np.abs(x))*np.abs(x-1+K*x))/K, lw=2)
Lv = 1 + 0.5 * K**2 / (1 + K)
plt.ylim(0, Lv+1)
plt.title('$K = %g,\\quad L = 1 + K^2 / (2K+2) \\approx %g$' % (K, Lv))
plt.grid()
plt.show()"
"x = np.linspace(0, 1, 1000)
xs = np.array([0, 0.15, 0.35, 0.65, 0.85, 1])
plt.plot(x, np.sin(2*np.pi*x), lw=2, label='$f(x)$')
plt.plot(xs, np.sin(2*np.pi*xs), lw=2, label='$s(x)$')
plt.legend(loc='center left', bbox_to_anchor=(1, .5))
plt.show()"
"xs = np.array([0, .3, .5, .7, 1.]); ys = np.sin(2*np.pi*xs)
m = cubic_spline(xs, ys)
plt.plot(np.linspace(0, 1, 1000), np.sin(np.linspace(0, 2*np.pi, 1000)), 'r', lw=2)
for i in range(1, len(m)):
    x = np.linspace(xs[i-1], xs[i])
    plt.plot(x, hermite(ys[i-1], m[i-1], ys[i], m[i], xs[i-1], xs[i], x), 'b', lw=2)
plt.plot(xs, ys, 'g.', ms=10)
plt.show()"
"import numpy as np
import matplotlib.pyplot as plt
%matplotlib inline
plt.rc('font', **{'size' : 14})

def f(x): return 4 / (1 + x*x)
exact = 4 * np.arctan(0.5)

ns = 6 + 3 * 2**np.arange(1, 12)
errs = []
for n in ns:
    x = np.linspace(0, 0.5, n+1)
    fv = f(x)
    I1 = rectangle(fv, x[1] - x[0])
    I2 = trapezoid(fv, x[1] - x[0])
    I3 = simpson(fv, x[1] - x[0])
    I4 = threeeights(fv, x[1] - x[0])
    errs.append([abs(I1-exact), abs(I2-exact), abs(I3-exact), abs(I4-exact)])

errs=np.array(errs)
    
plt.figure(figsize=(10, 7))
plt.loglog(ns, errs[:, 0], 'b.-', label='Rectangles', lw=2, ms=10)
plt.loglog(ns, errs[:, 1], 'g.-', label='Trapezoids', lw=2, ms=10)
plt.loglog(ns, errs[:, 2], 'r.-', label='Simpson''s', lw=2, ms=10)
plt.loglog(ns, errs[:, 3], 'm.-', label='3/8 rule', lw=2, ms=10)
plt.legend(loc='center left', bbox_to_anchor=(1, .5))
plt.xlabel('$n$')
plt.ylabel('$\\varepsilon$')
plt.xlim(ns[0], ns[-1])
plt.grid()
plt.show()"
"def f(x): return 4 / (1 + x*x)
exact = 4 * np.arctan(0.5)

ns = 2**np.arange(1, 10)
errs = []
for n in ns:
    fv = f(np.linspace(0, 0.5, n+1))
    Is = simpson(fv, 0.5/n)
    Ig = gauss2(f, 0, 0.5, n)
    errs.append([abs(Is-exact), abs(Ig-exact)])

errs=np.array(errs)
    
plt.figure(figsize=(10, 7))
plt.loglog(ns, errs[:, 0], 'b.-', label='Simpson''s', lw=2, ms=10)
plt.loglog(ns, errs[:, 1], 'r.-', label='Gaussian 2pt', lw=2, ms=10)
plt.legend(loc='center left', bbox_to_anchor=(1, .5))
plt.xlabel('$n$')
plt.ylabel('$\\varepsilon$')
plt.xlim(ns[0], ns[-1])
plt.grid()
plt.show()"
"import warnings
warnings.filterwarnings('ignore')
import numpy as np

def difference_solution(N, eps=0):
    T = 1.0
    tau = T / N
    u = np.zeros(N+1)
    u[0] = 1 + eps * np.random.rand(1)
    for n in range(N):
        k1 = -np.sin(u[n])
        k2 = -np.sin(u[n] + tau/2 * k1)
        u[n+1] = u[n] + tau * k2 + tau * eps * np.random.rand(1)
    return u
# Проверяем устойчивость
for N in 2**np.arange(3, 10):
    u = difference_solution(N)
    v = difference_solution(N, 1e-6)
    print('N =', N, ' max |u_n - v_n| =', np.linalg.norm(u - v, np.inf))"
"import matplotlib.pyplot as plt
%matplotlib inline

def exact_solution(t): return 2 * np.arctan(np.exp(-t) * np.tan(0.5))
# Сравним численное решение с точным
plt.plot(np.linspace(0, 1, 1000), exact_solution(np.linspace(0, 1, 1000)))
for N in [10, 20, 40]:
    u = difference_solution(N)
    y = exact_solution(np.linspace(0, 1, N+1))
    plt.plot(np.linspace(0, 1, N+1), u, '.',
        label='N = %d, error = %e' % (N, np.linalg.norm(u - y, np.inf)))
plt.legend(); plt.show()"
"def unstable_method(N, eps=0):
    T = 1.0
    tau = T / N
    u = np.zeros(N+1)
    u[0] = 1 + eps * np.random.rand(1)
    u[1] = 1 - tau * np.sin(1) + eps * np.random.rand(1)
    for n in range(1, N):
        rhs = 0.9 * np.sin(u[n]) + 0.1 * np.sin(u[n-1])
        u[n+1] = 0.5*(3*u[n-1]-u[n] - 5*tau*rhs + 5*tau*eps*np.random.rand(1))
    return u
# Проверяем устойчивость
for N in 2**np.arange(3, 10):
    u = unstable_method(N)
    v = unstable_method(N, 1e-6)
    print('N =', N, ' max |u_n - v_n| =', np.linalg.norm(u - v, np.inf))"
"# Сравним численное решение с точным
plt.plot(np.linspace(0, 1, 1000), exact_solution(np.linspace(0, 1, 1000)))
for N in [5, 10, 20]:
    u = unstable_method(N)
    y = exact_solution(np.linspace(0, 1, N+1))
    plt.plot(np.linspace(0, 1, N+1), u, '.-',
        label='N = %d, error = %e' % (N, np.linalg.norm(u - y, np.inf)))
plt.legend(); plt.show()"
"def exact_solution(t): return 2 * np.arctan(np.exp(-t) * np.tan(0.5))
# Сравним численное решение методом РК4 с точным
plt.plot(np.linspace(0, 1, 1000), exact_solution(np.linspace(0, 1, 1000)))
for N in [5, 10, 20]:
    u = rk4(N)
    y = exact_solution(np.linspace(0, 1, N+1))
    plt.plot(np.linspace(0, 1, N+1), u, '.',
        label='N = %d, error = %e' % (N, np.linalg.norm(u - y, np.inf)))
plt.legend(); plt.show()"
"T, Y = adaptive_stepsize(aren, aren_init, aren_tmax, euler, 1e-4)
orbit_plot(Y, skip=20) # Отмечаем точкой каждый 20й шаг"
"T, Y = adaptive_stepsize(aren, aren_init, aren_tmax, midpoint, 1e-4)
orbit_plot(Y, skip=5) # Отмечаем точкой каждый 5й шаг"
"T, Y = adaptive_stepsize(aren, aren_init, aren_tmax, rk4, 1e-4)
orbit_plot(Y, skip=1) # Отмечаем точкой каждый шаг"
"T, Y = fixed_stepsize(aren, aren_init, aren_tmax, rk4, aren_tmax/5000)
orbit_plot(Y, skip=50) # Отмечаем точкой каждый 50й шаг"
"T, Y = adaptive_stepsize(chem, chem_init, chem_tmax, euler, 1e-2)
T2, Y2 = adaptive_stepsize(lambda t,u:-0.5*u, np.array([1]), chem_tmax, euler, 1e-2)
plt.figure(figsize=(14, 5))
plt.subplot(1,2,1); plt.plot(T, Y[:, 0], '.-'); plt.grid(); 
plt.subplot(1,2,2); plt.plot(T2, Y2[:, 0], '.-') # <<похожая>> задача
plt.grid(); plt.show()"
"T1, Y1 = adaptive_stepsize(chem, chem_init, 0.5, euler, 1e-2)
T2, Y2 = fixed_stepsize(chem, Y1[-1,:], chem_tmax-0.5, euler, 0.08)
plt.plot(T1, Y1[:, 0], '.-')
plt.plot(T2+0.5, Y2[:, 0], '.-')
plt.axis([0, 5, 0, 1])
plt.grid(); plt.show()"
"T, Y = adaptive_stepsize(chem, chem_init, chem_tmax, imeuler, 1e-2)
plt.plot(T, Y[:, 0], '.-')
plt.grid(); plt.show()"
"T, Y = adaptive_stepsize(chem, chem_init, chem_tmax, immidpoint, 1e-2)
plt.plot(T, Y[:, 0], '.-')
plt.grid(); plt.show()"
"T, Y = fixed_stepsize(chem, chem_init, chem_tmax, imeuler, 0.2)
plt.plot(T, Y[:, 0], '.-')
plt.ylim(0, 1)
plt.grid(); plt.show()"
"T, Y = fixed_stepsize(chem, chem_init, chem_tmax, immidpoint, 0.2)
plt.plot(T, Y[:, 0], '.-')
plt.ylim(0, 1)
plt.grid(); plt.show()"
"X,Y = np.meshgrid(np.linspace(-3, 3), np.linspace(-2, 2))
Z = X + 1j * Y
plt.figure(figsize=(14, 10))
plt.rc('font', size=22)
plt.contour(X, Y, np.abs(1 + Z), [1], colors='k')
plt.contourf(X, Y, np.abs(1 + Z), [0, 1], colors='gray')
plt.xlabel('${\\rm Re}\; z$')
plt.ylabel('${\\rm Im}\; z$')
plt.title('$r(z) = 1+z$')
plt.grid()"
"X1, u1 = solve_advection(step, g, upwind, M= 50, courant=0.5)
X2, u2 = solve_advection(step, g, upwind, M=100, courant=0.5)
X3, u3 = solve_advection(step, g, upwind, M=200, courant=0.5)
X4, u4 = solve_advection(step, g, upwind, M=400, courant=0.5)
plt.plot(X1, u1, label='M=50'); plt.plot(X2, u2, label='M=100');
plt.plot(X3, u3, label='M=200'); plt.plot(X4, u4, label='M=400')
plt.plot(X3, step(X3 - 0.6), 'k', label='exact solution')
plt.ylim(-0.1, 1.1); plt.legend(loc='lower left')
plt.title('Upwind, $\sigma = 0.5$'); plt.show()"
"X1, u1 = solve_advection(step, g, upwind, M= 50, courant=0.5)
X2, u2 = solve_advection(step, g, upwind, M=100, courant=0.25)
X3, u3 = solve_advection(step, g, upwind, M=200, courant=0.125)
X4, u4 = solve_advection(step, g, upwind, M=400, courant=0.0625)
plt.plot(X1, u1, label='M=50'); plt.plot(X2, u2, label='M=100');
plt.plot(X3, u3, label='M=200'); plt.plot(X4, u4, label='M=400')
plt.plot(X3, step(X3 - 0.6), 'k', label='exact solution')
plt.ylim(-0.1, 1.1); plt.legend(loc='lower left')
plt.title(r'Upwind, $\tau = 25h^2$'); plt.show()"
"X, u = solve_advection(step, g, upwind, courant=0.5, verbose=True)
plt.plot(X, u)
plt.plot(X, step(X - 0.6), 'k', label='exact solution')
plt.ylim(-0.1, 1.2)
plt.title('Upwind, $M = 200, \sigma=0.5$'); plt.show()"
"X, u = solve_advection(step, g, central, courant=0.5, verbose=True)
plt.plot(X, u)
plt.plot(X, step(X - 0.6), 'k', label='exact solution')
plt.ylim(-0.1, 1.2)
plt.title('Central, $M = 200, \sigma=0.5$'); plt.show()"
"# Схему с центральной разностью можно считать устойчивой при
# выполнении условия т=O(h^2), то есть при малых числах Куранта
X, u = solve_advection(step, g, central, courant=0.01, verbose=True)
plt.plot(X, u)
plt.plot(X, step(X - 0.6), 'k', label='exact solution')
plt.ylim(-0.1, 1.2)
plt.title('Central, $M = 200, \sigma=0.01$'); plt.show()"
"X, u = solve_advection(step, g, lax, courant=0.5, verbose=True)
plt.plot(X, u)
plt.plot(X, step(X - 0.6), 'k', label='exact solution')
plt.ylim(-0.1, 1.1)
plt.title('Lax, $M = 200$'); plt.show()"
"X1, u1 = solve_advection(step, g, lax, M= 50, courant=0.5)
X2, u2 = solve_advection(step, g, lax, M=100, courant=0.5)
X3, u3 = solve_advection(step, g, lax, M=200, courant=0.5)
X4, u4 = solve_advection(step, g, lax, M=400, courant=0.5)
plt.plot(X1, u1, label='M=50'); plt.plot(X2, u2, label='M=100');
plt.plot(X3, u3, label='M=200'); plt.plot(X4, u4, label='M=400')
plt.plot(X3, step(X3 - 0.6), 'k', label='exact solution')
plt.ylim(-0.1, 1.1); plt.legend(loc='lower left')
plt.title('Lax, $\sigma = 0.5$'); plt.show()"
"X1, u1 = solve_advection(step, g, lax, M= 50, courant=0.5)
X2, u2 = solve_advection(step, g, lax, M=100, courant=0.25)
X3, u3 = solve_advection(step, g, lax, M=200, courant=0.125)
X4, u4 = solve_advection(step, g, lax, M=400, courant=0.0625)
plt.plot(X1, u1, label='M=50'); plt.plot(X2, u2, label='M=100');
plt.plot(X3, u3, label='M=200'); plt.plot(X4, u4, label='M=400')
plt.plot(X3, step(X3 - 0.6), 'k', label='exact solution')
plt.ylim(-0.1, 1.1); plt.legend(loc='lower left')
plt.title(r'Lax, $\tau = 25h^2$'); plt.show()"
"p = 0.45 # probability for head
n = 100
heads = (np.random.random(n) < p).sum()

fig, ax = plt.subplots()
ax.bar([0, 1], [heads, n-heads])
ax.set_xticks((0,1))
ax.set_xticklabels(('Heads', 'Tails'))
ax.set_ylabel('Frequency')
sns.despine()"
"print(""P-value for two-sided biomial test for a fair coin: {:.2g}"".format(
    proportion.binom_test(heads, n, prop=0.5)
))
"
"min_heads, max_heads = proportion.binom_test_reject_interval(0.5, n, alpha=0.05)
min_heads, max_heads, heads"
"rv = stats.binom(n, 0.5)
max_freq = rv.pmf(n//2)
fig, ax = plt.subplots()
ax.fill_between([min_heads, max_heads], [max_freq*1.1]*2, alpha=0.2)
ax.plot(rv.pmf(np.arange(0, n+1)))
ax.axvline(heads, color='k', ls='--')
ax.annotate('rejection region', (0.4, 1.01), xycoords='axes fraction')
ax.annotate('observed', 
            (heads, 0.05), xytext=(-60, 0), 
            xycoords='data', textcoords='offset points',
            arrowprops=dict(arrowstyle='->')
)
ax.set(
    ylim=(0, max_freq*1.1),
    xlabel='Heads',
    ylabel='Probability'
)
sns.despine()"
"t = np.linspace(0, 12, 12)
R0s = np.array([0.05, 0.15, 0.3, 0.45, 0.6, 0.9, 1.4])
N0 = 0.1
h = 0.5
ε = 0.8
K = 0.2
rs = []

fig, axes = plt.subplots(2, 1, figsize=(10, 8), sharex=False, sharey=False)

ax = axes[1]
for R0 in R0s:
    y = monod_growth(t, R0, N0, h, ε, K)
    R, N = y[:,0], y[:,1]
    N += np.random.normal(0, 0.01)
#     ax.plot(t, R, '--', label='R')
    ax.plot(t, N, '--o', label=R0)
    rs.append( fit_expo(t, N) )

ax.set(
    xlabel='Time',
    ylabel='Concentration',
#     yscale='log'
)
ax.legend(title='R0', loc='center right', frameon=True)

axes[0].plot(R0s, R0s*ε*h/(R0s+K), 'o--', lw=3, label='Real')
axes[0].plot(R0s, rs, 'o--', label='Approx')
axes[0].set(xlabel='R', ylabel=r'$\frac{1}{N} \cdot \frac{dN}{dt}$')

fig.tight_layout()
sns.despine()"
"fig, ax = plt.subplots()
y = monod_growth(t, R0, N0, h, ε, K)
R, N = y[:,0], y[:,1]
ax.plot(t, R, '-', label='R')
ax.plot(t, N, '-', label='N')
ax.plot(t, ε*R+N, '-', label='M')
ax.legend()"
person_element.attrib
"# %load examples/csvdict.py
import csv
import os.path

csv_path = os.path.join('examples', 'persons.csv')
with open(csv_path, 'r', encoding='utf-8', newline='') as csv_file:
    for person in csv.DictReader(csv_file, delimiter=';'):
        print(person)
"
person_element.attrib
"import datetime

def log(message, level='INFO', **keywords):
    time_text = datetime.datetime.now().isoformat()
    message_to_log = time_text + ' ' + level + ': '
    for word in message.split():
        if word.startswith('$'):
            key = word[1:]
            message_to_log += keywords[key]
        else:
            message_to_log += word
        message_to_log += ' '
    print(message_to_log)

log('Hello world!')
log('Here we go, $name', name='Alice')
log('Cannot write $item to $path', level='ERROR', item='tax data', path='/tmp/tax.txt')"
alice.age()
"baerbel.date_of_birth = date(1991, 2, 15)
baerbel.age()"
baerbel
"class FailureAndErrorTest(unittest.TestCase):
    def test_error(self):
        with open('no_such_file.tmp', 'r', encoding='utf-8'):
            assertEqual(1, 0)

    def test_failure(self):
        self.assertEqual(1, 0)        

run_test_class(FailureAndErrorTest)"
"import os.path
import unittest

class TextFileTest(unittest.TestCase):
    def setUp(self):
        text_path = os.path.join('examples', 'der_rote_komet.txt')
        self._text_file = open(text_path, 'r', encoding='utf-8')

    def tearDown(self):
        self._text_file.close()

    def test_has_lines(self):
        line_count = len(list(self._text_file))
        self.assertGreater(line_count, 0)
        
    def test_has_line_with_umlaut_u(self):
        self.assertTrue(any('ü' in line for line in self._text_file))

run_test_class(TextFileTest)"
"date_of_birth = datetime.date(1987, 3, 28)
age = datetime.date.today() - date_of_birth
print(age.days)"
"import os
import datetime

some_txt_path = os.path.join('examples', 'some.txt')  # path to the file to examine

last_modified_timestamp = os.path.getmtime(some_txt_path)  # timestamp when last modified
datetime.datetime.fromtimestamp(last_modified_timestamp)  # ...converted to datetime"
"!grep "">"" data/Crassostrea_gigas.GCA_000297895.1.29.pep.all.fa | wc -l"
"!grep "">"" data/Crassostrea_gigas.GCA_000297895.1.29.pep.all.fa > analyses/pep.title"
"import warnings
warnings.filterwarnings('ignore')
import pandas as pd
pd.read_table('http://www.flightstats.com/go/FlightStatus/flightStatusByAirport.do')"
"fig, axes = plt.subplots(figsize=(7.2,4.8), dpi=100)

axes.plot(ONEE, epsb1w.imag, 'C3--', label=r'$\epsilon_{b}(\omega)$', lw=2)
axes.plot(ONEE, epsl1w.imag, 'C4', label=r'$\epsilon_{\ell}(\omega)$', lw=2)

axes.set_xlabel('One-photon energy (eV)')
axes.set_ylabel(r'Im$[\epsilon(\omega)]$')
axes.set_xlim([2.5, 10])
axes.set_ylim([0, 80])
axes.legend();"
"fig, axes = plt.subplots(figsize=(7.2,4.8), dpi=100)

axes.plot(2*ONEE, RsP, lw=2)
axes.set_xlabel('Two-photon energy (eV)')
axes.set_ylabel(r'$\mathcal{R}_{sP}\,\,(10^{-20}\times\,\mathrm{cm}^{2}/\mathrm{W})$')
axes.set_xlim([2.5, 5])
axes.set_ylim([0, 0.06]);"
pd?
"import warnings
warnings.filterwarnings('ignore')
import pandas as pd
%matplotlib inline

df = pd.DataFrame([1,2,4,8,16,32,64,128,256])
df.plot()"
"import numpy as np

x = np.array([0,1])
w = np.array([0.5, 0.5])
b = -0.7
print (""w*x = {0}"".format(w*x))
print (""np.sum(w*x) = {0}"".format(np.sum(w*x)))
print (""np.sum(w*x) + b = {0}"".format(np.sum(w*x) + b))

### AND
def AND(x1,x2):
    x = np.array([x1, x2])
    w = np.array([0.5, 0.5])
    b = -0.7
    tmp = np.sum(w*x) + b
    if tmp <= 0:
        return 0
    else:
        return 1

print ('---AND---')
print (AND(0,0))
print (AND(1,0))
print (AND(0,1))
print (AND(1,1))

### NAND
def NAND(x1,x2):
    x = np.array([x1, x2])
    w = np.array([-0.5, -0.5])
    b = 0.7
    tmp = np.sum(w*x) + b
    if tmp <= 0:
        return 0
    else:
        return 1

print ('---NAND---')
print (NAND(0,0))
print (NAND(1,0))
print (NAND(0,1))
print (NAND(1,1))


### OR
def OR(x1,x2):
    x = np.array([x1, x2])
    w = np.array([0.9, 0.9])
    b = -0.5
    tmp = np.sum(w*x) + b
    if tmp <= 0:
        return 0
    else:
        return 1

print ('---OR---')
print (OR(0,0))
print (OR(1,0))
print (OR(0,1))
print (OR(1,1))
"
"csv_url = 'csv/H29-05-01.csv'
df = pd.read_csv(csv_url, encoding=""SHIFT-JIS"")
df"
"csv_url = 'csv/H29-05-01.csv'
df = pd.read_csv(csv_url, skiprows=4, encoding=""SHIFT-JIS"")
df"
"#df.plot.bar()
#df.title(u'平成29年5月1日現在の人口推計')
df.plot.bar()"
"import warnings
warnings.filterwarnings('ignore')
import scipy as sp
from scipy.integrate import fixed_quad
f = lambda x: 1/(1+x)
I = fixed_quad(f,0,1,n=4)
print(""A integral deu {}"".format(I))"
help(simps)
"A1 = tabelaFourier(4,3)
print(A1)
"
"D=np.dot(A2,A2.T)
D1=D[1:,1:]
print(D1)"
"y = np.array([2,-1,3, 4, 5])
E=np.dot(A2,y.T)
print(E[1:])"
"a = [D1[u,u]/E[u+1] for u in range(5)]
print(a)"
"%matplotlib inline
f0= lambda x: 1
f1= lambda x: np.cos(x)
f2= lambda x: np.sin(x)
f3= lambda x: np.cos(2*x)
f4= lambda x: np.sin(2*x)
f = lambda x: a[0]*f0(x)+ a[1]*f1(x) + a[2]*f2(x) + a[3]*f3(x)+a[4]*f4(x)
from matplotlib.pyplot import plot
t=np.linspace(-0.1,6.4,100)
x=A2[0,:]
plot(x,y,""ro"", t,f(t))"
two_kids & at_least_one_boy
"def observed_boy(outcome): return 'b' in outcome

two_kids_b & observed_boy"
"import random
random.sample(list(two_kids_w), 8)"
"random.sample(list(two_kids_wb), 5)"
"monty & t(""Open3"")"
"from collections import Counter

Counter(simulate_monty('switch') for _ in range(10 ** 5))"
Counter(simulate_monty('stick') for _ in range(10 ** 5))
"DK = ProbDist(GG=121801., GB=126840.,
              BG=127123., BB=135138.)
DK"
"%matplotlib inline 
import matplotlib.pyplot as plt

X = list(range(1000, 1000000, 10000))
plt.plot(X, [util(x) for x in X])
print('Y axis is util(x); x axis is in thousands of dollars.')"
"from collections import Counter

random.seed(123456)

c = Counter(simulate_st_pete() for _ in range(100000))
c"
"random.seed('running')

for i in range(10):
    plot_running_averages(simulate_st_pete, 100000);"
"def ellsburg():
    show('R', 'r')
    show('B', 'k')
    show('RY', 'r--')
    show('BY', 'k--')
    plt.xlabel('Number of black balls')
    plt.ylabel('Expected value of each gamble')
    
blacks = list(range(68))
urns   = [Counter(R=33, B=b, Y=67-b) for b in blacks]
    
def show(colors, line):
    scores = [score(colors, urn) for urn in urns]
    plt.plot(blacks, scores, line)
    
def score(colors, urn): return sum(urn[c] for c in colors)

ellsburg()"
"def ellsburg2():
    show2('R', 'r')
    show2('B', 'k')
    show2('RY', 'r--')
    show2('BY', 'k--')
    plt.xlabel('Different combinations of two urns')
    plt.ylabel('Expected value of each gamble')
    
def show2(colors, line):
    urnpairs = [(u1, u2) for u1 in urns for u2 in urns]
    urnpairs.sort(key=lambda urns: avgscore('B', urns))
    X = list(range(len(urnpairs)))
    plt.plot(X, [avgscore(colors, urns) for urns in urnpairs], line)
    
ellsburg2()"
"c = {
    '123': 0,
    '132': 0,
    '213': 0,
    '231': 0,
    '312': 0,
    '321': 0
}

for i in range(10000):
    c[naive('123')] += 1
    
print(c)"
"labels = list(sorted(c.keys()))
values = [c[x] for x in labels]
y_pos = np.arange(len(values))

plt.bar(y_pos, values, align='center', alpha=0.5, color='blue')
plt.xticks(y_pos, labels)
plt.title(""Naive Sort"")"
"c = {
    '123': 0,
    '132': 0,
    '213': 0,
    '231': 0,
    '312': 0,
    '321': 0
}

for i in range(10000):
    c[fisheryates('123')] += 1
    
print(c)"
"labels = list(sorted(c.keys()))
values = [c[x] for x in labels]
y_pos = np.arange(len(values))

plt.bar(y_pos, values, align='center', alpha=0.5, color='blue')
plt.xticks(y_pos, labels)
plt.title(""Fisher-Yates Sort"")"
"draw_de_bruijn_graph(create_de_bruijn_graph(2, 3))"
"res = de_bruijn_nx(10, 4)
res += res[:3]  # wrap around
print('Solution (overview): %s...%s' % (res[:50], res[-50:]))
print('Good solution?', check_solution(10, 4, res))"
"p = plot((f.subs(G,25),(t,0,1)),((f-25).subs(G,25),(t,1,2)),((f-50).subs(G,25),(t,2,3)), \
         ylabel = '$f = 25 t$ [V]', xlabel = '$t$ [sec]', xlim=(0,3), figsize=(10,8))"
"a[9], b[9]"
"fig,ax = plt.subplots(figsize=(10,8))
ax.bar(-.5+arange(1,10),c,alpha=.5)
ax.set_xticks([1,2,3,4,5,6,7,8,9,10])
ax.set_xlabel('$n$',fontsize=16)
ax.set_ylabel('Fourier coefficients $a_n^2 + b_n^2$',fontsize=16)"
"import warnings
warnings.filterwarnings('ignore')
%pylab inline
import matplotlib as mpl
mpl.rcParams['lines.linewidth']=2
mpl.rcParams['lines.color']='r'
mpl.rcParams['figure.figsize']=(12,8)
mpl.rcParams['font.size']=14
mpl.rcParams['axes.labelsize']=20"
"x = array([10.0,20.03,30.01,40.02,50.02,60.01,70.00,80.01])
y = array([1.62,2.04,4.05,2.85,3.84,3.81,4.86,5.02])
plot(x,y,'o')
xlim([0,90])
ylim([1,6])
xlabel('$x$'),ylabel('$y$')

# best fit seems to be linear
p = polyfit(x,y,1)
Y = polyval(p,x)
plot(x,Y,'--')
text(10,5,'Y = %4.3f x + %4.3f' % (p[0],p[1]))"
"n = len(x)
m = 1 # number of independent variables
nu = n - (m+1)
Syx = sqrt(sum((Y - y)**2)/nu)
print (Syx)"
"e = (Y-y)
plot(arange(1,n+1),e/Syx,'go')
plot([0,n+1],[-2,-2],'b-',[0,n+1],[2,2],'r-')
plot([3],e[2]/Syx,'rx',mfc='none',markersize=30)
xlim([0,n+1]),ylim([-3,3])
xlabel('$n$'),ylabel('$e/S_{yx}$')"
"u = numpy.ones(nx)      #numpy function ones()
u[int(.5 / dx):int(1 / dx + 1)] = 2  #setting u = 2 between 0.5 and 1 as per our I.C.s
print(u)"
"pyplot.plot(numpy.linspace(0, 2, nx), u);"
"pyplot.plot(numpy.linspace(0, 2, nx), u);"
"from IPython.display import YouTubeVideo
YouTubeVideo('iz22_37mMkk')"
YouTubeVideo('xq9YTcv-fQg')
YouTubeVideo('y2WaK7_iMRI')
linearconv(41) #convection using 41 grid points
linearconv(61)
linearconv(71)
linearconv(85)
linearconv(41)
linearconv(61)
linearconv(81)
linearconv(101)
linearconv(121)
"from IPython.display import YouTubeVideo
YouTubeVideo('Yw1YPBupZxU')"
"import warnings
warnings.filterwarnings('ignore')
import numpy                 #loading our favorite library
from matplotlib import pyplot    #and the useful plotting library
%matplotlib inline

nx = 41
dx = 2 / (nx - 1)
nt = 20    #the number of timesteps we want to calculate
nu = 0.3   #the value of viscosity
sigma = .2 #sigma is a parameter, we'll learn more about it later
dt = sigma * dx**2 / nu #dt is defined using sigma ... more later!


u = numpy.ones(nx)      #a numpy array with nx elements all equal to 1.
u[int(.5 / dx):int(1 / dx + 1)] = 2  #setting u = 2 between 0.5 and 1 as per our I.C.s

un = numpy.ones(nx) #our placeholder array, un, to advance the solution in time

for n in range(nt):  #iterate through time
    un = u.copy() ##copy the existing values of u into un
    for i in range(1, nx - 1):
        u[i] = un[i] + nu * dt / dx**2 * (un[i+1] - 2 * un[i] + un[i-1])
        
pyplot.plot(numpy.linspace(0, 2, nx), u);"
"from IPython.display import YouTubeVideo
YouTubeVideo('y2WaK7_iMRI')"
"x, nu, t = sympy.symbols('x nu t')
phi = (sympy.exp(-(x - 4 * t)**2 / (4 * nu * (t + 1))) +
       sympy.exp(-(x - 4 * t - 2 * sympy.pi)**2 / (4 * nu * (t + 1))))
phi"
"phiprime = phi.diff(x)
phiprime"
"ufunc = lambdify((t, x, nu), u)
print(ufunc(1, 4, 3))"
"from matplotlib import pyplot
%matplotlib inline

###variable declarations
nx = 101
nt = 100
dx = 2 * numpy.pi / (nx - 1)
nu = .07
dt = dx * nu

x = numpy.linspace(0, 2 * numpy.pi, nx)
un = numpy.empty(nx)
t = 0

u = numpy.asarray([ufunc(t, x0, nu) for x0 in x])
u"
"pyplot.figure(figsize=(11, 7), dpi=100)
pyplot.plot(x, u, marker='o', lw=2)
pyplot.xlim([0, 2 * numpy.pi])
pyplot.ylim([0, 10]);"
"pyplot.figure(figsize=(11, 7), dpi=100)
pyplot.plot(x,u, marker='o', lw=2, label='Computational')
pyplot.plot(x, u_analytical, label='Analytical')
pyplot.xlim([0, 2 * numpy.pi])
pyplot.ylim([0, 10])
pyplot.legend();"
"%%timeit
u = numpy.ones((ny, nx))
u[int(.5 / dy): int(1 / dy + 1), int(.5 / dx):int(1 / dx + 1)] = 2

for n in range(nt + 1): ##loop across number of time steps
    un = u.copy()
    row, col = u.shape
    for j in range(1, row):
        for i in range(1, col):
            u[j, i] = (un[j, i] - (c * dt / dx * 
                                  (un[j, i] - un[j, i - 1])) - 
                                  (c * dt / dy * 
                                   (un[j, i] - un[j - 1, i])))
            u[0, :] = 1
            u[-1, :] = 1
            u[:, 0] = 1
            u[:, -1] = 1"
"%%timeit
u = numpy.ones((ny, nx))
u[int(.5 / dy): int(1 / dy + 1), int(.5 / dx):int(1 / dx + 1)] = 2

for n in range(nt + 1): ##loop across number of time steps
    un = u.copy()
    u[1:, 1:] = (un[1:, 1:] - (c * dt / dx * (un[1:, 1:] - un[1:, 0:-1])) -
                              (c * dt / dy * (un[1:, 1:] - un[0:-1, 1:])))
    u[0, :] = 1
    u[-1, :] = 1
    u[:, 0] = 1
    u[:, -1] = 1"
"import warnings
warnings.filterwarnings('ignore')
from mpl_toolkits.mplot3d import Axes3D    ##New Library required for projected 3d plots

import numpy
from matplotlib import pyplot, cm
%matplotlib inline

###variable declarations
nx = 81
ny = 81
nt = 100
c = 1
dx = 2 / (nx - 1)
dy = 2 / (ny - 1)
sigma = .2
dt = sigma * dx

x = numpy.linspace(0, 2, nx)
y = numpy.linspace(0, 2, ny)

u = numpy.ones((ny, nx)) ##create a 1xn vector of 1's
un = numpy.ones((ny, nx)) ##

###Assign initial conditions

##set hat function I.C. : u(.5<=x<=1 && .5<=y<=1 ) is 2
u[int(.5 / dy):int(1 / dy + 1),int(.5 / dx):int(1 / dx + 1)] = 2 

###Plot Initial Condition
##the figsize parameter can be used to produce different sized images
fig = pyplot.figure(figsize=(11, 7), dpi=100)
ax = fig.gca(projection='3d')                      
X, Y = numpy.meshgrid(x, y)                            
surf = ax.plot_surface(X, Y, u[:], cmap=cm.viridis)

"
"u = numpy.ones((ny, nx))
u[int(.5 / dy):int(1 / dy + 1), int(.5 / dx):int(1 / dx + 1)] = 2

for n in range(nt + 1): ##loop across number of time steps
    un = u.copy()
    row, col = u.shape
    for j in range(1, row):
        for i in range(1, col):
            u[j, i] = (un[j, i] - (c * dt / dx * (un[j, i] - un[j, i - 1])) -
                                  (c * dt / dy * (un[j, i] - un[j - 1, i])))
            u[0, :] = 1
            u[-1, :] = 1
            u[:, 0] = 1
            u[:, -1] = 1

fig = pyplot.figure(figsize=(11, 7), dpi=100)
ax = fig.gca(projection='3d')
surf2 = ax.plot_surface(X, Y, u[:], cmap=cm.viridis)"
"u = numpy.ones((ny, nx))
u[int(.5 / dy):int(1 / dy + 1), int(.5 / dx):int(1 / dx + 1)] = 2

for n in range(nt + 1): ##loop across number of time steps
    un = u.copy()
    u[1:, 1:] = (un[1:, 1:] - (c * dt / dx * (un[1:, 1:] - un[1:, :-1])) -
                              (c * dt / dy * (un[1:, 1:] - un[:-1, 1:])))
    u[0, :] = 1
    u[-1, :] = 1
    u[:, 0] = 1
    u[:, -1] = 1

fig = pyplot.figure(figsize=(11, 7), dpi=100)
ax = fig.gca(projection='3d')
surf2 = ax.plot_surface(X, Y, u[:], cmap=cm.viridis)

    "
"from IPython.display import YouTubeVideo
YouTubeVideo('tUg_dE3NXoY')"
"fig = pyplot.figure(figsize=(11, 7), dpi=100)
ax = fig.gca(projection='3d')
X, Y = numpy.meshgrid(x, y)

ax.plot_surface(X, Y, u, cmap=cm.viridis, rstride=2, cstride=2)
ax.set_xlabel('$x$')
ax.set_ylabel('$y$');"
"fig = pyplot.figure(figsize=(11, 7), dpi=100)
ax = fig.gca(projection='3d')
X, Y = numpy.meshgrid(x, y)

ax.plot_surface(X, Y, u, cmap=cm.viridis, rstride=2, cstride=2)
ax.set_xlabel('$x$')
ax.set_ylabel('$y$');"
"fig = pyplot.figure(figsize=(11, 7), dpi=100)
ax = fig.gca(projection='3d')
X, Y = numpy.meshgrid(x, y)
ax.plot_surface(X, Y, v, cmap=cm.viridis, rstride=2, cstride=2)
ax.set_xlabel('$x$')
ax.set_ylabel('$y$');"
"from IPython.display import YouTubeVideo
YouTubeVideo('tUg_dE3NXoY')"
"fig = pyplot.figure()
ax = fig.gca(projection='3d')
X, Y = numpy.meshgrid(x, y)
surf = ax.plot_surface(X, Y, u, rstride=1, cstride=1, cmap=cm.viridis,
        linewidth=0, antialiased=False)

ax.set_xlim(0, 2)
ax.set_ylim(0, 2)
ax.set_zlim(1, 2.5)

ax.set_xlabel('$x$')
ax.set_ylabel('$y$');"
diffuse(10)
diffuse(14)
diffuse(50)
"from IPython.display import YouTubeVideo
YouTubeVideo('tUg_dE3NXoY')"
"###(plot ICs)
fig = pyplot.figure(figsize=(11, 7), dpi=100)
ax = fig.gca(projection='3d')
X, Y = numpy.meshgrid(x, y)
ax.plot_surface(X, Y, u[:], cmap=cm.viridis, rstride=1, cstride=1)
ax.plot_surface(X, Y, v[:], cmap=cm.viridis, rstride=1, cstride=1)
ax.set_xlabel('$x$')
ax.set_ylabel('$y$');"
"fig = pyplot.figure(figsize=(11, 7), dpi=100)
ax = fig.gca(projection='3d')
X, Y = numpy.meshgrid(x, y)
ax.plot_surface(X, Y, u, cmap=cm.viridis, rstride=1, cstride=1)
ax.plot_surface(X, Y, v, cmap=cm.viridis, rstride=1, cstride=1)
ax.set_xlabel('$x$')
ax.set_ylabel('$y$');"
"from IPython.display import YouTubeVideo
YouTubeVideo('tUg_dE3NXoY')"
"plot2D(x, y, p)"
"plot2D(x, y, p)"
"from IPython.display import YouTubeVideo
YouTubeVideo('ZjfxA3qq2Lg')"
"from IPython.display import YouTubeVideo
YouTubeVideo('iwL8ashXhWU')"
"plot2D(x, y, p)"
"from IPython.display import YouTubeVideo
YouTubeVideo('ZjfxA3qq2Lg')"
"%matplotlib inline

# plot the grid of points
width = 10.0
height = (y_end - y_start) / (x_end - x_start) * width
pyplot.figure(figsize=(width, height))
pyplot.xlabel('x', fontsize=16)
pyplot.ylabel('y', fontsize=16)
pyplot.xlim(x_start, x_end)
pyplot.ylim(y_start, y_end)
pyplot.scatter(X, Y, s=5, color='#CD2305', marker='o')"
"# plot the streamlines
width = 10.0
height = (y_end - y_start) / (x_end - x_start) * width
pyplot.figure(figsize=(width, height))
pyplot.xlabel('x', fontsize=16)
pyplot.ylabel('y', fontsize=16)
pyplot.xlim(x_start, x_end)
pyplot.ylim(y_start, y_end)
pyplot.streamplot(X, Y, u_source, v_source,
                  density=2, linewidth=1, arrowsize=2, arrowstyle='->')
pyplot.scatter(x_source, y_source,
               color='#CD2305', s=80, marker='o');"
"# plot the streamlines
width = 10.0
height = (y_end - y_start) / (x_end - x_start) * width
pyplot.figure(figsize=(width, height))
pyplot.xlabel('x', fontsize=16)
pyplot.ylabel('y', fontsize=16)
pyplot.xlim(x_start, x_end)
pyplot.ylim(y_start, y_end)
pyplot.streamplot(X, Y, u_sink, v_sink,
                  density=2, linewidth=1, arrowsize=2, arrowstyle='->')
pyplot.scatter(x_sink, y_sink,
               color='#CD2305', s=80, marker='o');"
"# compute the velocity of the pair source/sink by superposition
u_pair = u_source + u_sink
v_pair = v_source + v_sink

# plot the streamlines of the pair source/sink
width = 10.0
height = (y_end - y_start) / (x_end - x_start) * width
pyplot.figure(figsize=(width, height))
pyplot.xlabel('x', fontsize=16)
pyplot.ylabel('y', fontsize=16)
pyplot.xlim(x_start, x_end)
pyplot.ylim(y_start, y_end)
pyplot.streamplot(X, Y, u_pair, v_pair,
                  density=2.0, linewidth=1, arrowsize=2, arrowstyle='->')
pyplot.scatter([x_source, x_sink], [y_source, y_sink], 
               color='#CD2305', s=80, marker='o');"
"# superposition of the source on the freestream
u = u_freestream + u_source
v = v_freestream + v_source
psi = psi_freestream + psi_source

# plot the streamlines
width = 10
height = (y_end - y_start) / (x_end - x_start) * width
pyplot.figure(figsize=(width, height))
pyplot.grid(True)
pyplot.xlabel('x', fontsize=16)
pyplot.ylabel('y', fontsize=16)
pyplot.xlim(x_start, x_end)
pyplot.ylim(y_start, y_end)
pyplot.streamplot(X, Y, u, v, density=2, linewidth=1, arrowsize=1, arrowstyle='->')
pyplot.scatter(x_source, y_source, color='#CD2305', s=80, marker='o')

# calculate the stagnation point
x_stagnation = x_source - strength_source / (2 * numpy.pi * u_inf)
y_stagnation = y_source

# display the stagnation point
pyplot.scatter(x_stagnation, y_stagnation, color='g', s=80, marker='o')

# display the dividing streamline
pyplot.contour(X, Y, psi, 
               levels=[-strength_source / 2, strength_source / 2], 
               colors='#CD2305', linewidths=2, linestyles='solid');"
"# superposition of a source and a sink on the freestream
u = u_freestream + u_source + u_sink
v = v_freestream + v_source + v_sink
psi = psi_freestream + psi_source + psi_sink

# plot the streamlines
width = 10
height = (y_end - y_start) / (x_end - x_start) * width
pyplot.figure(figsize=(width, height))
pyplot.xlabel('x', fontsize=16)
pyplot.ylabel('y', fontsize=16)
pyplot.xlim(x_start, x_end)
pyplot.ylim(y_start, y_end)
pyplot.streamplot(X, Y, u, v,
                  density=2, linewidth=1, arrowsize=1, arrowstyle='->')
pyplot.scatter([x_source, x_sink], [y_source, y_sink],
               color='#CD2305', s=80, marker='o')
pyplot.contour(X, Y, psi,
               levels=[0.], colors='#CD2305', linewidths=2, linestyles='solid');"
"# compute the pressure coefficient field
cp = 1.0 - (u**2 + v**2) / u_inf**2

# plot the pressure coefficient field
width = 10
height = (y_end - y_start) / (x_end - x_start) * width
pyplot.figure(figsize=(1.1 * width, height))
pyplot.xlabel('x', fontsize=16)
pyplot.ylabel('y', fontsize=16)
pyplot.xlim(x_start, x_end)
pyplot.ylim(y_start, y_end)
contf = pyplot.contourf(X, Y, cp,
                        levels=numpy.linspace(-2.0, 1.0, 100), extend='both')
cbar = pyplot.colorbar(contf)
cbar.set_label('$C_p$', fontsize=16)
cbar.set_ticks([-2.0, -1.0, 0.0, 1.0])
pyplot.scatter([x_source, x_sink], [y_source, y_sink],
               color='#CD2305', s=80, marker='o')
pyplot.contour(X, Y, psi,
               levels=[0.], colors='#CD2305', linewidths=2, linestyles='solid');"
"# plot the streamlines
width = 10
height = (y_end - y_start) / (x_end - x_start) * width
pyplot.figure(figsize=(width, height))
pyplot.xlabel('x', fontsize=16)
pyplot.ylabel('y', fontsize=16)
pyplot.xlim(x_start, x_end)
pyplot.ylim(y_start, y_end)
pyplot.streamplot(X, Y, u_vortex, v_vortex,
                  density=2, linewidth=1, arrowsize=1, arrowstyle='->')
pyplot.scatter(x_vortex, y_vortex, color='#CD2305', s=80, marker='o');"
"# superposition of the sink and the vortex
u = u_vortex + u_sink
v = v_vortex + v_sink
psi = psi_vortex + psi_sink

# plot the streamlines
width = 10
height = (y_end - y_start) / (x_end - x_start) * width
pyplot.figure(figsize=(width, height))
pyplot.xlabel('x', fontsize=16)
pyplot.ylabel('y', fontsize=16)
pyplot.xlim(x_start, x_end)
pyplot.ylim(y_start, y_end)
pyplot.streamplot(X, Y, u, v, density=2, linewidth=1, arrowsize=1, arrowstyle='->')
pyplot.scatter(x_vortex, y_vortex, color='#CD2305', s=80, marker='o');"
"from IPython.display import YouTubeVideo
from datetime import timedelta

start=int(timedelta(hours=0, minutes=4, seconds=25).total_seconds())

YouTubeVideo(""loCLkcYEWD4"", start=start)"
"# plot the streamlines
width = 10
height = (y_end - y_start) / (x_end - x_start) * width
pyplot.figure(figsize=(width, height))
pyplot.xlabel('x', fontsize=16)
pyplot.ylabel('y', fontsize=16)
pyplot.xlim(x_start, x_end)
pyplot.ylim(y_start, y_end)
pyplot.streamplot(X, Y, u, v, density=2, linewidth=1, arrowsize=1, arrowstyle='->')
pyplot.scatter(x_doublet, y_doublet, color='#CD2305', s=80, marker='o')

# calculate the cylinder radius and add the cylinder to the figure
R = math.sqrt(kappa / (2 * math.pi * u_inf))
circle = pyplot.Circle((0, 0), radius=R, color='#CD2305', alpha=0.5)
pyplot.gca().add_patch(circle)

# calculate the stagnation points and add them to the figure
x_stagn1, y_stagn1 = +math.sqrt(kappa / (2 * math.pi * u_inf)), 0.0
x_stagn2, y_stagn2 = -math.sqrt(kappa / (2 * math.pi * u_inf)), 0.0
pyplot.scatter([x_stagn1, x_stagn2], [y_stagn1, y_stagn2],
               color='g', s=80, marker='o');"
"# calculate the cylinder radius
R = math.sqrt(kappa / (2 * math.pi * u_inf))

# calculate the stagnation points
x_stagn1, y_stagn1 = (+math.sqrt(R**2 - (gamma / (4 * math.pi * u_inf))**2),
                      -gamma / (4 * math.pi * u_inf))
x_stagn2, y_stagn2 = (-math.sqrt(R**2 - (gamma / (4 * math.pi * u_inf))**2),
                      -gamma / (4 * math.pi * u_inf))

# plot the streamlines
width = 10
height = (y_end - y_start) / (x_end - x_start) * width
pyplot.figure(figsize=(width, height))
pyplot.xlabel('x', fontsize=16)
pyplot.ylabel('y', fontsize=16)
pyplot.xlim(x_start, x_end)
pyplot.ylim(y_start, y_end)
pyplot.streamplot(X, Y, u, v,
                  density=2, linewidth=1, arrowsize=1.5, arrowstyle='->')
circle = pyplot.Circle((0.0, 0.0), radius=R, color='#CD2305', alpha=0.5)
pyplot.gca().add_patch(circle)
pyplot.scatter(x_vortex, y_vortex, color='#CD2305', s=80, marker='o')
pyplot.scatter([x_stagn1, x_stagn2], [y_stagn1, y_stagn2],
               color='g', s=80, marker='o');"
"# calculate the surface tangential velocity on the cylinder
theta = numpy.linspace(0.0, 2 * math.pi, 100)
u_theta = -2 * u_inf * numpy.sin(theta) - gamma / (2 * math.pi * R)

# compute the surface pressure coefficient
cp = 1.0 - (u_theta / u_inf)**2

# if there was no vortex
u_theta_no_vortex = -2 * u_inf * numpy.sin(theta)
cp_no_vortex = 1.0 - (u_theta_no_vortex / u_inf)**2

# plot the surface pressure coefficient
size = 6
pyplot.figure(figsize=(size, size))
pyplot.grid(True)
pyplot.xlabel(r'$\theta$', fontsize=18)
pyplot.ylabel('$C_p$', fontsize=18)
pyplot.xlim(theta.min(), theta.max())
pyplot.plot(theta, cp,
            label='with vortex', color='#CD2305', linewidth=2, linestyle='-')
pyplot.plot(theta, cp_no_vortex,
            label='without vortex', color='g', linewidth=2, linestyle='-')
pyplot.legend(loc='best', prop={'size':16});"
"from IPython.display import YouTubeVideo
YouTubeVideo('POHre1P_E1k')"
"# plot the streamlines
width = 4
height = (y_end - y_start) / (x_end - x_start) * width
pyplot.figure(figsize=(width, height))
pyplot.grid()
pyplot.xlabel('x', fontsize=16)
pyplot.ylabel('y', fontsize=16)
pyplot.streamplot(X, Y, u, v,
                  density=2, linewidth=1, arrowsize=1, arrowstyle='->')
# plot the sources
pyplot.scatter(x_source, y_source,
               color='#CD2305', s=80, marker='o')
# compute the velocity magniture and indices of the stagnation point
# note: the stagnation point is approximated as the point with the smallest velocity magnitude
magnitude = numpy.sqrt(u**2 + v**2)
j_stagn, i_stagn = numpy.unravel_index(magnitude.argmin(), magnitude.shape)
# plot the stagnation point
pyplot.scatter(x[i_stagn], y[j_stagn],
               color='black', s=40, marker='D')
pyplot.xlim(x_start, x_end)
pyplot.ylim(y_start, y_end);"
"# plot the streamlines
width = 4
height = (y_end - y_start) / (x_end - x_start) * width
pyplot.figure(figsize=(width, height))
pyplot.grid()
pyplot.xlabel('x', fontsize=16)
pyplot.ylabel('y', fontsize=16)
pyplot.streamplot(X, Y, u, v,
                  density=2, linewidth=1, arrowsize=1, arrowstyle='->')
# plot the source sheet
pyplot.axvline(0.0,
               (y_min - y_start) / (y_end - y_start),
               (y_max - y_start) / (y_end - y_start),
               color='#CD2305', linewidth=4)
# compute the velocity magniture and indices of the stagnation point
# note: stagnation point approximated as point with smallest velocity magnitude
magnitude = numpy.sqrt(u**2 + v**2)
j_stagn, i_stagn = numpy.unravel_index(magnitude.argmin(), magnitude.shape)
# plot the stagnation point
pyplot.scatter(x[i_stagn], y[j_stagn],
               color='black', s=40, marker='D')
pyplot.xlim(x_start, x_end)
pyplot.ylim(y_start, y_end);"
"data = rnd.normal(loc=5, scale=2, size=100)  # Array with 100 values
print('mean of data: ', np.mean(data))
print('standard deviation of data: ', np.std(data))"
"data = rnd.normal(loc=6, scale=2, size=100)
hist_data = plt.hist(data)
plt.xlabel('bins')
plt.ylabel('number of data points')
print('number of data points in each bin:', hist_data[0])
print('limits of the bins:', hist_data[1])"
"hist_data = plt.hist(data, bins=12, range=(0, 12))
print('number of data points in each bin:', hist_data[0])
print('limits of the bins:', hist_data[1])
plt.xlabel('bins')
plt.ylabel('number of data points');"
"from scipy.stats import norm
a = plt.hist(data, bins=12, range=(0, 12), density=True)
x = np.linspace(0, 12, 100)
y = norm.pdf(x, loc=6, scale=2) # mu=6, sig=2
plt.plot(x, y, 'r')
plt.xlabel('value')
plt.ylabel('probability');"
"data = rnd.normal(loc=10, scale=2, size=100)
lower, median, upper = np.percentile(data, [2.5, 50, 97.5])
print('2.5 percentile:', lower)
print('50 percentile:', median)
print('97.5 percentile:', upper)
print('95% interval:', lower, ' to ', upper)"
"rnd.seed(10)
data = rnd.normal(loc=10, scale=2, size=500)
plt.boxplot(data);"
"data.hist(figsize=(10, 4), density=True, sharex=True, sharey=True);"
"data.test1[5] = np.nan  # Replace the value of test1 with index 5 to nan
data.hist(cumulative=True, sharex=True, figsize=(10, 4), density=True);"
data.boxplot();
"plt.figure(figsize=(10, 4))
ax1 = plt.subplot(121)
data.hist(column='test1', ax=ax1)  # Makes histogram of column test1
ax2 = plt.subplot(122)
data.boxplot(column='test2', ax=ax2);  # Makes boxplot of column test2"
"from scipy.stats import norm
mu = 100
sig = 10
data = rnd.normal(loc=mu, scale=sig, size=1000)
print('mean of data is:', np.mean(data))
print('standard deviation of data is:', np.std(data))
plt.subplot(211)
a = plt.hist(data, bins=20, range=(50, 150), density=True)
x = np.linspace(50, 150, 100)
y = norm.pdf(x, loc=mu, scale=sig)
plt.plot(x, y, 'r')
plt.xlim(50, 150)
plt.ylabel('probability')
plt.subplot(212)
b = plt.hist(data, bins=20, range=(50, 150), cumulative=True, \
             density=True, align='right')
y = norm.cdf(x, mu, sig)
plt.plot(x, y, 'r')
plt.xlim(50, 150)
plt.xlabel('bins')
plt.ylabel('probability');"
"mu = 20
sig = 4
p25, p75 = norm.ppf([0.25, 0.75], loc=mu, scale=sig)
print('IQR pdf:', p25, p75)
data = rnd.normal(loc=mu, scale=sig, size=100)
d25, d75 = np.percentile(data, [25, 75])
print('IQR of data ', d25, d75)
plt.hist(data, bins=20, cumulative=True, density=True, align='right')
plt.axvline(d25, color='r')
plt.axvline(d75, color='r')
plt.axvline(p25, color='k')
plt.axvline(p75, color='k')
plt.xlabel('bins')
plt.ylabel('cumulative number of data points');"
"print('mean moisture content: ', w.moisture.mean())
print('standard deviation of moisture content: ', w.moisture.std())
a = plt.boxplot(w.moisture)"
"plt.boxplot(w.moisture[w.moisture < 30])
plt.ylim(10.5, 17);"
"w.hist(column='bstrength', density=True)
plt.xlabel('bending strength (N/m$^2$)')
five = w.bstrength.quantile(0.05)
print('5 empirical percentile: ', five)
plt.axvline(five, color='r');"
"from scipy.stats import norm
w.hist(column='bstrength', density=True)
meanstrength = w.bstrength.mean()
stdstrength = w.bstrength.std()
x = np.linspace(0, 100, 100)
y = norm.pdf(x, loc=meanstrength, scale=stdstrength)
plt.plot(x, y, 'r')
plt.axvline(five, color='r')
plt.axvline(meanstrength - 1.64 * stdstrength, color='k');"
"for i in range(5):
    a = 2 * rnd.standard_normal(100) + 4
    print('mean a:', np.mean(a))"
"from scipy.stats import norm
xvalue_05 = norm.ppf(0.05)
xvalue_95 = norm.ppf(0.95)
print('5% limit:', xvalue_05)
print('95% limit:', xvalue_95)
print('check if it works for 5%:', norm.cdf(xvalue_05))
print('check if it works for 95%:', norm.cdf(xvalue_95))
# Next, specify a mean and standard deviation
xvalue_05_musig = norm.ppf(0.05, loc=20, scale=10) # mu = 20, sigma = 10
print('5% limit with mu=20, sig=10:', xvalue_05_musig)
print('check:', norm.cdf(xvalue_05_musig, loc=20, scale=10))"
"from scipy.stats import t
xvalue_05 = t.ppf(0.05, 39, loc=20, scale=2)
xvalue_95 = t.ppf(0.95, 39, loc=20, scale=2)
print('5% limit:  ',xvalue_05)
print('95% limit: ',xvalue_95)
print('check if it works for 5%:', t.cdf(xvalue_05, 39, loc=20, scale=2))
print('check if it works for 95%:', t.cdf(xvalue_95, 39, loc=20, scale=2))"
"print('95 percentile Standard Normal:  ',norm.ppf(0.95))
print('95 percentile t-dist with n=99: ',t.ppf(0.95,99)) 
x = np.linspace(-4,4,100)
y1 = norm.pdf(x)
y2 = t.pdf(x,99)
plt.plot(x,y1,'b',label='Normal')
plt.plot(x,y2,'r',label='t-dist')
plt.legend();"
"from scipy.stats import gamma
x = np.linspace(1e-6, 10, 100)
y = gamma.pdf(x, 2, scale=1)
plt.plot(x, y)
plt.axvline(2, color='r');"
"x = np.linspace(1e-6, 10, 100)
y = gamma.pdf(x, 2)
plt.plot(x, y)
plt.axvline(2, color='r')
data = gamma.rvs(2, size=1000)
plt.hist(data, bins=20, normed=True);"
"rnd.seed(22)
mean_of_data = np.mean(2 * rnd.standard_normal((1000, 100)) + 4, 1)
print('The mean of the means is:', np.mean(mean_of_data))
print('The standard deviation of the means is:', np.std(mean_of_data, ddof=1))
plt.figure()
plt.boxplot(mean_of_data)
plt.ylim(3, 5)
plt.figure()
plt.hist(mean_of_data, normed=True)
plt.xlim(3,5)"
"rnd.seed(22)
mean_of_data = np.mean(2 * rnd.standard_normal((1000, 1000)) + 4, 1)
print('The mean of the means is:', np.mean(mean_of_data))
print('The standard deviation of the means is:', np.std(mean_of_data, ddof=1))
plt.figure()
plt.boxplot(mean_of_data)
plt.ylim(3,5)
plt.figure()
plt.hist(mean_of_data)
plt.xlim(3, 5)"
"rnd.seed(2)
data = 4 * rnd.standard_normal(20) + 39
mu = np.mean(data)
sig = np.std(data, ddof=1)
sighat = np.std(data, ddof=1) / np.sqrt(20)
print('mean of the data:', mu)
print('std of the data:', sig)
print('std of the mean:', sighat)"
"x = np.linspace(37, 43, 100)
y = t.pdf(x, 19, loc=40, scale=sighat)
plt.plot(x, y)
perc025 = t.ppf(0.025, 19, loc=40, scale=sighat)
perc975 = t.ppf(0.975, 19, loc=40, scale=sighat)
plt.axvline(perc025, color='r')
plt.axvline(perc975, color='r')
plt.axvline(mu, color='k', lw=5)
plt.title('H0 cannot be rejected');"
"from pandas import read_csv
w = read_csv('douglas_data.csv', skiprows=[1], skipinitialspace=True)
mu20 = np.mean(w.bstrength[:20])
sig20 = np.std(w.bstrength[:20], ddof=1) / np.sqrt(20)
print('sample mean, standard deviation of sample mean: ', mu20, sig20)
x = np.linspace(30,70,100)
y = t.pdf(x, 19, loc=50, scale=sig20)
plt.plot(x,y)
perc025 = t.ppf(0.025, 19, loc=50, scale=sig20)
perc975 = t.ppf(0.975, 19, loc=50, scale=sig20)
plt.axvline(perc025, color='r')
plt.axvline(perc975, color='r')
plt.axvline(mu20, color='k', lw=4)
plt.title('H0 is rejected: mean is not 50 Pa');"
"from pandas import read_csv
w = read_csv('douglas_data.csv', skiprows=[1], skipinitialspace=True)
N = len(w.bstrength)
mu = np.mean(w.bstrength)
sig = np.std(w.bstrength, ddof=1) / np.sqrt(N)
print('sample mean, standard deviation of sample mean: ', mu, sig)
x = np.linspace(30, 70, 100)
y = t.pdf(x, N - 1, loc=50, scale=sig)
plt.plot(x, y)
perc025 = t.ppf(0.025, N - 1, loc=50, scale=sig)
perc975 = t.ppf(0.975, N - 1, loc=50, scale=sig)
plt.axvline(perc025, color='r')
plt.axvline(perc975, color='r')
plt.axvline(mu, color='k', lw=4)
plt.title('Not enough evidence to reject H0: mean may very well be 50');"
"from scipy.stats import norm, gamma
for N in [100, 1000, 10000]:
    data = gamma.rvs(2, size=(N, 20))
    mean_of_data = np.mean(data, 1)
    mu = np.mean(mean_of_data)
    sig = np.std(mean_of_data, ddof=1)
    plt.figure()
    plt.hist(mean_of_data, bins=20, normed=True)
    x = np.linspace(0, 4, 100)
    y = norm.pdf(x, loc=mu, scale=sig)
    plt.plot(x, y, 'r')
    plt.title('N=' + str(N))"
"def square_me(func, x):
    return func(x) ** 2
print('result of square_me function:', square_me(np.cos, 4))
print('directly taking the square  :', np.cos(4) ** 2)"
"def f(x):
    return 0.5 - np.exp(-x)

x = np.linspace(0, 4, 100)
y = f(x)
plt.plot(x, y)
plt.axhline(0, color='r', ls='--')"
"xzero = bisection(func=f, x1=0, x2=4, nmax=20)  
print('zero of function and function value: ', xzero, f(xzero))  "
"print('starting at x=1')
xzero = newtonsmethod(func=f, funcp=fp, xs=1)
print('xzero, f(xzero) ', xzero, f(xzero))

print('starting at x=4')
xzero = newtonsmethod(func=f, funcp=fp, xs=4, nmax=40)
print('xzero, f(xzero) ', xzero, f(xzero))"
"xzero = newtonsmethod(func=np.sin, funcp=np.cos, xs=1)
print('starting point is x=1')
print('xzero, sin(xzero):', xzero, np.sin(xzero))

xzero = newtonsmethod(func=np.sin, funcp=np.cos, xs=1.5)
print('starting point is x=1.5')
print('xzero, sin(xzero):', xzero, np.sin(xzero))
print('xzero / pi:', xzero / np.pi)"
"from scipy.optimize import fsolve
def h(x):
    return np.log(x ** 2) - 2

x0 = fsolve(h, 1)
print('x0, function value', x0, h(x0))"
"from scipy.optimize import fsolve
def g(x):
    return x + 2 * np.cos(x)
x = np.linspace(-2, 4, 100)
x0 = fsolve(g, 1)
plt.plot(x, g(x))
plt.plot(x0, g(x0), 'ro')
plt.axhline(y=0, color='r')"
"xp = np.array([-2, 1, 4])
yp = np.array([2, -1, 4])
A = np.zeros((3, 3))
rhs = np.zeros(3)
for i in range(3):
    A[i] = xp[i] ** 2, xp[i], 1  # Store one row at a time
    rhs[i] = yp[i]
print('Array A:')
print(A)
print('rhs:',rhs)"
"plt.plot(xp, yp, 'ro')
x = np.linspace(-3, 5, 100)
y = sol[0] * x ** 2 + sol[1] * x + sol[2]
plt.plot(x, y, 'b');"
"import scipy.sparse as sp
A = sp.diags([1 * np.ones(3), 
              2 * np.ones(4), 
              3 * np.ones(3)], 
             [-1, 0, 1], format='csc')
print('Sparse matrix A')
print(A) # Gives the way A is stored: row, column, value
print('Full matrix A as an array')
print(A.toarray())  # Returns the equivalent full array"
"from scipy.sparse.linalg import spsolve
print('type of A:', type(A))
b = np.arange(4)
x = spsolve(A,b)  # x is solution of Ax=b
print('right-hand-side defined as:', b)
print('verify A @ x gives same:', A @ x)"
"tp = np.array([0, 0.25, 0.5, 0.75])
yp = np.array([ 3, 1, -3, 1])
A = np.zeros((4, 4))
rhs = np.zeros(4)
for i in range(4):
    A[i] = np.cos(1 * np.pi * tp[i]), np.cos(2 * np.pi * tp[i]), \
           np.cos(3 * np.pi * tp[i]), np.cos(4 * np.pi * tp[i])  # Store one row at a time
    rhs[i] = yp[i]
sol = np.linalg.solve(A, rhs)
print('a,b,c,d: ',sol)

t = np.linspace(0, 1, 100)
y = sol[0] * np.cos(1 * np.pi * t) + sol[1] * np.cos(2 * np.pi * t) + \
    sol[2] * np.cos(3 * np.pi * t) + sol[3] * np.cos(4 * np.pi * t)
plt.plot(t, y, 'b', label='wave')
plt.plot(tp, yp, 'ro', label='data')
plt.legend(loc='best');"
"def fpoly(x, N):
    rv = np.zeros(N + 1)
    for n in range(N + 1):
        rv[n] = x ** n
    return rv

print(fpoly(2, 4))"
"x = np.linspace(-1,4,100)
y = fpolyeval(x, a)
plt.plot(xp, yp, 'ko', label='data')
plt.plot(x, y, label='fitted poly')
plt.legend(loc='best');"
"k = 10
D = 10
h1star = 20
h2star = 22
L = 1000
P = 0.001
N = 40

d0 = -2 * np.ones(N + 1)  # main diagonal
d0[0] = 1  # first value of main diagonal is 1
d0[-1] = 1 # last value of main diagonal is 1
dplus1 = np.ones(N) # diagonal right above main diagonal, position 1
dplus1[0] = 0    # first value of diagonal is 0
dmin1 = np.ones(N)  # diagonal right below main diagonal, position -1
dmin1[-1] = 0    # last value of diagonal is 0
A = np.diag(d0, 0) + np.diag(dplus1, 1) + np.diag(dmin1, -1)
# Right hand side
delx = L / N
rhs = -P * delx ** 2 / (k * D) * np.ones(N + 1)
rhs[0] = h1star
rhs[-1] = h2star
# Solve for the head and plot
h = np.linalg.solve(A, rhs)
x = np.linspace(0, L, N + 1)
plt.plot(x, h)
plt.xlabel('x (m)')
plt.ylabel('head (m)')
print('maximum head ', np.max(h))"
"cv = 1e-6  # m^2/s
h = 2  # m
N = 40
delt = 4e4  # seconds
#
delz = h / N
mu = delz ** 2 / (cv * delt)

d0 = -(2 + mu) * np.ones(N + 1)
d0[-1] = 1
dp1 = np.ones(N)
dp1[0] = 2
dm1 = np.ones(N)
dm1[-1] = 0
A = np.diag(d0) + np.diag(dp1, 1) + np.diag(dm1, -1)

p = np.ones(N + 1)
for i in range(5):
    for j in range(10):
        rhs = -mu * p
        rhs[-1] = 0
        p = np.linalg.solve(A, rhs)
    plt.plot(np.arange(0, h + 0.01, delz), p, label=str((i + 1) * 10) + ' timesteps')
    print('maximum value after', 10 * ( i + 1), 'timesteps is:', p[0])
plt.xlim(0, 2)
plt.legend(loc='best')
plt.xlabel('z')
plt.ylabel('p')
plt.yticks(np.linspace(0, 1, 11))
plt.title('Consolidation');"
"cv = 1e-6  # m^2/s
h = 2  # m
N = 40
delt = 4e4  # seconds
#
delz = h / N
mu = delz ** 2 / (cv * delt)

d0 = -(2 + mu) * np.ones(N + 1)
d0[-1] = 1
dp1 = np.ones(N)
dp1[0] = 2
dm1 = np.ones(N)
dm1[-1] = 0
A = np.diag(d0) + np.diag(dp1, 1) + np.diag(dm1, -1)
Ainv = np.linalg.inv(A)

p = np.ones(N + 1)
for i in range(5):
    for j in range(10):
        rhs = -mu * p
        rhs[-1] = 0
        p = Ainv @ rhs
    plt.plot(np.arange(0, h + 0.01, delz), p, label=str((i + 1) * 10) + ' timesteps')
    print('maximum value after', 10 * ( i + 1), 'timesteps is:', p[0])
plt.xlim(0, 2)
plt.legend(loc='best')
plt.xlabel('z')
plt.ylabel('p')
plt.yticks(np.linspace(0, 1, 11))
plt.title('Consolidation');"
"from scipy.sparse import diags
from scipy.sparse.linalg import spsolve 
k = 10
D = 10
h1star = 42
h2star = 40
L = 1000
P = 0.001
N = 10000
d0 = -2 * np.ones(N + 1)  # main diagonal
d0[0] = 1  # first value of main diagonal is 1
d0[-1] = 1 # last value of main diagonal is 1
dplus1 = np.ones(N) # diagonal right above main diagonal, position 1
dplus1[0] = 0    # first value of diagonal is 0
dmin1 = np.ones(N)  # diagonal right below main diagonal, position -1
dmin1[-1] = 0    # last value of diagonal is 0
A = diags([dmin1, d0, dplus1], [-1, 0, 1], format='csc')
# Right hand side
delx = L / N
rhs = -P * delx ** 2 / (k * D) * np.ones(N + 1)
rhs[0] = h1star
rhs[-1] = h2star
h = spsolve(A, rhs)
plt.plot(h)
plt.xlabel('node number')
plt.ylabel('head (m)');"
tran.plot();  # plot all columns
tran['bus'].plot(kind='bar');
"rain = read_csv('rotterdam_rainfall_2012.txt', skiprows=9,
                parse_dates=['YYYYMMDD'], index_col='YYYYMMDD',
                skipinitialspace=True)
rain.RH[rain.RH<0] = 0  # remove negative values
rain.RH = rain.RH * 0.1 # convert to mm/day
monthlyrain = rain.RH.resample('M', kind='period').sum()
display(monthlyrain)
monthlyrain.plot(kind='bar')
plt.ylabel('mm/month')
plt.xlabel('month');"
"rain['totalq'] = rain.precip * rain.area * 1e-3
#
print('Five countries with largest annual influx:')
rain.sort_values(by='totalq', ascending=False, inplace=True)
print(rain[:5])
#
rain.totalq[:10].plot(kind='bar');"
"rain = read_csv('rotterdam_rainfall_2012.txt', skiprows=9,
                parse_dates=['YYYYMMDD'], skipinitialspace=True)
# convert to mm/d
rain.iloc[:,2] = rain.iloc[:,2] * 0.1
# set negative values to zero
rain.loc[rain.RH < 0, 'RH'] = 0
rain.RH.plot()
plt.xlabel('day')
plt.ylabel('daily rainfall (mm/day)')
plt.xlim(0, 365)
print('Maximum daily rainfall', rain.RH.max())
print('Date of maximum daily rainfall', rain.YYYYMMDD[rain.RH.idxmax()])"
"monthlyrain = np.zeros(12)
for i in range(len(rain)):
    month = rain.iloc[i,1].month
    monthlyrain[month - 1] += rain.iloc[i, 2]
print(monthlyrain)
#
plt.bar(np.arange(12), monthlyrain, width=0.8)
plt.xlabel('month')
plt.ylabel('monthly rainfall (mm/month)')
plt.xticks(np.arange(12), ['J', 'F', 'M', 'A', 'M', 'J', 'J', 'A', 'S', 'O', 'N', 'D']);"
"weather = read_csv('rotterdam_weather_2000_2010.txt', skiprows=11,
                parse_dates=['YYYYMMDD'], index_col='YYYYMMDD', skipinitialspace=True)
weather.TG = 0.1 * weather.TG
weather.RH = 0.1 * weather.RH
weather.EV24 = 0.1 * weather.EV24
weather.loc[weather.RH < 0, 'RH'] = 0
yearly_rain = weather.RH.resample('A', kind='period').sum()
yearly_evap = weather.EV24.resample('A', kind='period').sum()
yearly_temp = weather.TG.resample('A', kind='period').mean()
ax1 = yearly_rain.plot()
ax1 = yearly_evap.plot()
plt.ylabel('Rain/evap (mm/year)')
ax2 = yearly_temp.plot(secondary_y=True)
plt.xlabel('Year')
plt.ylabel('Mean yearly temperature (deg C)')
plt.legend(ax1.get_lines() + ax2.get_lines(),
           ['rain', 'evap', 'temp'], loc='best');"
"rnd.randint(0, 1+1, 10)"
"rnd.randint(0, 1+1, 10)"
"plt.bar(range(0, 3), number_of_tails)
plt.xticks(range(0, 3))
plt.xlabel('number of tails')
plt.ylabel('occurence in 100 trials');"
"plt.bar(range(0, 3), cum_prob, width=1)
plt.xticks(range(0, 3))
plt.xlabel('number of tails in two flips')
plt.ylabel('cumulative probability');"
"N = 1000
tails = np.sum(rnd.randint(0, 1+1, (5, 1000)), axis=0)
counttails = np.zeros(6, dtype='int')
for i in range(6):
    counttails[i] = np.count_nonzero(tails == i)
plt.bar(range(0, 6), counttails / N)
plt.xlabel('number of tails in five flips')
plt.ylabel('probability');"
"cumprob = np.cumsum(counttails / N)
print('cumprob:', cumprob)
plt.bar(range(0, 6), cumprob, width=1)
plt.xlabel('number of tails in five flips')
plt.ylabel('cumulative probability');"
"dice = rnd.randint(1, 6+1, (2, 1000))
highest_dice = np.max(dice, 0)
outcome = np.zeros(6)
for i in range(6):
    outcome[i] = np.sum(highest_dice == i+1) / 1000
plt.bar(np.arange(1, 7), height=outcome, width=1)
plt.xlabel('highest dice in two throws')
plt.ylabel('probability');"
"coins = data.coins()
edges = filters.sobel(coins)

plt.imshow(edges, cmap='gray');"
"# You can ignore the code below--it's just
# to make a pretty plot of the results.
fig, ax = plt.subplots(figsize=(10, 5))
ax.plot(y, '-o', label='Image slice', linewidth=3)
ax.plot(seed_positions, np.zeros_like(seed_positions), 'r^',
        label='Seeds', markersize=15)

for n, label in enumerate(np.unique(result)):
    mask = (result == label)
    ax.bar(x[mask][:-1], result[mask][:-1],
           width=1, label='Region %d' % n,
           alpha=0.1)

ax.vlines(np.argwhere(np.diff(result)) + 0.5, -0.2, 4.1, 'm',
          linewidth=3, linestyle='--')

from scipy.interpolate import interp1d

#c = interp1d(x, y, kind='cubic')
#t = np.linspace(0, len(y) - 1, 100)
#ax.plot(t, c(t), 'g')

ax.legend(loc='upper left', numpoints=1)
ax.axis('off')
ax.set_ylim(-0.2, 4.1);"
"threshold = filters.threshold_otsu(edges)
print(threshold)
# Euclidean distance transform
# How far do we have to travel from a non-edge to find an edge?
non_edges = (edges < threshold)
distance_from_edge = ndi.distance_transform_edt(non_edges)

plt.imshow(distance_from_edge, cmap='viridis');"
"from skimage import feature

# -------------------------------------------------#
peaks = feature.peak_local_max(distance_from_edge, min_distance=10)
print(""Peaks shape:"", peaks.shape)
# -------------------------------------------------#

peaks_image = np.zeros(coins.shape, np.bool)
peaks_image[tuple(np.transpose(peaks))] = True
seeds, num_seeds = ndi.label(peaks_image)

plt.imshow(edges, cmap='gray')
plt.plot(peaks[:, 1], peaks[:, 0], 'ro');
plt.axis('image')"
"ws = watershed(edges, seeds)

from skimage import color
plt.imshow(color.label2rgb(ws, coins));"
"from skimage import segmentation
plt.imshow(segmentation.mark_boundaries(coins, seg_coins))"
"%matplotlib inline
sns.barplot(x=x1, y=y1)

"
"sns.barplot(x=x2, y=y2)"
"import warnings
warnings.filterwarnings('ignore')
import matplotlib.pyplot as plt
import numpy as np

%matplotlib inline

x = np.arange(-5, 5.1, 0.1)

def sigmoid(vec):
    return 1/(1+np.exp(-x))
    
def tanh_activation(vec):
    return (np.exp(x) - np.exp(-x))/(np.exp(x) + np.exp(-x))

def rectlinear(vec):
    output = vec.copy()
    output[vec<0] = 0
    return output

plt.figure(figsize=(8,8))
plt.subplot(2,2,1)
plt.plot(x, x)
plt.subplot(2,2,2)
plt.plot(x, sigmoid(x))
plt.subplot(2,2,3)
plt.plot(x, tanh_activation(x))
plt.subplot(2,2,4)
plt.plot(x, rectlinear(x))
plt.show()"
"sigma = np.array([[0.14,-0.1],[-0.1,0.14]])
mu1 = np.array([-0.4,0.2])
mu2 = np.array([0.4,0.5])

d1 = np.random.multivariate_normal(mean=mu1, cov=sigma, size=200)
d2 = np.random.multivariate_normal(mean=mu2, cov=sigma, size=200)

x1 = np.concatenate((d1[:,0], d2[:,0]))
x2 = np.concatenate((d1[:,1], d2[:,1]))
labels = np.concatenate((['Positive']*d1.shape[0], 
                         ['Negative']*d2.shape[0]))

df = pandas.DataFrame(dict(x1=x1, x2=x2, y=labels))

df = df.reindex(np.random.permutation(df.index))

sns.lmplot(""x1"", ""x2"", hue=""y"", data=df, fit_reg=False)"
""
"import warnings
warnings.filterwarnings('ignore')
import matplotlib.pyplot as plt
import math
%matplotlib inline

def idf1(N, df):
    return math.log10(1. * N / df)
def idf2(N, df):
    return math.log10(1. * (N - df) / df)

N = 1000000
dfs = range(1,100000)[::1000]
plt.figure()
idfs1 = [idf1(N, df) for df in dfs]
idfs2 = [idf2(N, df) for df in dfs]
plt.plot(dfs, [idf1(N, df) for df in dfs], 'bo-')
plt.plot(dfs, [idf2(N, df) for df in dfs], 'r.')
plt.show()
print(idfs1[:10] + idfs1[-10:])
print(idfs2[:10] + idfs2[-10:])"
"print(ngrams(1, ['a b c']))
print(ngrams(2, ['a b c']))
print(ngrams(3, ['a b c']))
print(ngrams(4, ['a b c']))"
"%pylab inline
# 1-grams, 1-grams + 2-grams, ...
x = [sum(sizes[:i]) for i in range(1,max_n)]
print(x)
plot(x)
xlabel('ngram size')
ylabel('number of terms')"
"def doc2model_smooth(doc, smooth_term, vocab):
    """"""Convert a document d into a language model M_d using Laplacian (+1) smoothing.""""""
    counts = Counter(doc)
    for term in vocab:
        counts[term] = (counts[term] + smooth_term) / (1. * len(doc) + smooth_term * len(vocab))
    return counts

vocab = ['the', 'united', 'states', 'won', 'nine', 'gold', 'medals', 'in', 'olympics']

m_d = doc2model_smooth(['the', 'united', 'states', 'won', 'nine', 'gold', 'medals',
                        'in', 'the', 'olympics'],
                       smooth_term=1, vocab=vocab)
m_d"
"# Use same function to create a query model.
m_q = doc2model_smooth(['the', 'olympics'], smooth_term=1, vocab=vocab)
m_q"
"def mse(m_q, m_d):
    terms = m_q.keys() | m_d.keys()
    return np.mean(([(m_q[t] - m_d[t])**2 for t in terms]))

mse(m_q, m_d)"
"def kl(m_q, m_d):
    return np.mean(([ m_q[t] * math.log2(m_q[t] / m_d[t]) for t in m_q]))
kl(m_q, m_d)"
"def entropy(p):
    return -sum(pi * math.log2(pi) for pi in p.values())

probs = np.arange(1,10) / 10
entropies = [entropy({'a': p, 'b': 1 - p}) for p in probs]
plt.figure()
plt.plot(probs, entropies)
plt.xlabel('P(a)', size=16)
plt.ylabel('H(P)', size=16)
plt.title('Entropy is maximized when P(a)=P(b)=.5', size=15)
plt.show()"
"# How do these different measures compare?

def plot_measures(m_q2):
    maes = []
    mses = []
    cosines = []
    kls = []

    # Loop over possible document distributions.
    probs = np.arange(1,10) / 10
    for pi in probs:
        m_d2 = {'a': pi, 'b': 1 - pi}
        print(m_d2)
        maes.append(mae(m_q2, m_d2))
        mses.append(mse(m_q2, m_d2))
        cosines.append(cosine(m_q2, m_d2))
        kls.append(kl(m_q2, m_d2))

    # plot.
    plt.figure(figsize=(10,5))
    plt.plot(probs, maes, label='mae')
    plt.plot(probs, mses, label='mse')
    plt.plot(probs, cosines, label='1-cosine')
    plt.plot(probs, kls, label='kl')
    plt.xlabel('$P(a|M_d)$', size=16)
    plt.ylabel('distance($M_q, M_d$)', size=16)
    plt.legend(loc='best')
    plt.show()
    
plot_measures({'a': .5, 'b': .5})"
"plot_measures({'a': .9, 'b': .1})"
"print(hash('dog'))
print(hash('cat'))
# See the Python's implementation of hash here: 
# http://stackoverflow.com/questions/2070276/where-can-i-find-source-or-algorithm-of-pythons-hash-function"
"# What's a defaultdict?
from collections import defaultdict
d = {}
# d['zebra'] # KeyError
# Adding default values is a bit of a pain...
if 'zebra' in d:
    d['zebra'] += 1
else:
    d['zebra'] = 0


# defaultdict does it for you:
d2 = defaultdict(lambda: 0)
d2['zebra'] = 10
d2['cat'] = 20
d2['dog']  # NB: Looking up a key inserts it in the default dict.
print('mouse=', d2['mouse'])
print(d2)

# value is the length of the dictionary
d3 = defaultdict(lambda: len(d3))
d3['zebra']
d3['cat']
d3['dog']  
print(d3)"
"# This is what linear looks like....
%pylab inline
plot([54 * x + 25 for x in range(20)])"
"import warnings
warnings.filterwarnings('ignore')
import matplotlib.pyplot as plt
%matplotlib inline

plt.figure(figsize=(8,4))
plt.scatter([0,0], [0, 1], color='b')
plt.scatter([10,10], [0, 1], color='r', marker='^')
plt.xlim(-1, 11)
plt.ylim(-1, 11)
plt.xlabel('term 1')
plt.ylabel('term 2')
plt.show()

plt.figure(figsize=(8,4))
plt.scatter([0,0], [0, 1], color='b')
plt.scatter([10,10], [0, 1], color='r', marker='^')
plt.annotate('$\mu_1$', xy=(5,1), size=20)
plt.scatter([5], [1], color='k')
plt.annotate('$\mu_2$', xy=(5,0), size=20)
plt.scatter([5], [0], color='k')
plt.xlim(-1, 11)
plt.ylim(-1, 11)
plt.xlabel('term 1')
plt.ylabel('term 2')
plt.show()

plt.figure(figsize=(8,4))
plt.scatter([0,10], [1, 1], color='b')
plt.scatter([0,10], [0, 0], color='r', marker='^')
plt.annotate('$\mu_1$', xy=(5,1), size=20)
plt.scatter([5], [1], color='k')
plt.annotate('$\mu_2$', xy=(5,0), size=20)
plt.scatter([5], [0], color='k')
plt.xlim(-1, 11)
plt.ylim(-1, 11)
plt.xlabel('term 1')
plt.ylabel('term 2')
plt.show()"
"# Here's an example. 
# Exercise: work out the means/cluster assignments
# until convergence.
def plot(points, cluster_assignments, means):
    plt.figure()
    for point, asg in zip(points, cluster_assignments):
        plt.scatter([point[0]], [point[1]], marker='o' if asg==1 else '^')
    for i, m in enumerate(means):
        plt.annotate('$\mu_%d$' % i, xy=m, size=20)
        plt.scatter([m[0]], [m[1]], color='k')
    plt.show()
    
plot([(0, 0), (0, 1), (2, 0), (3,1), (3,0)], [0,0,0,1,1], [(1,0), (4,0)])"
"import warnings
warnings.filterwarnings('ignore')
%pylab inline

# Compare arithmetic mean of precision and recall to F1 for fixed precision of 50%.

def arith_mean(p, r):
    return (p + r) / 2.0

def f1(p, r):
    return (2.0 * p * r) / (p + r)

p = .5
r = [0, .1, .2, .3, .4, .5, .6, .7, .8, .9, 1.0]

xlabel('recall')
ylabel('mean')
plot(r, [arith_mean(p, ri) for ri in r], label='arithmetic')
plot(r, [f1(p, ri) for ri in r], label='f1')
legend(loc='best')"
"xlabel('recall')
ylabel('precision')
plot([.2, .3], [.4, .3], 'bo')
xlim((0, .4))
ylim((0, 1.1))"
"# compute precision/recall at each sublist of size 1 to 10
xlabel('recall')
ylabel('precision')
precisions = [1, .5, .33, .5, .4, .5, .43, .375, .33, .3]
recalls = [.1, .1, .1, .2, .2, .3, .3, .3, .3, .3]
plot(recalls, precisions, 'bo')
xlim((0, .4))
ylim((0, 1.1))"
"# Interpolated precision: max of precisions to right of value
xlabel('recall')
ylabel('precision')
interpolated_pre = [max(precisions[i:]) for i in range(len(precisions))]
print(interpolated_pre)
step(recalls, interpolated_pre, 'bo')
xlim((0, .4))
ylim((0, 1.1))"
"import warnings
warnings.filterwarnings('ignore')
from IPython.display import YouTubeVideo

YouTubeVideo(""x4xegDME5C0"", width=560, height=315, list=""PLGBbVX_WvN7as_DnOGcpkSsUyXB1G_wqb"")"
help(np.linalg)
"from numpy.linalg import norm, det
norm"
np.dot
np.linalg.det(H)
"np.isclose(np.dot(Hinv, H), np.eye(6))"
"np.set_printoptions(precision=3)
print(np.dot(Hinv, H))"
np.linalg.cond(H)
"A = np.array([
    [1, 0, 0],
    [2, 1, 1],
    [-1, 0, 1]
])

np.linalg.eig(A)"
"M = np.arange(36, dtype=float).reshape(4, 9)
M"
"# De la segunda a la tercera fila, incluida
M[1:3]"
"# Hasta la tercera fila sin incluir y de la segunda a la quinta columnas saltando dos
M[:2, 1:5:2]
#M[1:2:1, 1:5:2]  # Equivalente"
"%matplotlib inline
import matplotlib.pyplot as plt

plt.matshow(tablero, cmap=plt.cm.gray_r)"
"import warnings
warnings.filterwarnings('ignore')
import numpy as np
#para ver la versión que tenemos instalada:
np.__version__"
np.empty(10)
"array = np.array([ 1, 1+2j, True, 'aerodinamica'])
array"
"print(id(lista))
lista.append('fluidos')
print(lista)
print(id(lista))"
"print(id(array))
array = np.append(array, 'fluidos')
print(array)
print(id(array))"
"%%timeit
sum(lista)"
"%%timeit
np.sum(array)"
"%%timeit
np.linspace(0,100,1000000)"
"%%timeit
my_linspace_FORTRAN(0,100,1000000)"
"%%timeit
my_linspace_PYTHONIC(0,100,1000000)"
"plt.figure(""Evolución temporal"", figsize=(8,5))
plt.title(""Evolución temporal"")
plt.plot(t, solucion[:, 0], label='presa')
plt.plot(t, solucion[:, 1], label='depredador')
plt.xlabel('tiempo')
plt.ylabel('población')
plt.legend()
# plt.savefig('evolucion_temporal.png')"
"plt.figure(""Presas vs depredadores"", figsize=(8,5))
plt.plot(solucion[:, 0], solucion[:, 1])
plt.xlabel('presas')
plt.ylabel('depredadores')
# plt.savefig('presas_vs_depredadores.png')"
"x_max = np.max(solucion[:,0]) * 1.05
y_max = np.max(solucion[:,1]) * 1.05

x = np.linspace(0, x_max, 25)
y = np.linspace(0, y_max, 25)

xx, yy = np.meshgrid(x, y)
uu, vv = df_dt((xx, yy), 0, a, b, c, d)
norm = np.sqrt(uu**2 + vv**2)
uu = uu / norm
vv = vv / norm

plt.figure(""Campo de direcciones"", figsize=(8,5))
plt.quiver(xx, yy, uu, vv, norm, cmap=plt.cm.gray)
plt.plot(solucion[:, 0], solucion[:, 1])
plt.xlim(0, x_max)
plt.ylim(0, y_max)
plt.xlabel('presas')
plt.ylabel('depredadores')
# plt.savefig('campo_direcciones.png')"
"n_max = np.max(solucion) * 1.10

fig, ax = plt.subplots(1,2)

fig.set_size_inches(12,5)

ax[0].quiver(xx, yy, uu, vv, norm, cmap=plt.cm.gray)
ax[0].plot(solucion[:, 0], solucion[:, 1], lw=2, alpha=0.8)
ax[0].set_xlim(0, x_max)
ax[0].set_ylim(0, y_max)
ax[0].set_xlabel('presas')
ax[0].set_ylabel('depredadores')

ax[1].plot(t, solucion[:, 0], label='presa')
ax[1].plot(t, solucion[:, 1], label='depredador')
ax[1].legend()
ax[1].set_xlabel('tiempo')
ax[1].set_ylabel('población')
# plt.savefig('campo_direcciones_ev_temporal.png')"
"x = np.linspace(0, x_max, 100)
y = np.linspace(0, y_max, 100)
xx, yy = np.meshgrid(x, y)
constant = C(xx, yy, a, b, c, d)

plt.figure('distintas_soluciones', figsize=(8,5))
plt.contour(xx, yy, constant, 50, cmap=plt.cm.Blues)
plt.xlabel('presas')
plt.ylabel('depredadores')
# plt.savefig('distintas_soluciones.png')"
"#n_max = np.max(solucion) * 1.10

fig, ax = plt.subplots(1,2)

fig.set_size_inches(12,5)

ax[0].plot(solucion[:, 0], solucion[:, 1], lw=2, alpha=0.8)
ax[0].scatter(c/d, a/b)
levels = (0.5, 0.6, 0.7, 0.72, 0.73, 0.74, 0.75, 0.76, 0.77, 0.775, 0.78, 0.781)
ax[0].contour(xx, yy, constant, levels, colors='blue', alpha=0.3)
ax[0].set_xlim(0, x_max)
ax[0].set_ylim(0, y_max)
ax[0].set_xlabel('presas')
ax[0].set_ylabel('depredadores')

ax[1].plot(t, solucion[:, 0], label='presa')
ax[1].plot(t, solucion[:, 1], label='depredador')
ax[1].legend()
ax[1].set_xlabel('tiempo')
ax[1].set_ylabel('población')
# plt.savefig('distintas_soluciones_ev_temporal.png')"
"x_ = np.linspace(0,10)
plt.figure('función logística', figsize=(8,5))
plt.plot(x_, logistic_curve(x_, 1, m=10, n=100, tau=1))
# plt.savefig('funcion_logistica.png')"
"n_max = np.max(solucion) * 1.10

fig, ax = plt.subplots(1,2)

fig.set_size_inches(12,5)

x_max = np.max(solucion_logistic[:,0]) * 1.05
y_max = np.max(solucion_logistic[:,1]) * 1.05

x = np.linspace(0, x_max, 25)
y = np.linspace(0, y_max, 25)

xx, yy = np.meshgrid(x, y)
uu, vv = df_dt_logistic((xx, yy), 0, a, b, c, d, r)
norm = np.sqrt(uu**2 + vv**2)
uu = uu / norm
vv = vv / norm

ax[0].quiver(xx, yy, uu, vv, norm, cmap=plt.cm.gray)
ax[0].plot(solucion_logistic[:, 0], solucion_logistic[:, 1], lw=2, alpha=0.8)
ax[0].set_xlim(0, x_max)
ax[0].set_ylim(0, y_max)
ax[0].set_xlabel('presas')
ax[0].set_ylabel('depredadores')

ax[1].plot(t, solucion_logistic[:, 0], label='presa')
ax[1].plot(t, solucion_logistic[:, 1], label='depredador')
ax[1].legend()
ax[1].set_xlabel('tiempo')
ax[1].set_ylabel('población')
# plt.savefig('campo_direcciones_ev_temporal_caso2.png')"
from IPython.html.widgets import interact
"interact(solucion_temporal_interact,
         a=(0.01,0.5), b=(0.01,0.5),
         c=(0.01,0.5), d=(0.01,0.5),
         x0=(1,80), y0=(1,50),
         tf=(50,300));"
"interact(mapa_fases_interact,
         a=(0.01,0.5), b=(0.01,0.5),
         c=(0.01,0.5), d=(0.01,0.5),
         x0=(1,80), y0=(1,50),
         tf=(50,300));"
"help(my_dict)
"
"my_generator = get_person()

print(my_generator)"
print(next(my_generator))
"import random
def get_int(n):
    last_value = 0
    while True:
        new_value = random.randint(1, n)
        print(""Yielding 0"")
        yield 0
        last_value = new_value
        print(""Yielding new value"")
        yield new_value
        
times = 50

for x in get_int(500):
    print(x)
    times -= 1
    if times == 0:
        break"
w
"norder = 4
nsim = 10
nbasepoints = 100

discount = 0.6
C, beta, xmax = -30, 2, 10
keep_action, replace_action = 0, 1

weights = np.zeros((norder+1,))

rng = np.random.RandomState(1234)
basepoints = rng.uniform(0, 10, nbasepoints)
rewards_keep = -4.*basepoints
reward_replace = C

for i in range(40):
    next_states_keep = rng.exponential(beta, (nbasepoints, nsim)) + basepoints[:,np.newaxis]
    overbound_states = (next_states_keep > xmax)
    next_states_keep[overbound_states] = rng.exponential(beta, np.sum(overbound_states))
    v_keep = np.mean(rewards_keep[:, np.newaxis] + discount*np.dot(poly(next_states_keep, deg=norder), weights), axis=1)

    next_state_replace = rng.exponential(beta, (nbasepoints, nsim))
    v_replace = np.mean(reward_replace + discount*np.dot(poly(next_state_replace, deg=norder), weights), axis=1)
    vhat = np.max(np.column_stack((v_keep, v_replace)), axis=1)
    qhat = np.argmax(np.column_stack((v_keep, v_replace)), axis=1)

    weights = np.linalg.lstsq(poly(basepoints, deg=norder), vhat)[0]
    indices = np.argsort(basepoints)
    if i == 1 or i == 19:
        plt.plot(basepoints[indices], np.dot(poly(basepoints, deg=norder), weights)[indices])
        plt.scatter(basepoints, vhat, marker='.')
    
plt.axvline(4.867, linestyle='dotted', c='k')    
plt.axhline(-48.67, linestyle='dotted', c='k') 
plt.xlim(0, 10)
#plt.ylim(0, 10)
#plt.savefig('inventory.pdf')"
qhat[indices]
"X, Y = make_moons(n_samples=500, noise=0.3)

X_train, X_test, Y_train, Y_test = train_test_split(X, Y)

plt.scatter(X_train[:, 0], X_train[:, 1], marker='o', c=Y_train, s=100, alpha=0.4)"
"C = 1.0
kernel_svc = svm.SVC(C=C, kernel='poly', degree=3)
kernel_svc.fit(X_train,Y_train)"
"Y_predict = kernel_svc.predict(X_test)
plt.scatter(X_test[:, 0], X_test[:, 1], marker='o', c=Y_predict, s=100, alpha=0.4)"
"h = 0.02
x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5
y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5
xx, yy = np.meshgrid(np.arange(x_min, x_max, h),
                         np.arange(y_min, y_max, h))
Z = kernel_svc.decision_function(np.c_[xx.ravel(), yy.ravel()]) # distancia al plano separador
Z = Z.reshape(xx.shape)

cm = plt.cm.RdBu
plt.contourf(xx, yy, -Z, cmap=cm, alpha=.8) # el signo menos viene por razones puramente esteticas del plot.

plt.scatter(X_test[:, 0], X_test[:, 1], marker='o', c=Y_test, s=100, alpha=0.5)"
"n_bad = 1.0 * len(Y_predict[Y_predict != Y_test])
n_total = 1.0 * len(Y_predict)
eff = n_bad / n_total
print(eff)"
"C = 1.0
gamma = 1.0
kernel_svc = svm.SVC(C=C, kernel='rbf', gamma=gamma)
kernel_svc.fit(X_train,Y_train)"
"h = 0.02
x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5
y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5
xx, yy = np.meshgrid(np.arange(x_min, x_max, h),
                         np.arange(y_min, y_max, h))
Z = kernel_svc.decision_function(np.c_[xx.ravel(), yy.ravel()]) # distancia al plano separador
Z = Z.reshape(xx.shape)

cm = plt.cm.RdBu
plt.contourf(xx, yy, -Z, cmap=cm, alpha=.8) # el signo menos viene por razones puramente esteticas del plot.

plt.scatter(X_test[:, 0], X_test[:, 1], marker='o', c=Y_test, s=100, alpha=0.5)"
"Y_predict = kernel_svc.predict(X_test)
plt.scatter(X_test[:, 0], X_test[:, 1], marker='o', c=Y_predict, s=100, alpha=0.4)"
"n_bad = 1.0 * len(Y_predict[Y_predict != Y_test])
n_total = 1.0 * len(Y_predict)
eff = n_bad / n_total
print(eff)"
"X, Y = make_circles(n_samples=500, noise=0.3, factor=0.4)

X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=.4)

plt.scatter(X_train[:, 0], X_train[:, 1], marker='o', c=Y_train, s=100, alpha=0.4)"
"C = 1.0
gamma = 1.0
kernel_svc = svm.SVC(C=C, kernel='rbf', gamma=gamma)
kernel_svc.fit(X_train,Y_train)"
"h = 0.02
x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5
y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5
xx, yy = np.meshgrid(np.arange(x_min, x_max, h),
                         np.arange(y_min, y_max, h))
Z = kernel_svc.decision_function(np.c_[xx.ravel(), yy.ravel()]) # distancia al plano separador
Z = Z.reshape(xx.shape)

cm = plt.cm.RdBu
plt.contourf(xx, yy, -Z, cmap=cm, alpha=.8) # el signo menos viene por razones puramente esteticas del plot.

plt.scatter(X_test[:, 0], X_test[:, 1], marker='o', c=Y_test, s=100, alpha=0.5)"
"from sklearn.model_selection import ShuffleSplit, GridSearchCV

gamma_range = np.logspace(-2, 2, 20)
print(gamma_range)
param_grid = dict(gamma=gamma_range)

cv = ShuffleSplit(n_splits=10, test_size=0.20)
grid = GridSearchCV(svm.SVC(kernel='rbf'), param_grid=param_grid, cv=cv)

grid.fit(X,Y)

print(""The best gamma={}, score={}"".format(grid.best_params_, grid.best_score_))"
"data = datasets.load_breast_cancer()
print(data.DESCR)"
"beta  = [0.1, 0.3, 0.4]
n_points = 100
x_1 = np.linspace(-1.0, 1.0, 100)
x_2 = -(x_1 * beta[1] -  beta[0])/beta[2] 
a = plt.plot(x_1,x_2, lw =10)
plt.quiver(beta[1], beta[2],angles='xy',scale_units='xy',scale=0.5)
a = plt.axis('equal')"
"X, Y = make_moons(n_samples=500, noise=0.3)

X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=.4)

plt.scatter(X_train[:, 0], X_train[:, 1], marker='o', c=Y_train, s=100, alpha=0.4)"
"C = 1.0
linear_svc = svm.SVC(C=C, kernel='linear')
linear_svc.fit(X_train,Y_train)"
"beta = linear_svc.coef_[0]
beta_0 = linear_svc.intercept_

l = - beta[0]/beta[1]

x_1 = np.linspace(X[:,0].min(), X[:,0].max(), 100)
x_2 = l * x_1 - beta_0 / beta[1]
a = plt.plot(x_1,x_2, lw =2)

plt.scatter(X_train[:, 0], X_train[:, 1], marker='o', c=Y_train, s=100, alpha=0.4)"
"distances = np.sum(linear_svc.support_vectors_*linear_svc.coef_[0], axis=1) + beta_0
dist_sort = np.argsort(distances)
plt.plot(distances[dist_sort])"
"x_1 = np.linspace(X_train[:,0].min(), X_train[:,0].max(), 100)
x_2 = l * x_1 - beta_0 / beta[1]
a = plt.plot(x_1,x_2, lw =2)

b = linear_svc.support_vectors_[dist_sort[0]]
x_2 = l * x_1 + (b[1] - l * b[0])
a = plt.plot(x_1,x_2, '--', color='black', lw =2)

b = linear_svc.support_vectors_[dist_sort[-1]]
x_2 = l * x_1 + (b[1] - l * b[0])
a = plt.plot(x_1,x_2, '--', color='black', lw =2)

plt.scatter(linear_svc.support_vectors_[:,0], linear_svc.support_vectors_[:,1], color='black')"
"n_bad = 1.0 * len(Y_predict[Y_predict != Y_test])
n_total = 1.0 * len(Y_predict)
eff = n_bad / n_total
print(eff)"
"h = 0.02
x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5
y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5
xx, yy = np.meshgrid(np.arange(x_min, x_max, h),
                         np.arange(y_min, y_max, h))
Z = linear_svc.decision_function(np.c_[xx.ravel(), yy.ravel()]) # distancia al plano separador
Z = Z.reshape(xx.shape)

cm = plt.cm.RdBu
plt.contourf(xx, yy, -Z, cmap=cm, alpha=.8) # el signo menos viene por razones puramente esteticas del plot.

plt.scatter(X_test[:, 0], X_test[:, 1], marker='o', c=Y_test, s=100, alpha=0.5)"
"plt.xlabel('$x$')
plt.title('finite element mesh')
plt.plot(mesh, np.zeros(len(mesh)), 'ro-');"
"plt.xlabel('$x$')
plt.ylabel('$u$')
plt.title('Finite element solution for the elastic bar')
plt.plot(mesh, u, 'ro-');"
"plt.xlabel('$x$')
plt.ylabel('$u$')
plt.title('Finite element solution for the elastic bar')
for u, mesh in solutions:
    plt.plot(mesh, u, 'o-', label=str(len(mesh)-1) + ' cells');
plt.legend(loc='upper left');"
"plt.xlabel('$x$')
plt.ylabel('$u$')
plt.title('Finite element solution for the elastic bar')
for u, mesh in solutions:
    plt.plot(mesh, u, 'o-', label=str(len(mesh)-1) + ' cells');
plt.legend(loc='upper left');"
"plt.plot(l_array, p_array/norm[0])
plt.xlabel('$\lambda$')
plt.ylabel('$P(\lambda|data)$')"
"l_walk_high_sigma = sample_metropolis_hastings(p_model_data, obs_data, 100000, 1000.0)
l_walk_low_sigma = sample_metropolis_hastings(p_model_data, obs_data, 100000, 0.001)
l_walk_mid_sigma = sample_metropolis_hastings(p_model_data, obs_data, 100000, 1.0)"
"plt.plot(x_high ,y_high)
plt.plot(l_array, p_array/norm[0])"
plt.plot(l_walk_high_sigma)
"plt.plot(x_mid ,y_mid)
plt.plot(l_array, p_array/norm[0])"
plt.plot(l_walk_mid_sigma)
"plt.plot(x_low ,y_low)
plt.plot(l_array, p_array/norm[0])"
plt.plot(l_walk_low_sigma)
"w_high, R_high = gelman_rubin(p_model_data, obs_data, sigma=1E3)
w_low, R_low = gelman_rubin(p_model_data, obs_data, sigma=1E-3)
w_mid, R_mid = gelman_rubin(p_model_data, obs_data, sigma=1.0)"
"plt.plot(R_high[500:], label='$\sigma=10^3$')
plt.plot(R_mid[500:], label='$\sigma=1.0$')
plt.plot(R_low[500:], label='$\sigma=10^{-3}$')
plt.loglog()
plt.legend()
plt.xlabel('Iteration Number')
plt.ylabel('$\hat{R}$')"
"sns.pairplot(data, x_vars=['Income', 'Limit'], y_vars='Balance', size=7, aspect=0.7, kind='reg')"
clf.intercept_
"clf.score(X_test, Y_test)"
"cross_val_score(clf, X, Y, cv=5)"
"scaler = preprocessing.StandardScaler().fit(X_test)
scaler"
"print(scaler.mean_)
print(scaler.scale_)"
"print(np.mean(scaler.transform(X_test), axis=0))
print(np.std(scaler.transform(X_test), axis=0))"
boston.keys()
print(boston['DESCR'])
plt.plot(boston['target'])
"plt.scatter(X[:,6], X[:,12])
plt.xlabel(boston.feature_names[6])
plt.ylabel(boston.feature_names[12])"
"rf = RandomForestClassifier(n_estimators=50)
rf.fit(X[:200], Y[:200])"
"prediction = rf.predict(X[200:])
plt.scatter(Y[200:], prediction)"
plt.plot(square_sum)
"ii = np.argsort(rf.feature_importances_)
for name, value in zip(boston.feature_names[ii], rf.feature_importances_[ii]):
    print(name, value)"
"t, x_t = solve_lorenz(angle=0, N=10)"
"w = interactive(solve_lorenz, angle=(0.,360.), N=(0,50), sigma=(0.0,50.0), rho=(0.0,50.0))
display(w)"
w.kwargs
"plt.hist(xyz_avg[:,0])
plt.title('Average $x(t)$')"
"plt.hist(xyz_avg[:,1])
plt.title('Average $y(t)$')"
"# Carry out anova

from scipy import stats
f_val, p_val = stats.f_oneway(*data)
p_val"
"# This is how to deal with the nans

data = [df[col].dropna() for col in df]
f_val, p_val = stats.f_oneway(*data)
p_val"
"# Let us create a nice plot with a regression line fitted to the median of the samples

import numpy as np  # For the median
import matplotlib.pyplot as plt  # For the plot
import seaborn
%matplotlib inline

# Fit line to median of distributions
x = range(1, len(data) + 1)
y = [np.median(sample) for sample in data]
slope, intercept, r_val, p_val, slope_std_error = stats.linregress(x, y)

def line(x):
    """"""The regression line""""""
    return slope * x + intercept

plt.figure()
plt.violinplot(data);
x1, x2 = plt.xlim()
plt.plot((x1, x2), (line(x1), line(x2)), '--',
         label=""$y = {:.2f}x + {:.2f}$ ($p={:.2f}$)"".format(slope, intercept, p_val),
         ),
plt.legend(loc=4);
# plt.savefig(""../assets/images/regression_anova.svg"");  # Save to your path"
"# reflectivity plots
plt.figure(1,figsize=(9,6))
plt.plot(v_theta*1.8e2/np.pi,v_r_s,'k',linewidth=2.0)
plt.plot(v_theta*1.8e2/np.pi,v_r_p,'r',linewidth=2.0)


# labels
plt.xlabel(r'$\Theta^{\circ}$',fontsize=f_size+10)
plt.ylabel('R',fontsize=f_size+10)

# ticks
plt.xticks(fontsize=f_size)
plt.yticks(fontsize=f_size)

# grids
plt.grid()

#legends
plt.legend(['TE (s)','TM (p)'],loc='lower right',fontsize=f_size,fancybox=True);"
"# field plots
plt.figure(figsize=(9,6))

# plot
plt.plot(v_z,v_field_s,'k',linewidth=2.0)
plt.plot(v_z,v_field_p,'r',linewidth=2.0)
for n_d,d in enumerate(d_list):
    plt.axvline(d_list[0:n_d].sum(),color='gray',linestyle='dashed',linewidth=2.0)    

# labels
plt.ylabel(r'$|E_{\mathrm{y,TE}}|,|H_{\mathrm{y,TM}}|$',fontsize=f_size+10)
plt.xlabel(r'z (nm)',fontsize=f_size+5)

# ticks
plt.xticks(fontsize=f_size)
plt.yticks([5,10,15,20],fontsize=f_size)

# grids
plt.grid(color='gray',axis='y')

# legend
plt.legend(['TE (s)','TM (p)'],loc='upper left',fontsize=f_size,fancybox=True);"
"# reflectivity plots
plt.figure(1,figsize=(15,10))
plt.plot(v_theta*1.8e2/np.pi,v_r_p,'k',linewidth=2.0)

# labels
plt.xlabel(r'$\Theta^{\circ}$',fontsize=f_size+10)
plt.ylabel('R',fontsize=f_size+10)

# ticks
plt.xticks(fontsize=f_size)
plt.yticks(fontsize=f_size)

# grids
plt.grid()

#legends
plt.legend(['55 nm Au film reflectance at 633 nm'],loc='upper right',fontsize=f_size,frameon=False);"
"# field plots
plt.figure(figsize=(9,7.5))

# transverse magnetic field modulues
plt.subplot(2,2,1)

# plot
plt.plot(v_z,v_field,'k',linewidth=3.0)
plt.axvline(d_list[0],color='gray',linestyle='dashed',linewidth=2.0)
plt.axvline(d_list[1],color='gray',linestyle='dashed',linewidth=2.0)

# labels
plt.ylabel(r'$|H_{\mathrm{y}}|$',fontsize=f_size+10)

# ticks
plt.xticks([0,200,400],fontsize=f_size)
plt.yticks([0,2,4,6,8],fontsize=f_size)

# grids
plt.grid(color='gray')


# local absorbed power
plt.subplot(2,2,2)

# plot
plt.plot(v_z,v_abs,'k',linewidth=3.0)
plt.axvline(d_list[0],color='gray',linestyle='dashed',linewidth=2.0)
plt.axvline(d_list[1],color='gray',linestyle='dashed',linewidth=2.0)

# labels
plt.ylabel(r'Abs power (a.u.)',fontsize=f_size+5)

# ticks
plt.xticks([0,200,400],fontsize=f_size)
plt.yticks(fontsize=f_size)

# grids
plt.grid(color='gray')


# Sx component of the Poynting vector
plt.subplot(2,2,3)

# plot
plt.plot(v_z,v_S[:,0],'k',linewidth=3.0)
plt.axvline(d_list[0],color='gray',linestyle='dashed',linewidth=2.0)
plt.axvline(d_list[1],color='gray',linestyle='dashed',linewidth=2.0)

# labels
plt.xlabel(r'z (nm)',fontsize=f_size+10)
plt.ylabel(r'$S_{\mathrm{x}}$',fontsize=f_size+5)

# ticks
plt.xticks([0,200,400],fontsize=f_size)
plt.yticks(fontsize=f_size)

# grids
plt.grid(color='gray')


# Sz component of the Poynting vector
plt.subplot(2,2,4)

# plot
plt.plot(v_z,v_S[:,2],'k',linewidth=3.0)
plt.axvline(d_list[0],color='gray',linestyle='dashed',linewidth=2.0)
plt.axvline(d_list[1],color='gray',linestyle='dashed',linewidth=2.0)

# labels
plt.xlabel(r'z (nm)',fontsize=f_size+10)
plt.ylabel(r'$S_{\mathrm{z}}$',fontsize=f_size+5)

# ticks
plt.xticks([0,200,400],fontsize=f_size)
plt.yticks(fontsize=f_size)

# grids
plt.grid(color='gray')


plt.tight_layout()"
"# cmd plot
fg2=plt.figure(num=2,figsize=(15,10))
plt.plot(v_wl,1e4*(v_A_r-v_A_l)/(v_A_r).max(),'r-',
         v_wl,1e4*v_mcd,'ko',
         markersize=10,linewidth=3);

#-----ticks------
fsize=15;
plt.xticks(fontsize=fsize+10);plt.yticks(fontsize=fsize+10);

#------axis labels------
plt.xlabel(r'Wavelength (nm)',fontsize=fsize+15);
plt.ylabel(r'$\Delta$A\(A$_{\mathrm{max}}*$B)(T$^{-1}*10^{4}$)',fontsize=fsize+15);

#------plot legend------
plt.legend(('Tensorial','Scalar'), fontsize=fsize+15,loc='upper right',frameon=False);"
"# python reuses small integer objects within its runtime.
x = None
print(sys.getrefcount(0))"
"x = 0
print(sys.getrefcount(0))"
"# bigger objects are not cached and created one off.
sys.getrefcount(257)
print (256 + 1 is 256 + 1)"
"import warnings
warnings.filterwarnings('ignore')
## make imports
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import seaborn as sns
%matplotlib inline
sns.set(style=""ticks"", color_codes=True)

## load the iris data
iris = sns.load_dataset(""iris"")
g = sns.pairplot(iris)"
"g = sns.pairplot(iris, hue=""species"",palette=""husl"")"
"## make a histogram the matplotlib way
fig = plt.figure(figsize=(10,8))
ax = fig.add_subplot(1,1,1)
ax.hist(iris['sepal_length']);"
"## make a histogram the pandas way
ax = iris[""sepal_length""].plot(kind='hist')
ax.set_title(""my title"");"
"t, x_t = solve_lorenz(angle=0, N=10)"
"w = interactive(solve_lorenz, angle=(0.,360.), N=(0,50), sigma=(0.0,50.0), rho=(0.0,50.0))
display(w)"
w.kwargs
"plt.hist(xyz_avg[:,0])
plt.title('Average $x(t)$')"
"plt.hist(xyz_avg[:,1])
plt.title('Average $y(t)$')"
"plt.figure(figsize=(10,10))
#plt.plot(coords[:3], 'x', )
for (i,party) in enumerate(party_names):
    plt.annotate(party, coords[i])
plt.xlim(-5,5)
plt.ylim(-5,5)
plt.title('PCA of party statements for kieswijzer 2017')"
coalitions[~coalitions.has_pvv].sort_values(by='stress').head(10)
plt.plot(v1)
plt.plot(v2)
plt.plot(v3)
plt.plot(v4)
"%timeit compute_method1(x, pars)"
"%timeit compute_method2(x, pars)"
"%timeit compute_method3(x, pars)"
"%timeit compute_method4(x, pars)"
"ax = Emean_vs_A.plot(x='Mass Number', y='binding energy per nucleus',
          legend=False, color=(0.3,0.3,0.3,0.5), marker='o', alpha=0.7)
ax.set_xlabel('Mass number A (number of nucleons)', fontsize=16)
ax.set_ylabel('Average binding energy per nucleons [MeV]', fontsize=16)
ax.grid(True)
ax.tick_params(size=14)
plt.tight_layout()"
"ax2 = data.plot(x='Mass Number', y='binding energy per nucleus',
          legend=False, kind='scatter', color=(0.8,0.8,0.8,1))
ax2.set_xlabel('Mass number A (number of nucleons)', fontsize=16)
ax2.set_ylabel('Binding energy per nucleons [MeV]', fontsize=16)
ax2.grid(True)
ax2.tick_params(size=14)
ax2.set_xlim(0,260)
ax2.set_ylim(0, 9)
plt.tight_layout()"
"# Import cross-section data from http://home.earthlink.net/~jimlux/nuc/sigma.htm
data_DT=pd.read_csv('http://home.earthlink.net/~jimlux/nuc/tdn.txt', skiprows=16, delimiter='\t', 
                 names=('Energy [keV]','Sigma [Barns]'))
data_DD=pd.read_csv('http://home.earthlink.net/~jimlux/nuc/ddn.txt', skiprows=16, delimiter='\t', 
                 names=('Energy [keV]','Sigma [Barns]'))
Barn_to_msquare = 1e-28

ax=data_DT.plot(x='Energy [keV]', y='Sigma [Barns]', logy=True, logx=True, label='D-T', lw=2)
data_DD.plot(x='Energy [keV]', y='Sigma [Barns]', logy=True, logx=True, label='D-D', lw=2, ax=ax)
ax.set_ylim(10e-3, 10)
ax.set_xlim(1, 10e3)
ax.set_ylabel('$\sigma$ [barns]')
ax.grid(True, which='minor')
ax.set_title('Cross sections for D-T and D-D fusion reactions \n vs deuteron energy')"
"T = np.logspace(-2,3) # keV
sigmav = sigmav_avg(T) # m^-3 /s

fig,ax = plt.subplots()
ax.plot(T, sigmav)
ax.set_xscale('log')
ax.set_yscale('log')
ax.set_xlabel('T [keV]')
ax.set_ylabel('$<\sigma v>$ [$m^3/s$]')
ax.set_xlim(1, 1e3)
ax.set_ylim(1e-25, 1e-21)
ax.grid(True, which='minor', axis='both')
ax.set_title('Volumetric reaction rates of D-T reaction')"
"# Taken from Eqs.(4.18) J.Freidberg
def S_alpha_over_p(T):
    ""T in keV""
    #E_alpha = 3.5 # MeV
    #return 1/16*(E_alpha*1e6/e)/(T/e*k_B)**2*sigmav_avg(T/1e3)
    return 1.37*sigmav_avg(T)/1e-22/T**2
    
def S_B_over_p(T, Z_eff=1):
    ""T in keV""
    return 0.052/T**(3/2)

T = np.logspace(0,3) # keV
S_a = S_alpha_over_p(T) 
S_B = S_B_over_p(T)


fig, ax = plt.subplots()
ax.plot(T, S_a, label='$S_{alpha}/p^2$')
ax.plot(T, S_B, label='$S_B/p^2$')
ax.set_ylim(0, 0.03)
ax.set_xlim(1, 100)
ax.set_xscale('log')
ax.set_xlabel('T [keV]')
ax.set_ylabel('Power Density [$MW/m^3/atm^2$]')
ax.legend()
fig.tight_layout()"
"# setup a figure
fig = plt.figure()
ax = fig.add_axes([0,0,1,1], projection='3d')
# setup dummy lines and points to be filled after
lines = ax.plot([], [], color='r')[0]
pts = ax.plot([], [], color='r')[0]

plt.plot(x,y,z)
min(y)"
"x,y,z,vx,vy,vz = u_num.T
ax.plot(x,y,z, lw=2)
"
"fig=plt.figure()
axe=fig.add_axes([0,0,1,1], projection='3d')
axe.plot(x, y, z, color='r', lw=2)
axe.plot(x[-2:-1], y[-2:-1], z[-2:-1], marker='.', markersize=20, color='r') # particle point
axe.axis('on')
#axe.tick_params(axis='x', which='both',  labelbottom='off')
axe.view_init(30,60)"
"fig=plt.figure()
axe=fig.add_subplot(111, projection='3d')
axe.plot(x, y, z, color='r', lw=2)
axe.plot(x[-2:-1], y[-2:-1], z[-2:-1], marker='.', markersize=20, color='r') # particle point"
"map(compose(factors, triangle_num), range(1, 11))"
"import warnings
warnings.filterwarnings('ignore')
from IPython.core.display import HTML
def css_styling():
    styles = open(""./styles/custom.css"", ""r"").read  ()
    return HTML(styles)
css_styling()

%matplotlib inline
import numpy as np
import matplotlib.pyplot as plt
import os
print(os.getcwd())"
"#Quick visualize data
plt.grid(True)
plt.xlim([-100,5000])
dummy = plt.hist(X[:,0],label = 'col1')
dummy = plt.hist(X[:,1],label = 'col2')
dummy = plt.hist(X[:,2],label = 'col3')
plt.title('Clearly we need feature normalization.')
plt.xlabel('Column Value')
plt.ylabel('Counts')
dummy = plt.legend()"
"#Quick visualize the feature-normalized data
plt.grid(True)
plt.xlim([-5,5])
dummy = plt.hist(Xnorm[:,0],label = 'col1')
dummy = plt.hist(Xnorm[:,1],label = 'col2')
dummy = plt.hist(Xnorm[:,2],label = 'col3')
plt.title('Feature Normalization Accomplished')
plt.xlabel('Column Value')
plt.ylabel('Counts')
dummy = plt.legend()"
"#print ""Final result theta parameters: \n"",theta
print('Check of result: What is price of house with 1650 square feet and 3 bedrooms?')
ytest = np.array([1650.,3.])
#To ""undo"" feature normalization, we ""undo"" 1650 and 3, then plug it into our hypothesis
ytestscaled = [(ytest[x]-stored_feature_means[x+1])/stored_feature_stds[x+1] for x in range(len(ytest))]
ytestscaled.insert(0,1)
print(h(theta, ytestscaled))"
"print('Normal equation prediction for price of house with 1650 square feet and 3 bedrooms')
print(h(normEqtn(X,y),[1,1650.,3]))"
"import warnings
warnings.filterwarnings('ignore')
from IPython.core.display import HTML
def css_styling():
    styles = open(""./styles/custom.css"", ""r"").read  ()
    return HTML(styles)
css_styling()

%matplotlib inline
import numpy as np
import matplotlib.pyplot as plt
import os
print(os.getcwd())"
"r = np.exp(np.log(1.0 + 1.0)/ 5.0) - 1
print (r)"
"r = np.exp(np.log(1.0 + 1.0)/ 20.0) - 1
print(r)"
"import warnings
warnings.filterwarnings('ignore')
%matplotlib inline
import numpy as np
import matplotlib.pyplot as plt

def drawPolygon (n):
    """"""Draw an n-sided polygon centered at the origin.""""""
    theta = np.linspace (0, 2 * np.pi, n, endpoint=False)
    plt.fill (np.cos (theta), np.sin (theta))
    plt.axis ('equal')

def drawCircle (samples=100, r=1):
    """"""Draw a circle centered at the origin.""""""
    theta = np.linspace (0, 2 * np.pi, samples, endpoint=False)
    plt.plot (np.cos (theta), np.sin (theta))
    plt.axis ('equal')

drawCircle ()
drawPolygon (4)"
"drawCircle ()
drawPolygon (8)
drawPolygon (4)"
"plt.fill ((-1, 0, 1), (0, 1, 0));
plt.text (-0.55, 0.55, 'l')
plt.plot ((0, 0), (0, 1), 'w:')
plt.text (0.1, 0.4, 'h', color='w')
plt.plot ((-1, 0, 1), (0, -1, 0))
plt.axis ('equal');"
"def polygonFromTriangles (n):
    """"""Draw a polygon composed of Isosceles triangles.""""""
    theta = 2 * np.pi / n
    for i in range (n):
        plt.fill ((0, np.cos (theta * i), np.cos (theta * (i + 1))),
                 (0, np.sin (theta * i), np.sin (theta * (i + 1))))
    plt.axis ('equal')

# Draw a square
polygonFromTriangles (4)"
polygonFromTriangles (17)
"drawCircle ()
drawPolygon (8)
drawPolygon (4)
plt.plot ((0, 0, 1), (1, 0, 0), 'w-')
plt.plot ((0, np.cos (np.pi/4)), (0, np.sin (np.pi/4)), 'w--');"
"plt.plot ((0, 0.5, 1, 0), (0, 0.5, 0, 0))
plt.plot ((0.5, np.cos (np.pi/4), 1), (0.5, np.sin (np.pi/4), 0))
plt.axis ('equal')
plt.grid ()
plt.text (0.3, 0.35, '$h_4$')
plt.text (0.65, 0.25, '$b_4/2$')
plt.text (0.9, 0.3, '$b_8$')
plt.text (0.55, 0.65, '$r - h_4$');"
"def polyPi (n, b):
    """"""Compute area of n-sided regular polygon inscribed on unit circle with sides length b.""""""
    ia, h = isoscelesArea (b, 1)
    return n * ia, h

def polyPiLoop (n=None, epsilon=0.001):
    """"""Compute a polygon approximation to Pi.""""""

    # Start with a square
    sides = 4
    base = np.sqrt (2)
    previous = 0.0
    current, h = polyPi (sides, base)
    while current - previous > epsilon:
        previous = current
        base = doublePolygon (base, h)
        sides *= 2
        current, h = polyPi (sides, base)

    return current, sides, base

print (polyPiLoop ())
print (polyPiLoop (epsilon=1.0e-12))
print (polyPiLoop (epsilon=1.0e-30))"
"def convergence (n=25):
    """"""Return a vector of estimated values for pi at each iteration.""""""
    sides = 4
    base = np.sqrt (2)
    estimates = np.zeros (n, dtype=float)
    for i in range (n):
        current, h = polyPi (sides, base)
        estimates[i] = current
        base = doublePolygon (base, h)
        sides *= 2
        
    return estimates

estimates = convergence ()
f, a = plt.subplots (2, 1, sharex=True)
a[0].plot (estimates)
a[0].set_ylabel ('Estimated value of $\\pi$')
a[1].semilogy (np.pi - estimates)
a[1].set_xlabel ('Number of iterations')
a[1].set_ylabel ('Difference from $\\pi$');"
"samples = 100
tosses = np.random.uniform (0.0, 1.0, samples) < heads
print ('{0:d} tosses, {1:d} heads, {2:.1f}%'.format (
    samples,
    sum (tosses),
    100 * sum (tosses) / samples))"
"plt.plot (np.cumsum (tosses))
plt.grid ()
# plt.lines ((0, 0), (tosses, tosses))
plt.show ()"
"bins = 1000
theta = np.linspace (0, 1, bins)
prior = np.ones (bins)
plt.plot (theta, prior)
plt.show ()"
"likelihood = np.linspace (0, 2, bins)
plt.plot (theta, likelihood, label='heads')
plt.plot (theta, 2 - likelihood, label='tails')
plt.xlabel ('$\Theta$')
plt.legend ()
plt.show ()"
"plt.plot (theta, posterior (prior, likelihood), label='1head,0tail')
plt.plot (theta, posterior (prior, likelihood * (2 - likelihood)), label='1head,1tail')
plt.plot (theta, posterior (prior, likelihood**2 * (2 - likelihood)**2), label='2heads,2tails')
plt.legend ()
plt.show ()"
"estimates = np.matmul (likelihood.reshape ((-1, 1)), tosses.reshape ((1, -1)))
estimates += np.matmul (2 - likelihood.reshape ((-1, 1)), ~tosses.reshape ((1, -1)))
estimates = np.cumprod (estimates, 1)
plt.pcolormesh (estimates)
plt.plot (np.argmax (estimates, 0))
plt.show ()"
"heatmapper (estimates, heads)
plt.show ()"
"plot(x,u_initial)"
"plot(x, u_initial)
plot(x, u_past)
plot(x, u_present)
plt.xlabel('x')
plt.ylabel('u')"
"plot(x, u_initial)
plot(x, u_present)
plt.xlabel('x')
plt.ylabel('u')"
"import warnings
warnings.filterwarnings('ignore')
from IPython.display import VimeoVideo
VimeoVideo(48756378)"
"b = BloomFilter(15000, 20)

def fill(B, n_test=1000, upper=1e9, exact=False):
    if exact:
        exact_ = set()

    for n in range(n_test):
        r = np.random.randint(upper)
        B.add(r)
        if exact:
            exact_.add(r)
            
    if exact:
        return exact_
    
exact = fill(b, exact=True)

n_true = 0
for e in exact:
    if e in b:
        n_true += 1

def fakes(B, n_test=1000, upper=1e9):
    n_fake = 0
    for e in np.random.randint(upper, size=n_test):
        if int(e) in B:
            n_fake += 1
            
    return n_fake / n_test
        
print('True positive rate: %.3f False positive rate: %.3f.' % (n_true / len(exact),
                                                               fakes(b)))"
"sizes = np.arange(5000, 40000, step=2000)

for n_test in (1500, 3000, 4500):
    fake_rate = []
    for size in sizes:
        b = BloomFilter(size, 20)
        fill(b, n_test=n_test)
        fake_rate.append(fakes(b, n_test=10000))

    plt.plot(sizes, fake_rate, label='%i (%f.1) entries' % (n_test, b.entries()))

plt.legend(loc='best')
plt.xlabel('Bloom filter size [bits]')
plt.ylabel('False positive rate')"
"for n_test in (2500, 3000, 3500):
    hash_funs = np.arange(1, 20)
    fake_rate = []
    for k in hash_funs:
        b = BloomFilter(22000, k)
        fill(b, n_test=n_test)
        fake_rate.append(fakes(b, n_test=3000))

    plt.plot(hash_funs, fake_rate, label='%i entries' % n_test)

plt.legend(loc='best')
plt.xlabel('Number of hash functions $k$')
plt.ylabel('False positive rate')"
"cms = CountMinSketch(100, 300)

words = {}
for n in range(1000):
    word = ''.join([random.choice(string.ascii_lowercase) for _ in range(10)])
    count = np.random.randint(50)
    words[word] = count
    for _ in range(count):
        cms.update(word)

for n,word in enumerate(words):
    print(words[word], cms.query(word))
    if n > 20:
        break"
"# how many distinct elements in a stream of 100000
# random numbers?
hyperloglog((np.random.randint(0, int(1e9)) for i in range(100000)), b=8)"
"pythonic_mines(9,9,40)"
"b = np.zeros((9, 9))
N = 100
n_mines = 20
for n in range(N):
    b += pythonic_mines(*b.shape, n_mines)
    
b /= N
average = n_mines/float(b.shape[0]*b.shape[1])

f, (ax1, ax2) = plt.subplots(1, 2, figsize=(9.1,4))

im = ax1.imshow(b, interpolation='nearest', cmap=plt.cm.bwr)
divider1 = make_axes_locatable(ax1)
cax1 = divider1.append_axes(""right"", size=""10%"", pad=0.05)
plt.colorbar(im, cax=cax1)

ax2.hlines(average, xmin=0, xmax=b.shape[0]*b.shape[1])
ax2.plot(b.ravel())
ax2.set_xlim(0, b.shape[0]*b.shape[1])

y = b.ravel()
(m, b), _ = fit(range(y.shape[0]), y)
x = np.linspace(0, y.shape[0], 100)
ax2.plot(x, m*x + b)

f.tight_layout()

print('Average should be: %.3f'%(average))"
"t, x_t = solve_lorenz(angle=0, N=10)"
"w = interactive(solve_lorenz, angle=(0.,360.), N=(0,50), sigma=(0.0,50.0), rho=(0.0,50.0))
display(w)"
w.kwargs
"plt.hist(xyz_avg[:,0])
plt.title('Average $x(t)$')"
"plt.hist(xyz_avg[:,1])
plt.title('Average $y(t)$')"
"# define distribution function

def gauss(x, mu, sigma):
    return 1./sqrt(2*pi*sigma**2)*exp(-(x-mu)**2/(2*sigma**2))

def P(x):
    mu1, sigma1 = 0.1, 0.1
    mu2, sigma2 = 0.5, 0.07
    return gauss(x, mu1, sigma1)*0.3 + gauss(x, mu2, sigma2)*0.7

x = linspace(-1,1, 200)
plot(x, P(x)) "
"def step(x, stepsize = 0.1):
    return x + random.normal(0, stepsize)

def plot_path():
    figure()
    ax1 = gcf().add_axes([.15, .15, .55, .8])
    ax2 = gcf().add_axes([.70, .15, .25, .8])
    #ax2 = subplot(122)
    ax2.axes.yaxis.set_ticklabels([])
    ax2.axes.xaxis.set_ticklabels([])
    ax1.plot(path)
    ax2.hist(path, bins=linspace(-1,1,100), normed=1, orientation=""horizontal"", histtype=""stepfilled"", color=""#DDDDDD"")
    ax2.set_ylim([-0.4, 0.8])
    ax1.set_ylim([-0.4, 0.8])
    ax1.set_title(""stepsize = %.3f, acceptance = %.3f"" % (stepsize, acceptance))

    x = linspace(-1,1, 200)
    ax2.plot(P(x),x,""r"", lw=2)

random.seed(12)
for stepsize in [0.01, 0.1, 2.]:
    path, acceptance = metropolis(0., step, 1000, P, stepsize=stepsize)
    plot_path()
pass"
"def E(x):
    return x**2 - cos((cos(x*4)**8-1)*20)
    
    #return x**2-cos(x*gauss(x, 0, 0.3)*200)*gauss(x, 0, 0.5)*0.4
    #return x**2-cos(x/(0.5+x**2)*200)*gauss(x, 0, 0.5)*0.4

x = linspace(-1,1, 1000)
plot(x, E(x))"
"random.seed(1234)
def step(x, stepsize = 0.1):
    return x + random.normal(0, stepsize)

for stepsize in [0.01, 0.5, 5.]:
    path, acceptance = greedy(0.8, step, 2000, E, stepsize=stepsize)
    figure(figsize=(6,4))
    ax1 = gcf().add_axes([.15, .15, .45, .8])
    ax2 = gcf().add_axes([.70, .15, .35, .8])

    ax1.plot(path, E(path))
    x = linspace(-1,1, 1000)
    ax1.plot(x, E(x))
    ax2.semilogy(E(path)-E(0))
pass"
"T0 = 1.
def schedule(k):
    return T0/log(k+1)

def step(x, k, stepsize = 0.1):
    return x + random.normal(0, stepsize*sqrt(schedule(k)))

random.seed(1234)
nstep = 10000
x0 = 0.8
for stepsize in [0.01, 0.1, 5.]:
    path, acceptance = boltzmann_anneal(x0, step, nstep, E, schedule, stepsize=stepsize)
    figure(figsize=(12,4))
    ax1 = gcf().add_axes([.10, .15, .40, .8])
    ax2 = gcf().add_axes([.55, .15, .20, .8])
    ax3 = gcf().add_axes([.80, .15, .20, .8])

    ax1.plot(path, E(path))
    x = linspace(-1,1, 1000)
    ax1.plot(x, E(x))
    ax2.semilogy(E(path)-E(0))
    ax3.plot(schedule(arange(nstep)))
pass"
"T0 = 10.
def schedule(k):
    return T0/(k+10)

def step(x, k, stepsize = 0.1):
    return x + random.normal(0, stepsize*sqrt(schedule(k)))

random.seed(1234)
nstep = 10000
x0 = 0.8
for stepsize in [0.1, 1, 10.]:
    path, acceptance = boltzmann_anneal(x0, step, nstep, E, schedule, stepsize=stepsize)
    print(acceptance)
    figure(figsize=(12,4))
    ax1 = gcf().add_axes([.10, .15, .40, .8])
    ax2 = gcf().add_axes([.55, .15, .20, .8])
    ax3 = gcf().add_axes([.80, .15, .20, .8])

    ax1.plot(path, E(path))
    x = linspace(-1,1, 1000)
    ax1.plot(x, E(x))
    ax2.semilogy(E(path)-E(0))
    ax3.plot(schedule(arange(nstep)))
pass"
"T0 = 10.
def schedule(k):
    return T0/(k+1)

def step(x, k, stepsize = 0.1):
    return x + random.standard_cauchy() * stepsize*(schedule(k))

random.seed(1234)
nstep = 10000
x0 = 0.8
for stepsize in [0.1, 1, 10.]:
    path, acceptance = boltzmann_anneal(x0, step, nstep, E, schedule, stepsize=stepsize)
    print(acceptance)
    figure(figsize=(12,4))
    ax1 = gcf().add_axes([.10, .15, .40, .8])
    ax2 = gcf().add_axes([.55, .15, .20, .8])
    ax3 = gcf().add_axes([.80, .15, .20, .8])

    ax1.plot(path, E(path))
    x = linspace(-1,1, 1000)
    ax1.plot(x, E(x))
    ax2.semilogy(E(path)-E(0))
    ax3.plot(schedule(arange(nstep)))
pass"
"T0 = 10.
def schedule(k):
    eps=0.1
    return T0*exp(-k*eps)

def step(x, k, stepsize = 0.1):
    return x + random.standard_cauchy() * stepsize*sqrt(schedule(k))

random.seed(1234)
nstep = 2000
x0 = 0.8
for stepsize in [0.1, 1., 10., 100., 1., 1., 1.]:
    path, acceptance = boltzmann_anneal(x0, step, nstep, E, schedule, stepsize=stepsize)
    print(acceptance)
    figure(figsize=(12,4))
    ax1 = gcf().add_axes([.10, .15, .40, .8])
    ax2 = gcf().add_axes([.55, .15, .20, .8])
    ax3 = gcf().add_axes([.80, .15, .20, .8])

    ax1.plot(path, E(path))
    x = linspace(-1,1, 1000)
    ax1.plot(x, E(x))
    ax2.semilogy(E(path)-E(0))
    ax3.plot(schedule(arange(nstep)))
pass"
"import warnings
warnings.filterwarnings('ignore')
import numpy as np 

nx,ny=6,4
X = np.random.randint(0,5,size=(nx,ny))
X"
"X[:,slice(0,4,2)]"
X[stripe]
"# Plot the filled contour of the temperature.
pyplot.figure(figsize=(8.0, 5.0))
pyplot.xlabel('x [m]')
pyplot.ylabel('y [m]')
levels = numpy.linspace(20.0, 100.0, num=51)
contf = pyplot.contourf(x, y, T, levels=levels)
cbar = pyplot.colorbar(contf)
cbar.set_label('Temperature [C]')
pyplot.axis('scaled', adjustable='box');"
"# Plot the filled contour of the temperature.
pyplot.figure(figsize=(8.0, 5.0))
pyplot.xlabel('x [m]')
pyplot.ylabel('y [m]')
levels = numpy.linspace(20.0, 100.0, num=51)
contf = pyplot.contourf(x, y, T, levels=levels)
cbar = pyplot.colorbar(contf)
cbar.set_label('Temperature [C]')
pyplot.axis('scaled', adjustable='box');"
"# Plot the temperature along the rod.
pyplot.figure(figsize=(6.0, 4.0))
pyplot.xlabel('Distance [m]')
pyplot.ylabel('Temperature [C]')
pyplot.grid()
pyplot.plot(x, T, color='C0', linestyle='-', linewidth=2)
pyplot.xlim(0.0, L)
pyplot.ylim(0.0, 100.0);"
"# Plot the temperature along the rod.
pyplot.figure(figsize=(6.0, 4.0))
pyplot.xlabel('Distance [m]')
pyplot.ylabel('Temperature [C]')
pyplot.grid()
pyplot.plot(x, T, color='C0', linestyle='-', linewidth=2)
pyplot.xlim(0.0, L)
pyplot.ylim(0.0, 100.0);"
"# Plot the temperature along the rod.
pyplot.figure(figsize=(6.0, 4.0))
pyplot.xlabel('Distance [m]')
pyplot.ylabel('Temperature [C]')
pyplot.grid()
pyplot.plot(x, T, color='C0', linestyle='-', linewidth=2)
pyplot.xlim(0.0, L)
pyplot.ylim(0.0, 100.0);"
"# Plot the temperature along the rod.
pyplot.figure(figsize=(6.0, 4.0))
pyplot.xlabel('Distance [m]')
pyplot.ylabel('Temperature [C]')
pyplot.grid()
pyplot.plot(x, T, color='C0', linestyle='-', linewidth=2)
pyplot.xlim(0.0, L)
pyplot.ylim(0.0, 100.0);"
"# Compute the analytical temperature distribution.
T_exact = analytical_temperature(x, nt * dt, alpha, L, 100)

# Plot the numerical and analytical temperatures.
pyplot.figure(figsize=(6.0, 4.0))
pyplot.xlabel('Distance [m]')
pyplot.ylabel('Temperature [C]')
pyplot.grid()
pyplot.plot(x, T, label='numerical',
            color='C0', linestyle='-', linewidth=2)
pyplot.plot(x, T_exact, label='analytical',
            color='C1', linestyle='--', linewidth=2)
pyplot.legend()
pyplot.xlim(0.0, L)
pyplot.ylim(0.0, 100.0);"
"# Plot the error versus the time-step size.
pyplot.figure(figsize=(6.0, 6.0))
pyplot.grid()
pyplot.xlabel(r'$\Delta t$')
pyplot.ylabel('Relative $L_2$-norm\nof the error')
pyplot.loglog(dt_values, errors, label='Crank-Nicolson',
              color='black', linestyle='--', linewidth=2, marker='o')
pyplot.loglog(dt_values, errors_btcs, label='BTCS (implicit)',
              color='black', linestyle='--', linewidth=2, marker='s')
pyplot.legend()
pyplot.axis('equal');"
errors
"# Plot the error versus the grid-spacing size.
pyplot.figure(figsize=(6.0, 6.0))
pyplot.grid()
pyplot.xlabel(r'$\Delta x$')
pyplot.ylabel('Relative $L_2$-norm\nof the error')
dx_values = L / (numpy.array(nx_values) - 1)
pyplot.loglog(dx_values, errors,
              color='black', linestyle='--', linewidth=2, marker='o')
pyplot.axis('equal');"
"# Plot the error versus the grid-spacing size.
pyplot.figure(figsize=(6.0, 6.0))
pyplot.grid()
pyplot.xlabel(r'$\Delta x$')
pyplot.ylabel('Relative $L_2$-norm\nof the error')
dx_values = L / (numpy.array(nx_values) - 1)
pyplot.loglog(dx_values, errors,
              color='black', linestyle='--', linewidth=2, marker='o')
pyplot.axis('equal');"
errors
"# Plot the temperature along the rod.
pyplot.figure(figsize=(6.0, 4.0))
pyplot.xlabel('Distance [m]')
pyplot.ylabel('Temperature [C]')
pyplot.grid()
pyplot.plot(x, T, color='C0', linestyle='-', linewidth=2)
pyplot.xlim(0.0, L)
pyplot.ylim(0.0, 100.0);"
"# Increase the CFL number.
sigma = 5.0
dt = sigma * dx**2 / alpha  # time-step size
nt = 100  # number of time steps to compute

# Compute the temperature along the rod.
T = btcs_implicit(T0, nt, dt, dx, alpha, q)

# Plot the temperature along the rod.
pyplot.figure(figsize=(6.0, 4.0))
pyplot.xlabel('Distance [m]')
pyplot.ylabel('Temperature [C]')
pyplot.grid()
pyplot.plot(x, T, color='C0', linestyle='-', linewidth=2)
pyplot.xlim(0.0, L)
pyplot.ylim(0.0, 100.0);"
"# Set the font family and size to use for Matplotlib figures.
pyplot.rcParams['font.family'] = 'serif'
pyplot.rcParams['font.size'] = 16

# Plot the solution of the elevation.
pyplot.figure(figsize=(9.0, 4.0))  # set the size of the figure
pyplot.title('Elevation of the phugoid over the time')  # set the title
pyplot.xlabel('Time [s]')  # set the x-axis label
pyplot.ylabel('Elevation [m]')  # set the y-axis label
pyplot.xlim(t[0], t[-1])  # set the x-axis limits
pyplot.ylim(40.0, 160.0)  # set the y-axis limits
pyplot.grid()  # set a background grid to improve readability
pyplot.plot(t, z, color='C0', linestyle='-', linewidth=2);"
"# Plot the numerical solution and the exact solution.
pyplot.figure(figsize=(9.0, 4.0))  # set the size of the figure
pyplot.title('Elevation of the phugoid over the time')  # set the title
pyplot.xlabel('Time [s]')  # set the x-axis label
pyplot.ylabel('Elevation [m]')  # set the y-axis label
pyplot.xlim(t[0], t[-1])  # set the x-axis limits
pyplot.ylim(40.0, 160.0)  # set the y-axis limits
pyplot.grid()  # set a background grid to improve readability
pyplot.plot(t, z, label='Numerical',
            color='C0', linestyle='-', linewidth=2)
pyplot.plot(t, z_exact, label='Analytical',
            color='C1', linestyle='-', linewidth=2)
pyplot.legend();  # set the legend"
"# Plot the error versus the time-step size.
pyplot.figure(figsize=(6.0, 6.0))
pyplot.title('L1-norm error vs. time-step size')  # set the title
pyplot.xlabel('$\Delta t$')  # set the x-axis label
pyplot.ylabel('Error')  # set the y-axis label
pyplot.grid()
pyplot.loglog(dt_values, error_values,
              color='C0', linestyle='--', marker='o')  # log-log plot
pyplot.axis('equal');  # make axes scale equally"
"import warnings
warnings.filterwarnings('ignore')
from IPython.display import YouTubeVideo
YouTubeVideo('6i6qhqDCViA')"
"print('Distance traveled: {:.3f}'.format(x_rk2[idx_ground_rk2 - 1]))

# Plot the glider's path for both schemes.
pyplot.figure(figsize=(9.0, 6.0))
pyplot.subplot(121)
pyplot.grid()
pyplot.xlabel('x')
pyplot.ylabel('y')
pyplot.plot(x_euler[:idx_ground_euler], y_euler[:idx_ground_euler],
            label='Euler')
pyplot.plot(x_rk2[:idx_ground_rk2], y_rk2[:idx_ground_rk2],
            label='RK2')
pyplot.legend();
# Let's take a closer look!
pyplot.subplot(122)
pyplot.grid()
pyplot.xlabel('x')
pyplot.ylabel('y')
pyplot.plot(x_euler, y_euler, label='Euler')
pyplot.plot(x_rk2, y_rk2, label='RK2')
pyplot.xlim(0.0, 5.0)
pyplot.ylim(1.8, 2.5);"
"# Plot difference versus the time-step size.
pyplot.figure(figsize=(6.0, 6.0))
pyplot.title('L1-norm of the difference vs. time-step size')
pyplot.xlabel('$\Delta t$')
pyplot.ylabel('Difference')
pyplot.grid()
pyplot.loglog(dt_values[:-1], diff_values[:-1],
              color='C0', linestyle='--', marker='o')
pyplot.axis('equal');"
"print('Distance traveled: {:.3f}'.format(x_leapfrog[idx_ground_leapfrog - 1]))

# Plot the glider's path for the leapfrog scheme.
pyplot.figure(figsize=(9.0, 6.0))
pyplot.subplot(121)
pyplot.grid()
pyplot.xlabel('x')
pyplot.ylabel('y')
pyplot.plot(x_leapfrog[:idx_ground_leapfrog],
            y_leapfrog[:idx_ground_leapfrog])
# Let's take a closer look!
pyplot.subplot(122)
pyplot.grid()
pyplot.xlabel('x')
pyplot.ylabel('y')
pyplot.plot(x_leapfrog, y_leapfrog)
pyplot.xlim(0.0, 5.0)
pyplot.ylim(1.8, 2.5);"
"# Plot the path of the glider.
pyplot.figure(figsize=(9.0, 4.0))
pyplot.title('Path of the glider (flight time = {})'.format(T))
pyplot.xlabel('x')
pyplot.ylabel('y')
pyplot.grid()
pyplot.plot(x, y, color='C0', linestyle='-', linewidth=2);"
"# Plot the difference versus the time-step size.
pyplot.figure(figsize=(6.0, 6.0))
pyplot.title('L1-norm difference vs. time-step size')  # set the title
pyplot.xlabel('$\Delta t$')  # set the x-axis label
pyplot.ylabel('Difference')  # set the y-axis label
pyplot.grid()
pyplot.loglog(dt_values[:-1], diff_values,
              color='C0', linestyle='--', marker='o')  # log-log plot
pyplot.axis('equal');  # make axes scale equally"
"import warnings
warnings.filterwarnings('ignore')
from IPython.display import YouTubeVideo
YouTubeVideo('ysdU4mnRYdM')"
"# Initial conditions: zt = 64.0, z0 = 16.0, theta0 = 0.0.
plot_flight_path(64.0, 16.0, 0.0)"
"# Initial conditions: zt = 64.0, z0 = 16.0, theta0 = 180.0.
plot_flight_path(64.0, 16.0, 180.0)"
"# Initial conditions: zt = 16.0, z0 = 48.0, theta0 = 0.0.
plot_flight_path(16.0, 48.0, 0.0)"
"# Initial conditions: zt = 64.0, z0 = 16.0, theta0 = -90.0.
plot_flight_path(64.0, 16.0, -90.0)"
linear_convection(41)  # solve using 41 spatial grid points
linear_convection(61)
linear_convection(71)
linear_convection(85)
linear_convection_cfl(85)
linear_convection_cfl(121)
"# Plot the initial conditions.
pyplot.figure(figsize=(4.0, 4.0))
pyplot.title('Initial conditions')
pyplot.xlabel('x')
pyplot.ylabel('u')
pyplot.grid()
pyplot.plot(x, u0, color='C0', linestyle='--', linewidth=2)
pyplot.xlim(0.0, L)
pyplot.ylim(0.0, 2.5);"
"# Plot the solution after nt time steps
# along with the initial conditions.
pyplot.figure(figsize=(4.0, 4.0))
pyplot.xlabel('x')
pyplot.ylabel('u')
pyplot.grid()
pyplot.plot(x, u0, label='Initial',
            color='C0', linestyle='--', linewidth=2)
pyplot.plot(x, u, label='nt = {}'.format(nt),
            color='C1', linestyle='-', linewidth=2)
pyplot.legend()
pyplot.xlim(0.0, L)
pyplot.ylim(0.0, 2.5);"
"# Plot the initial conditions.
pyplot.figure(figsize=(4.0, 4.0))
pyplot.title('Initial conditions')
pyplot.xlabel('x')
pyplot.ylabel('u')
pyplot.grid()
pyplot.plot(x, u0, color='C0', linestyle='--', linewidth=2)
pyplot.xlim(0.0, L)
pyplot.ylim(0.0, 2.5);"
"# Plot the solution after nt time steps
# along with the initial conditions.
pyplot.figure(figsize=(4.0, 4.0))
pyplot.xlabel('x')
pyplot.ylabel('u')
pyplot.grid()
pyplot.plot(x, u0, label='Initial',
            color='C0', linestyle='--', linewidth=2)
pyplot.plot(x, u, label='nt = {}'.format(nt),
            color='C1', linestyle='-', linewidth=2)
pyplot.legend()
pyplot.xlim(0.0, L)
pyplot.ylim(0.0, 2.5);"
"# Compute the solution using Jacobi relaxation method.
p, ites, diff = laplace_2d_jacobi(p0, maxiter=20000, rtol=1e-8)
print('Jacobi relaxation: {} iterations '.format(ites) +
      'to reach a relative difference of {}'.format(diff))"
"%%timeit
laplace_2d_jacobi(p0, maxiter=20000, rtol=1e-8)"
"# Compute the analytical solution.
p_exact = laplace_solution(x, y, Lx, Ly)

# Compute the relative L2-norm of the error.
l2_norm(p, p_exact)"
"%%timeit
fib_it(500000)"
"%%timeit
fib_it(500000)"
"%%timeit
fib_it(500000)"
print(numba.__version__)
"# Compute the solution using Jacobi relaxation method.
p, ites, conv_jacobi = laplace_2d_jacobi(p0,
                                         maxiter=20000, rtol=1e-8)
print('Jacobi relaxation: {} iterations '.format(ites) +
      'to reach a relative difference of {}'.format(conv_jacobi[-1]))"
"%%timeit
laplace_2d_jacobi(p0, maxiter=20000, rtol=1e-8)"
"# Compute the solution using Gauss-Seidel relaxation method.
p, ites, conv_gs = laplace_2d_gauss_seidel(p0,
                                           maxiter=20000, rtol=1e-8)
print('Gauss-Seidel relaxation: {} iterations '.format(ites) +
      'to reach a relative difference of {}'.format(conv_gs[-1]))"
"%%timeit
laplace_2d_gauss_seidel(p0, maxiter=20000, rtol=1e-8)"
"# Compute the solution using SOR method.
omega = 1.0
p, ites, conv_sor = laplace_2d_sor(p0, omega,
                                   maxiter=20000, rtol=1e-8)
print('SOR (omega={}): {} iterations '.format(omega, ites) +
      'to reach a relative difference of {}'.format(conv_sor[-1]))"
"# Compute the solution using SOR method.
omega = 1.5
p, ites, conv_sor = laplace_2d_sor(p0, omega,
                                   maxiter=20000, rtol=1e-8)
print('SOR (omega={}): {} iterations '.format(omega, ites) +
      'to reach a relative difference of {}'.format(conv_sor[-1]))"
"%%timeit
laplace_2d_sor(p0, omega, maxiter=20000, rtol=1e-8)"
"# Compute the solution using tuned SOR method.
omega = 2.0 / (1.0 + numpy.pi / nx)
p, ites, conv_opt_sor = laplace_2d_sor(p0, omega,
                                       maxiter=20000, rtol=1e-8)
print('SOR (omega={:.4f}): {} iterations '.format(omega, ites) +
      'to reach a relative difference of {}'.format(conv_opt_sor[-1]))"
"%%timeit
laplace_2d_sor(p0, omega, maxiter=20000, rtol=1e-8)"
"# Compute the relative L2-norm of the error.
l2_norm(p, p_exact)"
"# Plot the convergence history for different methods.
pyplot.figure(figsize=(9.0, 4.0))
pyplot.xlabel('Iterations')
pyplot.ylabel('Relative $L_2$-norm\nof the difference')
pyplot.grid()
pyplot.semilogy(conv_jacobi, label='Jacobi')
pyplot.semilogy(conv_gs, label='Gauss-Seidel')
pyplot.semilogy(conv_sor, label='SOR')
pyplot.semilogy(conv_opt_sor, label='Optimized SOR')
pyplot.legend()
pyplot.xlim(0, 20000);"
"# Set parameters.
Lx = 1.0  # domain length in the x direction
Ly = 1.0  # domain length in the y direction
nx = 41  # number of points in the x direction
ny = 41  # number of points in the y direction

# Create the gridline locations.
x = numpy.linspace(0.0, Lx, num=nx)
y = numpy.linspace(0.0, Ly, num=ny)

# Compute the analytical solution.
p_exact = laplace_solution(x, y, Lx, Ly)

# Plot the analytical solution.
plot_3d(x, y, p_exact)"
"# Plot the initial conditions.
plot_3d(x, y, p0)"
"# Plot the numerical solution.
plot_3d(x, y, p)"
"# Plot the error versus the grid-spacing size.
pyplot.figure(figsize=(6.0, 6.0))
pyplot.xlabel(r'$\Delta x$')
pyplot.ylabel('Relative $L_2$-norm\nof the error')
pyplot.grid()
dx_values = Lx / (numpy.array(nx_values) - 1)
pyplot.loglog(dx_values, errors,
              color='black', linestyle='--', linewidth=2, marker='o')
pyplot.axis('equal');"
"# Plot the error versus the grid-spacing size.
pyplot.figure(figsize=(6.0, 6.0))
pyplot.xlabel(r'$\Delta x$')
pyplot.ylabel('Relative $L_2$-norm\nof the error')
pyplot.grid()
dx_values = Lx / (numpy.array(nx_values) - 1)
pyplot.loglog(dx_values, errors,
              color='black', linestyle='--', linewidth=2, marker='o')
pyplot.axis('equal');"
"# Set the initial conditions.
p0 = numpy.zeros((ny, nx))

# Compute the source term.
b = poisson_source(x, y, Lx, Ly)

# Plot the initial scalar field.
plot_3d(x, y, p0)"
"# Plot the solution.
plot_3d(x, y, p)"
"# Plot the convergence history.
pyplot.figure(figsize=(9.0, 4.0))
pyplot.xlabel('Iterations')
pyplot.ylabel('Relative $L_2$-norm\nof the difference')
pyplot.grid()
pyplot.semilogy(conv, color='C0', linestyle='-', linewidth=2)
pyplot.xlim(0, len(conv));"
"# Plot the error versus the grid-spacing size.
pyplot.figure(figsize=(6.0, 6.0))
pyplot.xlabel(r'$\Delta x$')
pyplot.ylabel('Relative $L_2$-norm\nof the error')
pyplot.grid()
dx_values = Lx / (numpy.array(nx_values) - 1)
pyplot.loglog(dx_values, errors,
              color='black', linestyle='--', linewidth=2, marker='o')
pyplot.axis('equal');"
"plt.plot(x, hrf(x))
plt.xlabel(""time"")
plt.ylabel(""HRF"")"
"plt.plot(x, hrf(x, shift=3))
plt.xlabel(""time"")
plt.ylabel(""HRF"")"
"plt.plot(x, convolved_hrf(x, [0, 5], [1, 1.5]))
plt.xlabel(""time"")
plt.ylabel(""HRF"")"
"plt.plot(x_realistic, convolved_hrf(x_realistic, [0, 5], [1, 1.5]))
plt.xlabel(""time samples"")
plt.ylabel(""HRF"")"
"x = np.arange(0, 100, 0.25)
plt.plot(x, random_voxel_data(x, [0, 2, 15, 37]))"
"plt.figure()
inc = loansData['Monthly.Income']
h = inc.hist()
plt.title('Histogram of Monthly Income')
plt.show()"
"plt.figure()
h = loansData['Monthly.LogIncome'].hist()
plt.title('Histogram of Log(Monthly Income)')
plt.show()"
"import warnings
warnings.filterwarnings('ignore')
import matplotlib.pyplot as plt
%matplotlib inline

plt.figure(figsize=(8,4))
plt.scatter([0,0], [0, 1], color='b')
plt.scatter([10,10], [0, 1], color='r', marker='^')
plt.xlim(-1, 11)
plt.ylim(-1, 11)
plt.xlabel('term 1')
plt.ylabel('term 2')
plt.show()

plt.figure(figsize=(8,4))
plt.scatter([0,0], [0, 1], color='b')
plt.scatter([10,10], [0, 1], color='r', marker='^')
plt.annotate('$\mu_1$', xy=(5,1), size=20)
plt.scatter([5], [1], color='k')
plt.annotate('$\mu_2$', xy=(5,0), size=20)
plt.scatter([5], [0], color='k')
plt.xlim(-1, 11)
plt.ylim(-1, 11)
plt.xlabel('term 1')
plt.ylabel('term 2')
plt.show()

plt.figure(figsize=(8,4))
plt.scatter([0,10], [1, 1], color='b')
plt.scatter([0,10], [0, 0], color='r', marker='^')
plt.annotate('$\mu_1$', xy=(5,1), size=20)
plt.scatter([5], [1], color='k')
plt.annotate('$\mu_2$', xy=(5,0), size=20)
plt.scatter([5], [0], color='k')
plt.xlim(-1, 11)
plt.ylim(-1, 11)
plt.xlabel('term 1')
plt.ylabel('term 2')
plt.show()"
"# Here's an example. 
# Exercise: work out the means/cluster assignments
# until convergence.
def plot(points, cluster_assignments, means):
    plt.figure()
    for point, asg in zip(points, cluster_assignments):
        plt.scatter([point[0]], [point[1]], marker='o' if asg==1 else '^')
    for i, m in enumerate(means):
        plt.annotate('$\mu_%d$' % i, xy=m, size=20)
        plt.scatter([m[0]], [m[1]], color='k')
    plt.show()
    
plot([(0, 0), (0, 1), (2, 0), (3,1), (3,0)], [0,0,0,1,1], [(1,0), (4,0)])"
"fig, ax = plt.subplots()

pdf[1947].plot(ax=ax, color='b', logy=True)
pdf[1967].plot(ax=ax, color='m', logy=True)
pdf[1987].plot(ax=ax, color='r', logy=True)
pdf[2004].plot(ax=ax, color='g', logy=True)
ax.legend()"
"from IPython.html.widgets import interact, fixed"
"# play with the colors 
fig, ax = plt.subplots()

years = [1947, 1967, 1987, 2004]

for year in years:
    darkness = (year-1940)/(2010-1940)
    pdf[year].plot(ax=ax, lw=2, color='b', alpha=darkness, logy=True)
    
ax.set_ylabel('Sales by Industry Rank (log scale)')
ax.set_xlabel('Industry Rank')
ax.legend(years, fontsize=10, handlelength=2, labelspacing=0.15)"
"model = smf.ols(formula = 'GSP ~ UNEMP + P_CAP + PC', data = df).fit()     #Fits the model

print(model.summary())  #Prints off a nice summary


"
"model1970 = smf.ols(formula='GSP ~ UNEMP + P_CAP + PC', data=df[df['YR']==1970]).fit()     #Fits the model

""""""
Notice that we've isolated a subset of the original dataframe (in this case the obs. corresponding to 1970)
by specificying 'data = df[df['YR']==1970]]' in the regression forumula above. 
""""""

print(model1970.summary())  #Prints off a nice summary"
"""""""
Below are a few examples of calling speficic elements of our regression
""""""
model.params          #Produces all coefficient estimates
model.params['P_CAP']  #Produces coefficient estimate for the regressor 'P_CAP'

model.bse               #Standard Errors for all regressors 
model.bse['P_CAP']      #Standard Errors for regressor 'P_CAP'

model.pvalues           #P-values for all regressors
model.pvalues['P_CAP']  #P-values for regressor 'P_CAP'

r_sqr = model.rsquared          #R-squared

print('The R^2 for our regression is',r_sqr)"
"import warnings
warnings.filterwarnings('ignore')
import sys

print('Welcome')
print(sys.version)"
"import warnings
warnings.filterwarnings('ignore')
import pandas as pd             # data package
import matplotlib.pyplot as plt # graphics 
import sys                      # system module, used to get Python version 
import datetime as dt           # date tools, used to note current date  

print('\nPython version: ', sys.version) 
print('Pandas version: ', pd.__version__)
print(""Today's date:"", dt.date.today())"
"# summarize results
for var in list(ep):
    print('\n', var, '\n', ep[var].value_counts().head(5), sep='')"
"ep['Stats'].str.contains('one', na=False).head(10)*1"
"print(""Python version: "", sys.version)
print(""Pandas version: "", pd.__version__)
print(""Matplotlib version: "", ml.__version__)"
"url1 = ""http://data.insideairbnb.com/united-states/""
url2 = ""ny/new-york-city/2016-02-02/data/listings.csv.gz""
full_df = pd.read_csv(url1+url2, compression=""gzip"")
full_df[[""id"", ""last_scraped"", ""name"", ""description"", ""number_of_reviews"", ""price"", ""review_scores_rating""]].head(3)"
full_df.dtypes
"df = full_df[[""id"", ""price"", ""number_of_reviews"", ""review_scores_rating"", ""bedrooms"", ""city"", ""neighbourhood""]]

df.tail(10)"
"df.replace({'price': {'\$': ''}}, regex=True, inplace=True)
df.replace({'price': {'\,': ''}}, regex=True, inplace=True)
df['price'] = df['price'].astype('float64', copy=False)"
"df.plot.scatter(x=""number_of_reviews"", y=""review_scores_rating"", figsize=(10, 8), alpha=0.2)"
"bins = [0, 5, 10, 25, 50, 100, 350]
boxplot_vecs = []

fig, ax = plt.subplots(figsize=(10, 8))

for i in range(1, 7):
    lb = bins[i-1]
    ub = bins[i]
    foo = df[""review_scores_rating""][df[""number_of_reviews""].apply(lambda x: lb <= x <= ub)].dropna()
    boxplot_vecs.append(foo.values)
    
ax.boxplot(boxplot_vecs, labels=bins[:-1])
ax.set_xlabel(""Number of Reviews"")
ax.set_ylabel(""Review Score"")
plt.show()"
"df.plot.scatter(x=""review_scores_rating"", y=""price"", figsize=(10, 8), alpha=0.2)"
"df.plot.scatter(x=""bedrooms"", y=""price"", figsize=(10, 8))"
"df[df[""neighbourhood""] == ""Upper West Side""].plot.scatter(x=""bedrooms"", y=""price"")"
"df[df[""city""] == ""New York""].plot.scatter(x=""bedrooms"", y=""price"")"
"for nuc in {'A','T','G','C'}:
    print(nuc, ':', sum([base==nuc for base in DNA]))"
"# convert the lat-long coordinates into a two-dimensional numpy array and plot it
coordinates = df.as_matrix(columns=['lon', 'lat'])

most_index = df['city'].value_counts().head(6).index
most = pd.DataFrame(df[df['city'].isin(most_index)])
most.drop_duplicates(subset=['city'], keep='first', inplace=True)

plt.figure(figsize=(10, 6), dpi=100)
co_scatter = plt.scatter(coordinates[:,0], coordinates[:,1], c='b', edgecolor='', s=15, alpha=0.3)

plt.title('Scatter plot of the full set of GPS points')
plt.xlabel('Longitude')
plt.ylabel('Latitude')

for i, row in most.iterrows():
    plt.annotate(row['city'], 
                 xy=(row['lon'], row['lat']),
                 xytext=(row['lon'] + 1.5, row['lat'] + 0.6),
                 bbox=dict(boxstyle='round', color='k', fc='w', alpha=0.6),
                 xycoords='data',
                 arrowprops=dict(arrowstyle='->', connectionstyle='arc3,rad=0.5', color='k', alpha=0.8))

plt.show()"
"# performs k-means on a set of observation vectors forming k clusters
# returns a k-length array of cluster centroid coordinates, and the final distortion
cluster_centroids1, distortion = kmeans(w, k, iter=i)

# plot the cluster centroids
plt.figure(figsize=(10, 6), dpi=100)
plt.scatter(cluster_centroids1[:,0], cluster_centroids1[:,1], c='y', s=100)
plt.show()"
"# the kmeans2 function classifies the set of observations into k clusters using the k-means algorithm
# returns a k by N array of centroids found at the last iteration of k-means,
# and an index of the centroid the i'th observation is closest to
# use optional argument minit='points' because the data is not evenly distributed
# minit='points' will choose k observations (rows) at random from data for the initial centroids
cluster_centroids2, closest_centroids = kmeans2(w, k, iter=i, minit='points')

# plot the cluster centroids
plt.figure(figsize=(10, 6), dpi=100)
plt.scatter(cluster_centroids2[:,0], cluster_centroids2[:,1], c='r', s=100)
plt.scatter(w[:,0], w[:,1], c='k', alpha=.3, s=10)
plt.show()"
"# plot the original full data set colored by cluster - not very useful with this many clusters
plt.figure(figsize=(10, 6), dpi=100)
plt.scatter(coordinates[:,0], coordinates[:,1], c=closest_centroids, s=100)
plt.show()"
"print('k =', k)
print('N =', N)

# the size of cluster_centroids1 and cluster_centroids2 should be the same as k
print(len(cluster_centroids1)) # appears some clusters collapsed, giving us a value less than k
print(len(cluster_centroids2))

# the size of closest_centroids should be the same as N
print(len(closest_centroids))

# the number of unique elements in closest_centroids should be the same as k
print(len(np.unique(closest_centroids)))"
"# for each set of coordinates in our full data set, add the closest_centroid from the kmeans2 clustering
rs = pd.DataFrame(df)
rs['closest_centroid'] = closest_centroids

# reduce the data set so there is only one row for each closest_centroid
rs.drop_duplicates(subset=['closest_centroid'], keep='first', inplace=True)
rs.head()"
"# plot the final reduced set of coordinate points
plt.figure(figsize=(10, 6), dpi=100)
plt.scatter(rs['lon'], rs['lat'], c='m', s=100)
plt.show()"
"# plot the cluster centroids vs the whitened coordinate points
plt.figure(figsize=(10, 6), dpi=100)
plt.scatter(cluster_centroids2[:,0], cluster_centroids2[:,1], c='r', alpha=.7, s=150)
plt.scatter(w[:,0], w[:,1], c='k', alpha=.3, s=10)
plt.show()"
"# plot the final reduced set of coordinate points vs the original full set
plt.figure(figsize=(10, 6), dpi=100)
rs_scatter = plt.scatter(rs['lon'], rs['lat'], c='r', alpha=.7, s=150)
df_scatter = plt.scatter(df['lon'], df['lat'], c='k', alpha=.3, s=5)

plt.title('Full data set vs k-means reduced set')
plt.legend((df_scatter, rs_scatter), ('Full set', 'Reduced set'), loc='upper left')
plt.xlabel('Longitude')
plt.ylabel('Latitude')

plt.show()"
"# reshape() since it needs input as a 2D matrix
encoded_dense = ohe_dense.fit_transform(orig.reshape(-1, 1))
encoded_dense"
ohe_dense.active_features_
"decoded_dense = encoded_dense.dot(ohe_dense.active_features_).astype(int)
decoded_dense"
ohe_sparse.active_features_
"np.allclose(orig, encoded_sparse.dot(ohe_sparse.active_features_))"
"import warnings
warnings.filterwarnings('ignore')
class Ball(object):
    pass

b = Ball()
b.__repr__()"
print(b)
"i = Image(filename='./images/ipython-image.png')
display(i)"
i
"from IPython.display import YouTubeVideo
YouTubeVideo('sjfsUzECqK0')"
"from IPython.display import IFrame
IFrame('https://ipython.org', width='100%', height=350)"
"from IPython.display import FileLink, FileLinks
FileLink('../Visualize/01-Introduction.ipynb')"
FileLinks('./')
"Image(""images/mackinlay1.png"", width=600)"
"Image(""images/mackinlay2.png"", width=600)"
"for i in range(10):
    print(np.random.rand())"
"μ = 2.0
σ = np.sqrt(1.0)
for i in range(10):
    print(np.random.normal(μ, σ))"
"for i in range(10):
    print(np.random.choice(['summer', 'winter'], p=[0.25, 0.75]))"
"np.random.choice(['summer', 'winter'], p=[0.25, 0.75], size=10)"
x[0:10]
"pdf, bins, patches = plt.hist(x, normed=True)
plt.xlabel('x')
plt.ylabel('P(x)');"
"interact(plot_uniform_pdf, n=(100,2000,100));"
"plt.hist(data, bins=20);"
"mu_hat = data.mean()
mu_hat"
"var_hat = data.var()
var_hat"
"theta_hat = [mu_hat, var_hat]
theta_hat"
data.min()
"for i in range(10):
    print(biased_var(theta, observations))"
"bias1 = var_dist1.mean() - theta[1]
bias2 = var_dist2.mean() - theta[1]
print(""Bias of biased esimator: {}"".format(bias1))
print(""Bias of unbiased esimator: {}"".format(bias2))"
"plt.plot(m, biased_data, label='Biased')
plt.plot(m, unbiased_data, label='Unbiased')
plt.hlines(theta[1], 0, 100, color='grey', alpha=0.8, label=""True value"")
plt.ylabel('Variance estimate')
plt.xlabel('Number of observations (m)')
plt.title('Biased/Unbiased Estimators: Variance of the Normal Dist.')
plt.legend();"
"fig, ax = plt.subplots(2, 1, sharex=True)
ax[0].hist(var_dist1, bins=20, normed=True)
ax[0].set_title('Biased')
ax[1].hist(var_dist2, bins=20, normed=True)
ax[1].set_title('Unbiased')
ax[1].set_xlabel('Estimated Variance')
plt.tight_layout();"
"var_dist1.mean(), var_dist2.mean()"
"var_dist1.var(), var_dist2.var()"
"m = 100
data = np.random.choice(['H','T'], m, p=[0.5, 0.5])
data"
fair(data)
"data2 = np.random.choice(data, size=len(data))
data2"
fair(data2)
"for i in range(20):
    re_data = bootstrap_data(data)
    print(fair(re_data))"
fairs
"plt.hist(fairs, bins=20)
plt.xlim(0, 1.0)
plt.title(""Bootstrapped coin fairness"")
plt.xlabel(""Fairness"")
plt.grid(True)
plt.ylabel(""Count"");"
"print(""Mean fairness:"", fairs.mean())
print(""95% confidence interval:"", np.percentile(fairs, (2.5,97.5)))"
"bias = fairs.mean() - fair(data)
var = fairs.var()
bias, var"
"x = ""abc""
# nur solche, die nicht mit ""_"" beginnen
print([a for a in dir(x) if not a.startswith(""_"")])"
inspect.getargspec(f2)
"x = 42
frame = inspect.currentframe()
print(inspect.getframeinfo(frame))
print(""Lokale Variable x: %s"" % frame.f_locals['x'])"
"# xx und yy werden die x und y koordinaten aller Punkte beinhalten.

xx = []
yy = []

for i in range(-20, 30):
    x = i / 10.
    y = -.3 * x** 5 + 10 * x**2 + x - 1
    xx.append(x)
    yy.append(y)

plt.plot(xx, yy)"
"# selbe daten, andere Darstellung
plt.plot(xx, yy, marker=""o"", color=""cyan"", ms=10, linestyle="""")"
"xx = [5, -6, 1, 8, -1, 11]
yy = [1, 2, 6, -1, 0, 1]

plt.scatter(xx, yy, s=100, color=""red"")
plt.scatter([2, 6], [3, 3], s=200, color=""blue"")"
"N = 100
x = np.random.randn(N)
y = 5 * np.random.rand(N)
colors = np.random.rand(N)
area = np.pi * (10 * np.random.rand(N) + y)**2

plt.scatter(x, y, s=area, c=colors, alpha=0.5)"
"X = np.linspace(-6, 6, 100) # 100 ist die Anzahl der gleichmäßig verteilten Punkte
Y = np.linspace(-3, 3, 100)
X, Y = np.meshgrid(X, Y)

U = -2 - X + 2 * Y**2
V = 1 + np.sqrt(X**2 + Y**2)

plt.streamplot(X, Y, U, V, color=U, linewidth=2, cmap=plt.cm.summer)
plt.colorbar()"
"speed = np.sqrt(U**2 + V**2)
speed /= speed.max()

plt.streamplot(X, Y, U, V, color=speed, linewidth=10*speed, cmap=plt.cm.summer)
plt.colorbar()"
"import matplotlib.pyplot as plt
import numpy as np

xx = np.linspace(-3, 3, 100)
yy = -xx**2 + 2*xx - 1

plt.plot(xx, yy)
plt.ylim(-20, 10)
ax = plt.gca()
ax.annotate('Maximum',
            xy=(1, 0), xycoords='data',
            xytext=(-2, 5), textcoords='data', size = 20,
            bbox = {'boxstyle' : 'round', 'fc' : '0.9'},
            arrowprops = {'arrowstyle' : '->', 'connectionstyle' : 'angle,angleA=0,angleB=270,rad=10'})"
"fp.cplot(lambda z : z, (-3, 3), (-3, 3), points=50000)"
"fp.cplot(lambda z: fp.sin(z.real**2 + z.imag**2) + z.imag*1j, (-4, 4), (-4, 4), points=30000)"
"rcParams[""figure.figsize""] = (12,10)
fp.cplot(fp.cos, (-7, 7), (-4, 4), points=50000)"
"import matplotlib.pyplot as plt

kochzeit = np.array([30, 5, 10, 50, 5])
verarbeitung = np.array([30,20,40, 5, 10])
vorbereitung = np.array([10, 5, 5, 7, 5])
idx = np.arange(len(kochzeit))
speisen = [""$S_{%d}$""%i for i in idx]

p1 = plt.bar(idx, kochzeit, align=""center"", color='orange', 
             label='Kochzeit')
p2 = plt.bar(idx, verarbeitung, align=""center"", color='y', 
             bottom=kochzeit, 
             yerr=.2 * kochzeit, 
             label='Verarbeitung')
p3 = plt.bar(idx, vorbereitung, align=""center"", color='g', 
             bottom=kochzeit+verarbeitung, 
             label='Vorbereitung')

plt.title('Speisezubereitung')
plt.xticks(idx, speisen)
plt.ylabel(""Minuten"")
plt.legend()
plt.legend()"
"import numpy as np
data1 = np.r_[4 * np.random.randn(1000) - 3,
              np.linspace(3, 6, 1000)**2 + np.random.randn(1000)]
_ = plt.hist(data1, 25, color=""grey"")"
"plt.scatter(data1, data2, c=""black"", marker=""x"", alpha=.5)"
"plt.hist2d(data1, data2, bins=30, cmap=plt.cm.bone_r)
plt.grid()
plt.colorbar()"
"plt.hexbin(data1, data2, gridsize=25, cmap=plt.cm.bone_r)
plt.colorbar()"
"fig, axes = plt.subplots(2, 2)
ax = axes[1, 0]
ax.add_artist(plt.Circle((2, 1), 3, color=""yellow""))
ax.set_xlim(-10, 10)
ax.set_ylim(-5, 5)
axes[1,1].grid()"
"fig, axes = plt.subplots(1, 2, sharey=True)
xx = np.linspace(-10, 10, 1000)
[ax.grid() for ax in axes] # grid für beide
axes[0].plot(np.sin(xx * xx))
axes[1].plot(np.cos(xx) * xx, color=""red"")
fig.subplots_adjust(wspace=0) # no horizontal space"
"fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(10,3))
xx = np.linspace(1, 20, 1000)
yy = np.log(xx**2 + 1) * np.sin(xx) + xx
ax1.plot(xx, yy)
ax2.scatter(sorted(xx), sorted(yy), s=1, marker=""."", color=""green"")
ax3.hist(yy, bins=20, color=""red"", alpha=.5)
fig.tight_layout()"
"xx = np.linspace(0, 4 * np.pi, 1000)
yy = np.sin(xx) + np.cos(2 * xx) + 3
plt.close(""all"")
axpolar = plt.gca(projection='polar')
axpolar.plot(xx, yy)
axpolar.plot(xx + .5 * np.pi, 1.2 * yy, color=""green"")
axpolar.plot(xx +  1 * np.pi, 1.5 * yy, color=""red"")
axpolar.set_rmax(7)"
"from mpl_toolkits.mplot3d import axes3d
import matplotlib.pyplot as plt
from matplotlib import cm

fig = plt.figure()
ax = fig.gca(projection='3d')
X, Y, Z = axes3d.get_test_data(0.05)
ax.plot_surface(X, Y, Z, rstride=8, cstride=8, alpha=0.3)
cset = ax.contour(X, Y, Z, zdir='z', offset=-100, cmap=cm.coolwarm)
cset = ax.contour(X, Y, Z, zdir='x', offset=-40, cmap=cm.coolwarm)
cset = ax.contour(X, Y, Z, zdir='y', offset=40, cmap=cm.coolwarm)

ax.set_xlabel('X')
ax.set_xlim(-40, 40)
ax.set_ylabel('Y')
ax.set_ylim(-40, 40)
ax.set_zlabel('Z')
ax.set_zlim(-100, 100)

plt.show()"
"from matplotlib import cm
from IPython.html.widgets import interact

sr = (-10, 10, .1)
@interact(a = sr, b = sr, c = sr, d = sr, elevation=(0,90), azimuth=(0,180))
def xy_grid3d(a = -2, b = -4, c = 4, d = 4, elevation=40, azimuth=30):
    X = np.linspace(-10, 10, 100)
    Y = np.linspace(-10, 10, 100)
    X, Y = np.meshgrid(X, Y)
    
    Z = 15 * np.exp(-.5 * (X - a)**2 - (Y - b)**2) 
    Z += - 25 * np.exp(.15 * (-(X - c)**2 - (Y - d)**2))
    Z += - .1 * (np.sin(X) - Y)**2

    fig = plt.figure()
    ax = fig.gca(projection='3d')
    ax.view_init(elev=elevation, azim=azimuth)
    ax.plot_surface(X, Y, Z, rstride=5, cstride=5, alpha=0.65, cmap=cm.afmhot)
    
    #cset = ax.contour(X, Y, Z, zdir='z', offset=-30, cmap=cm.coolwarm)
    #cset = ax.contour(X, Y, Z, zdir='x', offset=-15, cmap=cm.coolwarm)
    #cset = ax.contour(X, Y, Z, zdir='y', offset=-15, cmap=cm.coolwarm)
    
    ax.set_xlabel('X')
    ax.set_xlim(-10, 10)
    ax.set_ylabel('Y')
    ax.set_ylim(-10, 10)
    ax.set_zlabel('Z')
    ax.set_zlim(-30, 15)
    
    # Colorbar auf der rechten Seite
    m = plt.cm.ScalarMappable(cmap=cm.afmhot)
    m.set_array(Z)
    plt.colorbar(m, shrink=.5)
    
    plt.show()"
"p = plot(sqrt(x) * cos(x), sqrt(x), -sqrt(x), (x, 0, 10), show=False)
p[0].line_color=""red""
p[1].line_color=""blue""
p[2].line_color=""blue""
p.show()"
"p = plot_implicit(Or(
                 And(
                    x**2 + y**2 < 25, 
                    x**2 + y**2 > 10, 
                 ), 
                 x**2 - 5*x < y - 10), 
              (x,-10, 10), (y, -10, 10))"
"from sympy.plotting import plot3d_parametric_surface
from sympy import pi
from sympy.abc import u, v
R = 10
r = 3
p = plot3d_parametric_surface(
    (R + r * cos(u)) * cos(v),
    (R + r * cos(u)) * sin(v), 
    r * cos(u),
    (u, -pi, pi), (v, -pi, pi))"
"import warnings
warnings.filterwarnings('ignore')
import numpy as np
from scipy.spatial import ConvexHull

pts = np.random.normal(size=(100,2))
chull = ConvexHull(pts)
chull.vertices"
"%matplotlib inline
import matplotlib.pyplot as plt
for simplex in chull.simplices:
    plt.plot(pts[simplex, 0], pts[simplex, 1], ""g-"", lw=2, zorder=1)
plt.scatter(pts[:,0], pts[:,1], zorder=2)"
"import sympy as sy
from sympy.geometry import Point, Segment, Circle, Line, Triangle"
seg1.intersection(seg2)
seg1.midpoint
"l2 = Line(Point(2, -4), Point(3, -2))
c.intersection(l2)"
"xx = [20, 15,  11, 7.57, 9.6, 13,  21,  24, 28, 23, 20]
yy = [3, 2.5, 2.3, 2.66,  3, 3.1, 2.3, 2.5, 2.8, 3,  3]

plt.plot(xx, yy, ""o-"")
plt.xlim(5, 30)
plt.ylim(2, 3.3)"
"from sympy.plotting import plot
plot(sin_square, (x, -3, 15))"
"from IPython.html.widgets import interact
from sympy.abc import x
from sympy import exp, cos
from sympy.plotting import plot

@interact(a = (0, 20, .1), b = (0, 3, .1))
def damped_oscillation(a = 10, b = .5):
    f2 = a * exp(-b * x) * cos(a * x) / a
    plot(f2, (x, 0, 10), ylim=(-1, 1))"
"from IPython.html.widgets import interact
import numpy as np

@interact(a=(-2, 2, .1), b =(-2, 2, .1), c=(-2, 2, .1), d=(-2, 2, .1))
def elliptic_plot(a = 1., b = 0, c = 0, d = -1):
    phi = np.linspace(0, 8 * np.pi, 1000)
    m = np.array([[a, b],
                  [c, d]])
    v = np.c_[np.sin(phi), 
              np.cos(phi)].T
    r = np.log1p(phi)
    
    xx, yy = m.dot(r * v)
    
    plt.plot(xx, yy)
    plt.grid()
    plt.ylim(-4, 4)
    plt.xlim(-4, 4)"
"_ = plt.hist(data, 20, color=""grey"")"
"from sympy.ntheory.modular import crt
crt([2, 3, 13], [1, 2, 7])"
"import warnings
warnings.filterwarnings('ignore')
authornames = ! git log --pretty=format:""%aN <%aE>"" HEAD
for name in sorted(set(authornames), key = lambda n : n.split(""<"")[0].split()[-1]):
    print(name)"
w
v + w
v - w
v * w
v.dot(w)
"x = np.array([1,2,3])
np.sin(x)"
np.exp(x)
"np.arange(1, 5, .5)"
"np.linspace(0, 2, 10)"
"np.logspace(-1, 2, 10, base=10)"
f2
"eigenvalues, eigenvectors = LA.eig(m)
print(eigenvalues)
print(eigenvectors)"
LA.inv(m)
LA.inv(m).dot(m)
"opti1 = minimize(f1, x0, bounds = bounds)
opti1"
"x_opti = opti1[""x""]
x_opti"
"f3_root = root(f3, [2., 1.])
f3_root"
"import warnings
warnings.filterwarnings('ignore')
import datetime
print(""Zeitpunkt: %s"" % datetime.date.today())

import sys
print(""Python: %s"" % sys.version.splitlines()[0])

# bs4: beautifulsoup4
libs = ['numpy', 'scipy', 'matplotlib', 'sympy', 'mpmath', 'pandas', 'statsmodels',
        'sklearn', 'networkx', 'yaml', 'json', 'csv', 'sqlite3', 'cython', ""bs4""]
from importlib import import_module
for lib_name in sorted(libs):
    lib = import_module(lib_name)
    try:
        vers = lib.__version__
    except:
        vers = lib.version
    print(""{:<15s} {}"".format(lib_name, vers))"
"plt.plot(X.ravel(), Y.ravel(), 'k.')
for vert in verts:
    plt.fill(vert[:,0], vert[:,1], edgecolor='none', alpha=0.2)"
"T = matplotlib.transforms.Affine2D()
T.rotate_around(1, 1, 0.1 * np.pi)
"
"XY_t = T.transform(np.c_[X.ravel(), Y.ravel()])
X_t, Y_t = XY_t[:,0].reshape(X.shape), XY_t[:,1].reshape(Y.shape)
vert_t = [T.transform(vert) for vert in verts]
vert_t = grid2enclosure(X_t, Y_t)

plt.plot(X_t, Y_t, 'k.')
for vert in vert_t:
    plt.fill(vert[:,0], vert[:,1], edgecolor='none', alpha=0.2)"
"T.skew(0.8, 0.3)
"
"XY_t = T.transform(np.c_[X.ravel(), Y.ravel()])
X_t, Y_t = XY_t[:,0].reshape(X.shape), XY_t[:,1].reshape(Y.shape)
vert_t = [T.transform(vert) for vert in verts]
vert_t = grid2enclosure(X_t, Y_t)

plt.plot(X_t, Y_t, 'k.')
for vert in vert_t:
    plt.fill(vert[:,0], vert[:,1], edgecolor='none', alpha=0.2)"
"# note that this has 1 cell less
mesh = plt.pcolormesh(X_t, Y_t, X_t)"
"vor = scipy.spatial.Voronoi(np.c_[X_t.ravel(), Y_t.ravel()])
scipy.spatial.voronoi_plot_2d(vor)"
"# Note on np.random.nrand()
# distribution of mean 0 and variance 1
# returns number of integers passed
np.random.randn(4)"
grey_height[:5]
"plt.hist([grey_height, lab_height], stacked=True, color=['r', 'b'])
plt.show()"
"f = y(n) - Fraction(295, 100)*y(n - 1) + 2*y(n - 2)
display(f)"
"eqn = Eq(f, 0)
soln = rsolve(eqn, y(n))
display(soln)
soln.evalf()"
"# Create non-homogeneous equation
eqn = Eq(f, -63.685*(1.07**n))
display(eqn)"
"soln = rsolve(eqn, y(n), init={y(0) : 2000, y(1) : 2200})
display(soln)
soln.evalf()"
"# Plot
plot(soln, (n, 0, 12), xlabel=""year (n)"", ylabel=""fee"")

# Evaluate fee at 10 years
print(""Fee after 10 years (n=10): {}"".format(soln.subs(n, 10).evalf()))"
"n = Symbol(""n"", integer=True)
y = Function(""y"")

a, b, c = symbols(""a b c"")

f = a*y(n) - b*y(n - 1) + c*y(n - 2)
display(f)"
"eqn = Eq(f, 0)
soln = rsolve(eqn, y(n))
display(soln)"
"soln = rsolve(eqn, y(n), init={y(0) : 2, y(1) : 1})
display(soln)"
"eqn1 = eqn.subs('a', 4).subs('b', 1).subs('c', 2)
soln = rsolve(eqn1, y(n), init={y(0) : 1, y(1) : 2})
display(soln)

plot(soln, (n, 0, 12), xlabel=""$n$"", ylabel=""$y_{n}$"");"
"f = a*y(n) - b*y(n - 1) + c*y(n - 2)
eqn = Eq(f, 0)
display(eqn)"
"def plot_solution(a=6, b=1, c=8):
    # Substitute in parameters
    eqn1 = eqn.subs('a', a).subs('b', b).subs('c', c)
    
    # Check roots
    roots = ((-b - sqrt(b**2 - 4*a*c))/(2*a), (-b + sqrt(b**2 - 4*a*c))/(2*a))
    print(""Roots of characteristic eqn:"")
    display(roots)

    # Solve
    soln = rsolve(eqn1, y(n), init={y(0) : 1, y(1) : 2})
    print(""Solution:"")
    display(soln)
    
    # Plot
    plot(soln, (n, 0, 20), xlabel=""$n$"", ylabel=""$y_n$"", axis_center=""auto"", adaptive=False, nb_of_points=21)

    
interact(plot_solution, a=(-10, 11, 2), b=(-2, 20, 1), c=(1, 10, 1), continuous_update=False);"
"eqn = Eq(m*Derivative(y(t), t, t) + lmbda*Derivative(y(t), t) + k*y(t), 0)
display(eqn)"
"y = dsolve(eqn, y(t))
display(y)"
"y = Function(""y"")
x = symbols(""x"")
eqn = Eq(Derivative(y(x), x, x) + 2*Derivative(y(x), x) - 3*y(x), 0)
display(eqn)"
"y1 = dsolve(eqn)
display(y1)"
"eqn = Eq(lmbda**2 + 2*lmbda -3, 0)
display(eqn)"
solve(eqn)
"# Compute determinant
detA = np.linalg.det(A)
print(""Determinant of A: {}"".format(detA))

# Compute inverse
Ainv = np.linalg.inv(A)
print(""Inverse of A"")
print(Ainv)

# Check that inverse is correct
print(""A*A^-1: {}"".format(A*Ainv))"
"%matplotlib inline

# Set up plotting environment
from mpl_toolkits.mplot3d import Axes3D
import matplotlib.pyplot as plt
import numpy as np
from itertools import product, combinations

fig = plt.figure()
ax = fig.gca(projection='3d')
ax.set_aspect(""equal"")

# Draw cube
r = [0, 1]
for s, e in combinations(np.array(list(product(r, r, r))), 2):
    if np.sum(np.abs(s - e)) == r[1] - r[0]:
        ax.plot3D(*zip(s, e), color=""b"", marker=""o"")"
"# Draw orginal cube and transformed shape
fig = plt.figure()
ax = fig.gca(projection='3d')
ax.set_aspect(""equal"")
r = [0, 1]
for s, e in combinations(np.array(list(product(r, r, r))), 2):
    if np.sum(np.abs(s - e)) == r[1] - r[0]:
        ax.plot3D(*zip(s, e), color=""b"", marker=""o"")
        
        s = A.dot(s)
        e = A.dot(e)
        ax.plot3D(*zip(s, e), color=""r"", marker=""o"")"
"# Create a transformation matrix (diagonal)
A = np.array([[0.8, 0.0, 0], [0.0, 0.0, 0.0], [0.0, 0.0, 0.7]])

# Check determinant
print(""Det A: {}"".format(np.linalg.det(A)))

# Draw orginal cube and transformed shape
fig = plt.figure()
ax = fig.gca(projection='3d')
ax.set_aspect(""equal"")
r = [0, 1]
for s, e in combinations(np.array(list(product(r, r, r))), 2):
    if np.sum(np.abs(s - e)) == r[1] - r[0]:
        ax.plot3D(*zip(s, e), color=""b"", marker=""o"")
        s = A.dot(s)
        e = A.dot(e)
        ax.plot3D(*zip(s, e), color=""r"", marker=""o"")
        
        "
"# Create a transformation matrix (diagonal)
A = np.array([[0.8, 0.8, 0.8], [0.6, 1.0, 0.0], [-1.1, 0.0, 0.7]])

# Check determinant
print(""Det A: {}"".format(np.linalg.det(A)))

# Draw orginal cube and transformed shape
fig = plt.figure()
ax = fig.gca(projection='3d')
ax.set_aspect(""equal"")
r = [0, 1]
for s, e in combinations(np.array(list(product(r, r, r))), 2):
    if np.sum(np.abs(s - e)) == r[1] - r[0]:
        ax.plot3D(*zip(s, e), color=""b"", marker=""o"")
        
        s = A.dot(s)
        e = A.dot(e)
        ax.plot3D(*zip(s, e), color=""r"", marker=""o"")"
"# Create a transformation matrix (diagonal)
A = np.array([[2.0, 0.0, 0.0], [0.0, 1.1, 0.0], [0.0, 0.0, 1.1]])

# Check determinant
print(""Det A: {}"".format(np.linalg.det(A)))

# Draw orginal cube and transformed shape
fig = plt.figure()
ax = fig.gca(projection='3d')
ax.set_aspect(""equal"")
r = [0, 1]
for s, e in combinations(np.array(list(product(r, r, r))), 2):
    if np.sum(np.abs(s - e)) == r[1] - r[0]:
        ax.plot3D(*zip(s, e), color=""b"", marker=""o"")
        
        s = A.dot(s)
        e = A.dot(e)
        ax.plot3D(*zip(s, e), color=""r"", marker=""o"")"
"# Multiply A by -1 and print determinant
A = -A
print(""Det A: {}"".format(np.linalg.det(A)))

# Draw orginal cube and transformed shape
fig = plt.figure()
ax = fig.gca(projection='3d')
ax.set_aspect(""equal"")
r = [0, 1]
for s, e in combinations(np.array(list(product(r, r, r))), 2):
    if np.sum(np.abs(s - e)) == r[1] - r[0]:
        ax.plot3D(*zip(s, e), color=""b"", marker=""o"")
        
        s = A.dot(s)
        e = A.dot(e)
        ax.plot3D(*zip(s, e), color=""r"", marker=""o"")"
"import itertools

# Build pairs (0,0), (0,1), . . . (0, n-1), (1, 2), (1, 3), . . . 
pairs = itertools.combinations_with_replacement(range(len(evectors)), 2)

# Compute dot product of eigenvectors x_{i} \cdot x_{j}
for p in pairs:
    e0, e1 = p[0], p[1]
    print (""Dot product of eigenvectors {}, {}: {}"".format(e0, e1, evectors[:, e0].dot(evectors[:, e1])))"
"# Compute dot product of eigenvectors x_{i} \cdot x_{j}
pairs = itertools.combinations_with_replacement(range(len(evectors)), 2)
for p in pairs:
    e0, e1 = p[0], p[1]
    print (""Dot product of eigenvectors {}, {}: {}"".format(e0, e1, evectors[:, e0].dot(evectors[:, e1])))"
"Lambda = np.linalg.inv(evectors).dot(A.dot(evectors))
print(Lambda)"
"# Create a symmetric matrix
S = A + A.T

# Compute eigenvectors of S and print eigenvalues
lmbda, U = np.linalg.eig(S)
print(lmbda)

# R matrix
R = U.T

# Diagonalise S
Lambda = R.dot(S.dot(R.T))
print(Lambda)"
"# Create starting vector
x0 = np.random.rand(S.shape[0])

# Perform power iteration
for i in range(10):
    x0 = S.dot(x0)
    x0 = x0/np.linalg.norm(x0)
x1 = S.dot(x0)

# Get maxiumum exact eigenvalue (absolute value)
eval_max_index = abs(lmbda).argmax()
max_eig = lmbda[eval_max_index]

# Print estimated max eigenvalue and error 
max_eig_est = np.sign(x1.dot(x0))*np.linalg.norm(x1)/np.linalg.norm(x0)
print(""Estimate of largest eigenvalue: {}"".format(max_eig_est))
print(""Error: {}"".format(abs(max_eig - max_eig_est)))"
"# Create starting vector
x0 = np.random.rand(S.shape[0])

# Get eigenvector associated with maxiumum eigenvalue
eval_max_index = abs(lmbda).argmax()
evec_max = U[:,eval_max_index]

# Make starting vector orthogonal to eigenvector associated with maximum 
x0 = x0 - x0.dot(evec_max)*evec_max

# Perform power iteration
for i in range(10):
    x0 = S.dot(x0)
    x0 = x0/np.linalg.norm(x0)
x1 = S.dot(x0)

# Print estimated max eigenvalue and error
max_eig_est = np.sign(x1.dot(x0))*np.linalg.norm(x1)/np.linalg.norm(x0)
print(""Estimate of largest eigenvalue: {}"".format(max_eig_est))
print(""Error: {}"".format(abs(max_eig - max_eig_est)))   

# Get second largest eigenvalue
print(""Second largest eigenvalue (exact): {}"".format(lmbda[np.argsort(abs(lmbda))[-2]]))  "
"rayleigh_quotient = x1.dot(S).dot(x1)/(x1.dot(x1))
print(""Rayleigh_quotient: {}"".format(rayleigh_quotient))"
"x = Symbol(""x"")
y = Function(""y"")
f = Function(""f"")
eqn = Eq(Derivative(y(x), x, x) + 2*Derivative(y(x), x) + y(x), 0)
display(eqn)
dsolve(eqn)"
"eqn = Eq(Derivative(y(x), x, x) + 2*Derivative(y(x), x) + y(x), f(x))
dsolve(eqn)"
"# Compute eigenvectors to generate a set of orthonormal vector
evalues, evectors = np.linalg.eig(A)

# Verify that eigenvectors R[i] are orthogonal (see Lecture 8 notebook)
import itertools
pairs = itertools.combinations_with_replacement(range(np.size(evectors, 0)), 2)
for p in pairs:
    e0, e1 = p[0], p[1]
    print(""Dot product of eigenvectors vectors {}, {}: {}"".format(e0, e1, evectors[:, e0].dot(evectors[:, e1])))"
"Ap = (R).dot(A.dot(R.T))
print(Ap)"
"eqn = Eq(m*Derivative(y(t), t, t) + nu*Derivative(y(t), t) + k*y(t), f(t))
display(eqn)"
"soln = dsolve(eqn)
display(soln)"
"def plot_solution(m=10, ν=1, k=20):
    # Substitute in parameters
    eqn1 = eqn.subs(f(t), 0).subs('m', m).subs('nu', ν).subs('k', k)
    
    roots = (-ν - sqrt(ν**2 - 4*m*k))/(2*m), (-ν + sqrt(ν**2 - 4*m*k))/(2*m)
    print(""Roots of characteristic eqn:"", roots)
    
    # Solve and insert constants
    soln = dsolve(eqn1)
    soln = soln.subs('C1', 0).subs('C2', 1)

    # Plot position vs time
    plot(soln.args[1], (t, 0.0, 20.0), ylim=(-1, 1), xlabel=""time"", ylabel=""displacement"");

interact(plot_solution, m=(1, 100, 1), ν=(0, 30, 0.5), k=(0.1, 100, 1));"
"# Define natural frequency
ω = Symbol(""omega"")
ω = sqrt(k/m)

# Set f(t) = sin(\omega t)
α = symbols(""alpha"")
β = symbols(""beta"")

eqn1 = eqn.subs(f(t), sin(ω*t*α/β))
display(eqn1)"
"# Set numerical values for m, lambda and k
m, nu, k = 4, 0, 1
eqn1 = eqn1.subs('m', m).subs('k', k).subs('nu', nu)

def plot_solution(α=0, β=1):
    # Substitute in parameters
    eqn2 = eqn1.subs('alpha', α).subs('beta', β)
    
    print(""Forcing frequency as fraction of natural frequency: "", α/β)    
    
    # Solve and insert constants
    soln = dsolve(eqn2)
    soln = soln.subs('C1', 0).subs('C2', 1)
    display(soln)
           
    # Plot position vs time
    plot(soln.args[1], (t, 0.0, 200.0), xlabel=""time"", ylabel=""displacement"");

interact(plot_solution, α=(0, 10, 1), β=(1, 10, 1));"
"eqn = Eq(Derivative(x(t), t), v0*exp(-t/(tau)))
display(eqn)"
"x = dsolve(eqn, x(t))
display(x)"
"x = x.subs('C1', v0*tau)
display(x)"
"x = x.subs(v0, 100)

def plot(τ=1.0):
    x1 = x.subs(tau, τ)

    # Plot position vs time
    sympy.plot(x1.args[1], (t, 0.0, 10.0), xlabel=""time"", ylabel=""position"");

interact(plot, τ=(0.0, 10, 0.2));"
"t, m, k, alpha = symbols(""t m k alpha"")
v = Function(""v"")
eqn = Eq((m/k)*Derivative(v(t), t), alpha*alpha - v(t)*v(t))
display(eqn)"
"v = dsolve(eqn, v(t))
display(v)"
print(Q.dot(Q.T))
"import itertools

# Build pairs (0,0), (0,1), . . . (0, n-1), (1, 2), (1, 3), . . . 
pairs = itertools.combinations_with_replacement(range(len(Q)), 2)

# Compute dot product of column vectors q_{i} \cdot q_{j}
for p in pairs:
    col0, col1 = p[0], p[1]
    print (""Dot product of column vectors {}, {}: {}"".format(col0, col1, Q[:, col0].dot(Q[:, col1])))"
"# Compute dot product of row vectors q_{i} \cdot q_{j}
pairs = itertools.combinations_with_replacement(range(len(Q)), 2)
for p in pairs:
    row0, row1 = p[0], p[1]
    print (""Dot product of row vectors {}, {}: {}"".format(row0, row1, Q[row0, :].dot(Q[row1, :])))"
"import warnings
warnings.filterwarnings('ignore')
from sympy import *
init_printing()
from IPython.display import display
from sympy.plotting import plot_parametric

# This command makes plots appear inside the browser window
%matplotlib inline

# Create independent variable and function for x and y
s = Symbol('s')
xs = Function('xs')
ys = Function('ys')

# Pick concrete values for a and b
a = 5
b = 3

# Parametric representation of ellipse
xs = a*cos(s)
ys = b*sin(s)

# Plot parametric line
plot_parametric(xs, ys)"
"x, y = symbols('x y')
f = Eq(x**2/a**2 + y**2/b**2, 1)
display(f)
grad_f = (diff(f.args[0], x), diff(f.args[0], y))
display(grad_f)"
"from sympy.utilities.lambdify import lambdify
import numpy as np
import matplotlib.pyplot as plt

# Set plot limits (based on ellipse size)
plt.xlim(-(a + 1), a + 1)
plt.ylim(-(b + 1), b + 1)

# Make plot aspect ratio equal -> normal lines appear normal
ax = plt.axes()
ax.set_aspect('equal')

# Prepare the symbolix expression for x(s) and y(s) for plotting
xs = lambdify(s, xs, modules=['numpy'])
ys = lambdify(s, ys, modules=['numpy'])

# Plot ellipse
s = np.linspace(0, 7, 300)
plt.plot(xs(s), ys(s))

# Add tangent vector to plot
ax.arrow(x0, y0,  float(t0[0]),  float(t0[1]), label='tangent', color='g')
ax.arrow(x0, y0, -float(t0[0]), -float(t0[1]), color='g')

# Add normal vector to splot
ax.arrow(x0, y0,  float(n0[0]),  float(n0[1]), label='normal', color='r')

# Show plot
plt.show()"
"sample = np.random.normal(5, 4, 100)
sample_std = np.std(sample)
sample_mean = np.mean(sample)


print(""Sample standard deviation (found via np.std): %s"" % str(sample_std))
print(""Sample mean (found via np.mean): %s"" % str(sample_mean))"
"result = minimize(nll, [4,1], args=(sample))
mle_std = result[""x""][0]
mle_mean = result[""x""][1]

print(""Sample standard deviation (found via MLE): %s"" % str(mle_std))
print(""Sample mean (found via MLE): %s"" % str(mle_mean))"
"result = minimize(nll, [3,1,2], args=(x_matrix,y_array))
standard_error_mle = result.x[0]
beta_mle = result.x[1:]
print(""Standard error calculated with MLE: %s"" % str(standard_error_mle))
print(""Beta calculated with MLE: %s"" % str(beta_mle))
"
"beta_ols = np.linalg.inv(x_matrix.T * x_matrix) * (x_matrix.T * y_array)
print(""Beta calculated with OLS: %s"" % str(beta_ols))"
"error_ols = y_array - (x_matrix*beta_ols)
standard_error_ols = np.std(error_ols)
print(""Standard error calculated with OLS: %s"" % str(standard_error_ols))"
"y_pred = knn.predict(X_test)
print('Actual & Predicted y values')
print(np.vstack([y_test, y_pred]).T)
print('\n')

knn_accuracy_score = accuracy_score(y_test, y_pred)
print('Accuracy score: {}'.format(knn_accuracy_score))
print('\n')

print('Confusion Matrix')
print(confusion_matrix(y_test, y_pred))"
"sklearn_knn = KNeighborsClassifier(n_neighbors=5)
sklearn_knn.fit(X_train, y_train)
sklearn_y_pred = sklearn_knn.predict(X_test)

print('Actual & Predicted y values')
print(np.vstack([y_test, sklearn_y_pred]).T)
print('\n')

sklearn_knn_accuracy_score = accuracy_score(y_test, sklearn_y_pred)
print('Accuracy score: {}'.format(sklearn_knn_accuracy_score))
print('\n')

print('Confusion Matrix')
print(confusion_matrix(y_test, sklearn_y_pred))"
"eigval_a, eigvec_a=np.linalg.eig(a)
eigval_a, eigvec_a"
(eigvec_a[0]*eigvec_a[0].transpose())
(eigvec_a[1]*eigvec_a[1].transpose())
(eigvec_a[1]*eigvec_a[0].transpose())
(eigvec_a[0]*eigvec_a[1].transpose())
data
arr2.dtype
a
b
a1
c
e
f
arr_float
arr
arr*arr
arr-arr
1/arr
arr**0.5
data
names == 'Bob'
data[names == 'Bob']
"data[names == 'Bob', 2:]"
"data[names == 'Bob', 2]"
data[names != 'Bob']
data[~(names == 'Bob')]
data[(names == 'Bob') | (names == 'Will')]
data
data
arr
arr
"arr[[4, 3, 2, 5]]"
"arr[[-3, -5, -7]]"
np.sqrt(arr)
np.exp(arr)
"np.maximum(x, y)"
"arr = np.random.randn(7)*5
arr"
np.modf(arr)
np.floor(arr)
np.ceil(arr)
np.rint(arr)
np.isnan(arr)
"plot_tour(capital_cities)
plot_tour(east_cities)"
"tours = []
numtours = 30
for i in range(numtours):
    tour = east_cities.copy()
    random.shuffle(tour)
    tours.append(tour)
def plotone(i=0):
    tour = tours[i]
    plot_tour(tour)
    plt.title('Distance {d}'.format(d=tour_dist(tour)))
interact(plotone,i=(0,numtours-1))"
"tours = []
numtours = 30
for i in range(numtours):
    tour = capital_cities.copy()
    random.shuffle(tour)
    tours.append(tour)
def plotone(i=0):
    tour = tours[i]
    plot_tour(tour)
    plt.title('Distance {d}'.format(d=tour_dist(tour)))
interact(plotone,i=(0,numtours-1))"
"tour = east_cities.copy()
numtrials = 20000
results = anneal(tour,numtrials,temp=linear_temp)
def vis(i=0):
    fig = plt.figure(figsize=(10,5))
    plt.subplot(1,2,1)
    plot_tour(results[i][0])
    plt.subplot(1,2,2)
    plt.plot([r[1] for r in results])
    plt.plot([i], results[i][1], 'ro')
interact(vis,i=(0,numtrials-1))"
"tour = capital_cities.copy()
numtrials = 20000
results = anneal(tour,numtrials,temp=one_over_x_temp)
def vis(i=0):
    fig = plt.figure(figsize=(10,5))
    plt.subplot(1,2,1)
    plot_tour(results[i][0])
    plt.subplot(1,2,2)
    plt.plot([r[1] for r in results])
    plt.plot([i], results[i][1], 'ro')
interact(vis,i=(0,numtrials-1))"
"tour = capital_cities.copy()
numtrials = 20000
results = anneal(tour,numtrials,temp=linear_temp)
def vis(i=0):
    fig = plt.figure(figsize=(10,5))
    plt.subplot(1,2,1)
    plot_tour(results[i][0])
    plt.subplot(1,2,2)
    plt.plot([r[1] for r in results])
    plt.plot([i], results[i][1], 'ro')
interact(vis,i=(0,numtrials-1))"
"tour = capital_cities.copy()
numtrials = 40000
results = anneal(tour,numtrials,temp=combo_temp)
def vis(i=0):
    fig = plt.figure(figsize=(10,5))
    plt.subplot(1,2,1)
    plot_tour(results[i][0])
    plt.subplot(1,2,2)
    plt.plot([r[1] for r in results])
    plt.plot([i], results[i][1], 'ro')
interact(vis,i=(0,numtrials-1))"
%timeit string[::-1]
"%timeit """".join(list(reversed(string)))"
%timeit rev_concatenation(string)
%timeit rev_preappend(string)
%timeit rev_recrusion(string)
"# create a frequency count of the sites
dist = df['site'].value_counts()
dist.head(10)"
"# set style options (optional step)
plt.style.use('ggplot')

# create a bar graph to depict the frequencies
dist.plot(kind='bar', figsize=(14,6));"
"# plot only those that are seen more than once
dist[dist > 1].plot(kind='bar')"
"#Ahora, para pintar nuestra función, usaremos matplotlib a la manera habitual

#Primero crearemos un array 1D de unos 200 elementos donde almacenaremos los posibles valores del cociente de masas

cociente_masas = np.linspace(1,40, 200)

#El mínimo cociente es 1, es decir, la masa inicial y final es la misma, no se ha consumido combustible.
#El máximo cociente que he usado es 40, suponiendo que el peso del combustible es el 97,5% del peso total inicial de la nave

delta_v_adim = eq_cohete_adim(cociente_masas)

#Y pintamos al gusto


plt.figure(figsize=(12, 6))
plt.xlabel('$m_0/m_1$',fontsize=20)
plt.ylabel('$\Delta v /v_e$',fontsize=20)
plt.title ('$\Delta v /v_e  = f(m_0/m_1)$',fontsize=20)
plt.grid()
plt.plot(cociente_masas,delta_v_adim)"
"#Ahora, para pintar nuestra función, usaremos matplotlib a la manera habitual

#Esta vez, nuestra variable independiente va a ser el índice de carga de pago.
#Sus valores máximo y mínimo serán, lógicamente, 0 y 1

mu = np.linspace(0,1, 200)

#Ahora vamos a pintar 3 líneas para 3 valores del índice estructural
#Hemos visto que para el Ariane 5, es 0,13, así que este será uno de los que usemos.
#Pintaremos también uno con sigma = 0,2, un lanzador algo peor que el Ariane
#El otro valor será 0,08, un valor excepcionalmente bajo, que representaría un lanzador de calidad y eficacia inigualable


plt.figure(figsize=(12, 6))
plt.xlabel('$\mu$',fontsize=20)
plt.ylabel('$\Delta v /v_e$',fontsize=20)
plt.title ('$\Delta v /v_e  = f(\sigma,\mu)$',fontsize=20)
plt.grid()
plt.ylim(0,3)

plt.rc('font', size = 18)
plt.plot(mu,eq_cohete_adim_masa(0.2, mu),label='$\sigma = 0.2$')
plt.plot(mu,eq_cohete_adim_masa(0.13, mu),label='$\sigma = 0.13$')
plt.plot(mu,eq_cohete_adim_masa(0.08, mu),label='$\sigma = 0.08$')
plt.plot([0.0128, 0.0128], [0, 3],'k--', label='$\mu Ariane$')
plt.legend()"
"#Calculemos ahora el delta V del cohete!

#Primera Fase:
st1_sigma = 0.0439
st1_mu = 0.2264
st1_ve = 2.58
st1_dv_adim = eq_cohete_adim_masa(st1_sigma, st1_mu)
st1_dv = st1_dv_adim * st1_ve

#Segunda Fase:
st2_sigma = 0.0600
st2_mu = 0.2570
st2_ve = 4.13
st2_dv_adim = eq_cohete_adim_masa(st2_sigma, st2_mu)
st2_dv = st2_dv_adim * st2_ve

#Tercera Fase:
st3_sigma = 0.0787
st3_mu = 0.2832
st3_ve = 4.13
st3_dv_adim = eq_cohete_adim_masa(st3_sigma, st3_mu)
st3_dv = st3_dv_adim * st3_ve

total_dv = st1_dv + st2_dv + st3_dv

print(  'Etapa 1: ',st1_dv,
      '\nEtapa 2: ', st2_dv, '\nEtapa 3: ', st3_dv, '\nTotal  :', total_dv)"
"#Primera Fase, rendimiento en vacío:
st1_ve_vac = 2.98
st1_dv_vac = st1_dv_adim * st1_ve_vac
print(  'Etapa 1, nivel del mar:', st1_dv,
      '\nEtapa 1, en el vacío  :', st1_dv_vac,
      '\nDiferencia            :', st1_dv_vac - st1_dv)"
"pintar_cohete(13, 48.6, 421.4, 73.18, 0.763, 0.107)"
"v = interactive(pintar_cohete, 
               delta_v=(1.0,20.0, 1),
               m_pl = (1, 100, 1),
               isp = (250, 450, 10),
               t_w_ratio_motor = (40, 180, 10),
               t_w_ratio_total = (0.5, 5, 0.25),
               structural_ratio = (0.04, 0.12, 0.01))
display(v)"
"fig, axes = plt.subplots(2, 2, sharex='col', sharey='row')
axes[1, 1].axis('off')
ax_3d = fig.add_subplot(224, projection='3d')
ax_3d.set_aspect('equal')
ax_3d.set_adjustable('box-forced')

for ax in axes.ravel():
    ax.set_aspect('equal')
    ### This makes things worst
    #ax.set_adjustable('box-forced')

data = np.random.normal(scale=0.2, size=(3, 100)).cumsum(axis=1)
# put some anisotropy along z to ease orientation
data[2, :] /= 3. 

axes[0, 0].plot(data[0, :], data[1, :]) #x, y
axes[0, 1].plot(data[2, :], data[1, :]) #z, y
axes[1, 0].plot(data[0, :], data[2, :]) #x, z

ax_3d.plot(data[0, :], data[1, :], data[2, :])
#ax_3d.bbox = ax_3d.bbox.shrunk(0.8, 0.8)

"
"import warnings
warnings.filterwarnings('ignore')
%pylab inline
import numpy as np
import matplotlib.pyplot as plt
from scipy.integrate import odeint
from IPython.html.widgets import interact, interactive, fixed
import ToomreGalaxy as ToomreGalaxy
a = interact(ToomreGalaxy.Make_Plot_stars,results = fixed(np.load('Toomre_A.npy')), M = fixed(1.0e11), S = fixed(1.0e11), t = (0.1,20.1), dt = fixed(0.0075))
"
"import matplotlib.pyplot as plt
%matplotlib inline
fig, ax = plt.subplots(1, 1, figsize=(8, 6))
plt.plot(SGs,cap,'ro', Label='Table 12N Values')
plt.plot(x,y, label='Linear Regression Fit')
plt.plot(x_proj,y_proj, 'b', linestyle=':',label='Linear Regression Projection')
plt.plot(x_fit,y_fit, 'bo', fillstyle='none', Label='Linear Fit Regression Projection')
plt.plot(x_intr,y_intr,'g*',fillstyle='none', Label='Linear Extrapolation Projection')

plt.xlabel('SG Values')
plt.ylabel('16d Common Nail Capacity')
plt.title('SG vs. 16d Nail Capacity for 1-1/2 Side Member (12N)\nWhite Oak (SG = 0.73) Extrapolation')
plt.legend(loc='upper left', shadow=True)"
"import warnings
warnings.filterwarnings('ignore')
import warnings
warnings.filterwarnings('ignore')

#Below is inline matplotlib
%matplotlib inline

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from matplotlib import cm
from sklearn import linear_model
from sklearn.preprocessing import PolynomialFeatures
x = pd.read_csv('../data/LogisticRegression/ex5Logx.dat', header=None, names=['x1', 'x2'])
y = pd.read_csv('../data/LogisticRegression/ex5Logy.dat', header=None, names=['y'])
xy = pd.concat([x, y], axis=1)
logistic = linear_model.LogisticRegression(C=100000) # The larger the C, the less regularization.
poly = PolynomialFeatures(6) # Polynomial feature to overfit the data
logistic.fit(poly.fit_transform(x), y)

pos = xy[xy['y']==1]
neg = xy[xy['y']==0]

ax = pos.plot.scatter(x='x1', y='x2', marker='+', label='y=1')
neg.plot.scatter(x='x1', y='x2', color='yellow', marker='o', label='y=0', ax=ax)

x1c = np.linspace(-1.0, 1.2, 200)
x2c = np.linspace(-1.0, 1.2, 200)
z = np.zeros((len(x1c), len(x2c)))
for i in range(len(x1c)):
    for j in range(len(x2c)):
        z[i,j] = logistic.predict(poly.fit_transform([[x1c[i], x2c[j]]]))[0]

plt.contour(x1c, x2c, np.transpose(z), levels=[0], color='green', label='Decision boundary')"
"fig = plt.figure()
ax = plt.gca()
ax.set_yscale('log')
plt.plot(lengths, error)
plt.title('Transfer Matrix method error as a function of length')
plt.xlabel('Length')
plt.ylabel('error')"
"# generate potential
x = np.linspace(-3,3,L)
y = np.linspace(-4,4,N)
X, Y = np.meshgrid(x,y)
pot = np.tanh(Y**2-X**2)+1+mu

# plot the potential
fig = plt.figure()
ax = fig.add_subplot(1, 1, 1, projection='3d')
plt.xlabel('x')
plt.ylabel('y')
p = ax.plot_surface(X, Y, pot)
plt.show()"
"fig, ax = plt.subplots()
plot = ax.matshow(pot)
title = ax.set_title(""Crank-Nicholson method"")
ax.set_xlabel(r'$x$')
ax.set_ylabel(r'$y$')
plt.show()"
"en = 0.5
L = 200
N = 40
V = smooth_potential(en, L, xx=1, yy=1, h=.6, plot=True)"
"N = 40 # number of transverse lattice points
L = 200 # length of the scattering region

G = []

energies = np.linspace(.3, 1, 50)
for en in energies:
    V = smooth_potential(en, L, xx=1, yy=1, h=.6, plot=False)
    G.append(solve(en, V, L, en, opt = True)[0])
    
plt.plot(energies, G)
plt.title('Quantized Conductance of a quantum point contact (QPC)')
plt.xlabel('energy [t]')
plt.ylabel('conductance [e^2/h]')
plt.show()"
"G2 = []
for en in energies:
    V = smooth_potential(en, L, xx=1, yy=1, h=0, plot=False)
    G2.append(solve(en, V, L, en, opt = True)[0])

plt.title('Quantized Conductance of a quantum point contact (QPC) for different heigts of potential')
plt.xlabel('energy [t]')
plt.plot(energies, G, energies, G2)
plt.show()"
"from scipy.optimize import curve_fit
N = 20
G = []
lengths = np.arange(300, 1000, 20)
mu = 0.1
for L in lengths:
    V = np.random.rand(N,L)
    G.append(solve(mu, V, L, L, opt = False)[0])

def func(x, a, b, c):
    return a * np.exp(-b * x) + c

popt, pcov = curve_fit(func, lengths, G)

plt.figure()

plt.plot(lengths, G, 'ko', label=""Original Noised Data"")
plt.plot(lengths, func(lengths, *popt), 'r-', label=""Fitted Curve"")
plt.xlabel(""length [L]"")
plt.ylabel(""conductance [e^2/h]"")
plt.legend()
plt.show()"
"%matplotlib nbagg
plt.plot([1,2,3,4,5,8,7,8])
"
"%matplotlib nbagg
plt.plot([1,2,3,4,5,8,7,8])
"
"phi, c = poly_airy(x, y, 4)

phi"
"s11, s22, s12 = airy_stress(phi, x, y)
display(s11, s22, s12)"
"eqlist = conds2eqs(conds, (x, y))
eqlist"
sol
s11.subs(sol)
s12.subs(sol)
s22.subs(sol)
"phi, c = poly_airy(x, y, 3)

phi"
"conds = [s22.subs(y, h/2),
         s12.subs(y, h/2),
         s22.subs(y, -h/2),
         s12.subs(y, -h/2),
         s11.subs(x, L/2) - S*y,
         s12.subs(x, L/2),
         s11.subs(x, -L/2) - S*y,
         s12.subs(x, -L/2)]

conds"
"eqlist = conds2eqs(conds)

eqlist"
sol
s11.subs(sol)
s12.subs(sol)
s22.subs(sol)
"phi, d = poly_airy(x, y, 5)

phi"
"s11, s22, s12 = airy_stress(phi, x, y)
display(s11, s22, s12)"
"conds = [s22.subs(y, h/2) + q,
         s12.subs(y, h/2),
         s22.subs(y, -h/2),
         s12.subs(y, -h/2),
         integrate(s11.subs(x, L/2), (y,-h/2, h/2)),
         integrate(s12.subs(x, L/2), (y,-h/2, h/2)) - q*L/2,
         integrate(y*s11.subs(x, L/2), (y,-h/2, h/2)),
         diff(phi, x, 4) + diff(phi, y, 4) + 2*diff(phi, x, 2, y, 2)]
conds"
"eqlist = conds2eqs(conds)

eqlist"
sol
factor(s11.subs(sol))
factor(s12.subs(sol))
factor(s22.subs(sol))
"phi, d = poly_airy(x, y, 6)

phi"
"s11, s22, s12 = airy_stress(phi, x, y)
display(s11, s22, s12)"
"conds = [s22.subs(y, h/2) + (x + L/2)*q/L,
         s12.subs(y, h/2),
         s22.subs(y, -h/2),
         s12.subs(y, -h/2),
         integrate(s11.subs(x, L/2), (y,-h/2, h/2)),
         integrate(s12.subs(x, L/2), (y,-h/2, h/2)) - q*L/2,
         integrate(y*s11.subs(x, L/2), (y,-h/2, h/2)),
         diff(phi, x, 4) + diff(phi, y, 4) + 2*diff(phi, x, 2, y, 2)]
conds"
"eqlist = conds2eqs(conds)

eqlist"
sol
factor(s11.subs(sol))
factor(s12.subs(sol))
factor(s22.subs(sol))
"phi, d = poly_airy(x, y, 5)

phi"
"s11, s22, s12 = airy_stress(phi, x, y)
display(s11, s22, s12)"
"conds = [s22.subs(y, h/2) + q,
         s12.subs(y, h/2),
         s22.subs(y, -h/2),
         s12.subs(y, -h/2),
         integrate(s11.subs(x, 0), (y,-h/2, h/2)),
         integrate(s12.subs(x, 0), (y,-h/2, h/2)) + q*L,
         integrate(y*s11.subs(x, 0), (y,-h/2, h/2)) - q*L**2/2,
         diff(phi, x, 4) + diff(phi, y, 4) + 2*diff(phi, x, 2, y, 2)]
conds"
"eqlist = conds2eqs(conds)
eqlist"
sol
factor(s11.subs(sol))
factor(s12.subs(sol))
factor(s22.subs(sol))
"r, theta, C, alpha, q = symbols(""r  theta C alpha q"")
phi = C*(r**2*(alpha - theta) + r**2*sin(2*theta)/2 - r**2*cos(theta)**2*tan(alpha))
phi"
"Srr = simplify(1/r**2*diff(phi, theta, 2) + 1/r*diff(phi, r))
Srr"
"Stt = simplify(diff(phi, r, 2))
Stt"
"Srt = -simplify(diff(1/r*diff(phi, theta), r))
Srt"
"sol = solve(Stt.subs(theta, 0) + q, C)
sol"
"Srr2 = Srr.subs(C, sol[0])
Stt2 = Stt.subs(C, sol[0])
Srt2 = Srt.subs(C, sol[0])

display(Srr2)
display(Stt2)
display(Srt2)"
"Srt.subs(theta, 0)"
"trigsimp(Stt.subs(theta, alpha))"
"simplify((Srt.subs(theta, alpha)).rewrite(sin))"
"S_cart = simplify(Q.T *  S_polar * Q)
S_cart"
"from IPython.display import IFrame
IFrame('http://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.ndimage.morphology.distance_transform_edt.html',
    width='100%', height=400)"
"a = np.array(([0,1,1,1,1],
                  [0,0,1,1,1],
                  [0,1,1,1,1],
                  [0,1,1,1,0],
                  [0,1,1,0,0]))
dist, ind = distance_transform_edt(a, sampling=[2, 0.11], return_indices=True)
dist"
ind
"n = 200
a = np.random.randint(low=0, high=2, size=(n,n,n))
%timeit dist = distance_transform_edt(a, sampling=[2.001, 0.11, 0.73])"
"a = np.random.rand(n,n,n)
a[a < 0.5] = 0
a[a >= 0.5] = 1
%timeit dist = distance_transform_edt(a, sampling=[2.001, 0.11, 0.73])"
"a = np.random.rand(n,n,n)
a[a < 0.5] = 0
a[a >= 0.5] = 1
%timeit dist, ind = distance_transform_edt(a, sampling=[2.001, 0.11, 0.73], return_indices=True)"
"# simple shear first
dot(PS, SS)"
"# pure shear first
dot(SS, PS)"
"matrix_power(PSi, 10)"
"matrix_power(SSi, 10)"
orientation(F)
"plot(nrange, PSar, label='pure shear')
plot(nrange, SSar, label='simple shear')
plt.legend(loc=2)"
"plot(nrange, ang)"
"V = la.sqrtm(B)
print(V)
def_show(V)"
"U = la.sqrtm(C)
print(U)
def_show(U)"
"R, V = la.polar(F, 'left')
print(V)
def_show(V)"
"R, U = la.polar(F, 'right')
print(U)
def_show(U)"
"gammas = linspace(0, 10, 50)[1:]
thetas = []
for gamma in gammas:
    F = array([[1, gamma],[0, 1]])
    B = dot(F, F.T)
    e,v = eig(B)
    x, y = v[:, e.argmax()]
    thetas.append(degrees(arctan2(y, x)))
plot(gammas, thetas)
xlabel('gamma')
ylabel('theta');"
"gammas = linspace(0, 10, 1000)[1:]
ars = []
for gamma in gammas:
    F = array([[1, gamma],[0, 1]])
    B = dot(F, F.T)
    e,v = eig(B)
    ars.append(sqrt(max(e)/min(e)))
plot(gammas, ars)
xlabel('gamma')
ylabel('AR');"
"S = array([[10, 0], [0, 5]])
tau = []
sn = []

for theta in pi*uniform(size=5000):
    n = array([cos(theta), sin(theta)])
    sv = dot(S, n)
    sn.append(dot(sv, n))
    tau.append(norm(sv-n*dot(sv, n)))

plot(sn, tau, 'k.')
axis('equal')
margins(x=0.1, y=0.1)
show()"
"S = array([[5, 0, 0], [0, 9, 0], [0, 0, 15]])
tau = []
sn = []

for i in range(5000):
    n = rand_vec()
    sv = dot(S, n)
    sn.append(dot(sv, n))
    tau.append(norm(sv-n*dot(sv, n)))

plot(sn, tau, 'k.')
axis('equal')
margins(x=0.1, y=0.1)
show()
"
"# parametric definition of unit circle
theta = linspace(0, 2*pi, 300)
Xc, Yc = cos(theta), sin(theta)
plot(Xc, Yc, 'g')

# Apply deformation gradient and plot ellipse
F = array([[2, 0], [0, 0.5]])
xe, ye = dot(F, [Xc, Yc])
plot(xe, ye, 'r')
axis('equal');"
"# coordinates of square
Xs = [1, -1, -1, 1, 1]
Ys = [1, 1, -1, -1, 1]
plot(Xs, Ys, 'g')

# Apply deformation gradient and plot result
F = array([[2, 0], [0, 0.5]])
xn, yn = dot(F, [Xs, Ys])
plot(xn, yn, 'r')
axis('equal');"
"# create rectangular grid
X, Y = meshgrid(linspace(-2.2, 2.2, 21), linspace(-1.9, 1.9, 17))

# Apply deformation gradient. As [X, Y] is 3D array, we need
# special function to treat dot product properly. tensordot
# can do it.
x, y = tensordot(F, [X, Y], axes=1)

# plot displacement vectors on all point
quiver(X, Y, x-X, y-Y, angles='xy')
plot(Xc, Yc, 'g', xe, ye, 'r')
plot(Xs, Ys, 'g', xn, yn, 'r')
axis('equal');"
"# calculate displacements
J = F - eye(2)
u, v = tensordot(J, [X, Y], axes=1)

# plot
quiver(X, Y, u, v, angles='xy')
plot(Xc, Yc, 'g', xe, ye, 'r')
plot(Xs, Ys, 'g', xn, yn, 'r')
axis('equal');"
"# Deformation gradient
F = array([[0.5, 1],
           [0  , 2]])
def_field(F)
def_ellipse(F)"
"# Displacement gradient
J = array([[1,    1],
           [0, -0.5]])
dis_field(J)
dis_ellipse(J)"
"import warnings
warnings.filterwarnings('ignore')
from IPython.display import Image
Image('pics/formuli.png', embed=True)"
"from math import *
input_tree = [1,2,3,4,5,None, None, 2,3,None, 4, None, None, None, None, None, 19]
class TreeNode(object):
    def __init__(self, x):
        self.val = x
        self.left = None
        self.right = None
def create_tree(input_tree):
    if len(input_tree)==0:
        return None
    levels = int(ceil(log(len(input_tree),2)))
    
    for level in range(1, levels+1):
        print(input_tree[int(pow(2,max(0, level-1))-1): int(pow(2,max(0, level))-1)])
        
create_tree(input_tree)"
"
"
"#Plot the posterior for all three strains
p= np.linspace(0, 1, 300)

#make plots
strains= ['WT', 'ASH', 'AVA']

for strain in strains:
    n_r, n= revs_trials(df, strain)
    plt.plot(p, np.exp(log_posterior(n_r, n, p)), '-')
    
#prettify
plt.margins(y = 0.02)
plt.xlabel('prob of reversal, $p$')
plt.ylabel(r'$P(p\mid n_r, n, I)$')"
"#generate plot
gamma= np.linspace(0, 2, 200)
delta= np.linspace(-1, 1, 200)

#make coordinate for contour plot
dd, gg= np.meshgrid(delta, gamma)

#compute probabilities
n_r_ash, n_ash= revs_trials(df, 'ASH')
n_r_ava, n_ava= revs_trials(df, 'AVA')

post= posterior_delta_gamma(delta, gamma, n_r_ash, n_ash, n_r_ava, n_ava)

#plot the contour
plt.contourf(dd, gg, post, cmap= plt.cm.Blues, alpha= 0.7)
plt.contourf(dd, gg, post, cmap=plt.cm.Blues, alpha=0.7)
plt.xlabel(r'$\delta = p_\mathrm{AVA} - p_\mathrm{ASH}$', fontsize=24)
plt.ylabel(r'$\gamma = p_\mathrm{AVA} + p_\mathrm{ASH}$', fontsize=24)
plt.xlim((0.4, 0.9))
plt.ylim((0.9, 1.4))
plt.axes().set_aspect('equal', adjustable='box')
"
"# Integrate over gamma
post_delta = np.trapz(post, x=gamma, axis=0)

# Plot the distribution of delta 
plt.plot(delta, post_delta, '-')
plt.xlabel(r'$\delta = p_\mathrm{AVA} - p_\mathrm{ASH}$')
plt.ylabel(r'$P(\delta \mid D, I)$')
plt.margins(y=0.02)
"
"# Generate gamma and delta for the plot
gamma = np.linspace(0, 2, 200)
delta = np.linspace(-1, 1, 200)

# Make coordinates for contour plots
dd, gg = np.meshgrid(delta, gamma)

# Compute probability
n_r_ash, n_ash = revs_trials(df, 'WT')
n_r_ava, n_ava = revs_trials(df, 'ASH')
post = posterior_delta_gamma(delta, gamma, n_r_ash, n_ash, n_r_ava, n_ava)

# Integrate over gamma
post_delta = np.trapz(post, x=gamma, axis=0)

# Plot the distribution of delta 
plt.plot(delta, post_delta, '-')
plt.xlabel(r'$\delta = p_\mathrm{ASH} - p_\mathrm{WT}$')
plt.ylabel(r'$P(\delta \mid D, I)$')
plt.margins(y=0.02)
"
"# Generate gamma and delta for the plot
gamma = np.linspace(0, 2, 200)
delta = np.linspace(-1, 1, 200)

# Make coordinates for contour plots
dd, gg = np.meshgrid(delta, gamma)

# Compute probability
n_r_ash, n_ash = revs_trials(df, 'WT')
n_r_ava, n_ava = revs_trials(df, 'AVA')
post = posterior_delta_gamma(delta, gamma, n_r_ash, n_ash, n_r_ava, n_ava)

# Integrate over gamma
post_delta = np.trapz(post, x=gamma, axis=0)

# Plot the distribution of delta 
plt.plot(delta, post_delta, '-')
plt.xlabel(r'$\delta = p_\mathrm{AVA} - p_\mathrm{WT}$')
plt.ylabel(r'$P(\delta \mid D, I)$')
plt.margins(y=0.02)
"
"plot(t, x, 'b', label='Numeric')
plot(ta, x, 'g.', label='Analytic')
ylim(tc, 0)
legend()
xlabel('x')
ylabel('Teplota')
title('Tok na povrchu {:.2f} mW/m2'.format(1000 * k * (t[1] - t[0]) / dx));"
tshow(m)
tshow(m)
"plot(time, hf)
xlabel('Time [Ma]')
ylabel('Surface heatflow [mW/m2]')"
"plot(ti, x)
ylim(tc, 0);"
"plot(ti, x)
ylim(tc, 0);"
"t = ti
res = []
tt = []
total_time = 0
plot(ti ,x)
for j in range(10):
    for i in range(100):
        t = A.dot(t) + b
        res.append(t)
        total_time += dt
        tt.append(total_time/(365.25*24*3600))
    plot(t, x)
ylim(tc, 0);
res = asarray(res)"
"plot(tt, res[:,31])
xlabel('Time [years]')
ylabel('Temperature')"
"plot(t, x, 'b', label='Numeric')
plot(ta, x, 'g.', label='Analytic')
ylim(tc, 0)
legend()
xlabel('x')
ylabel('Teplota')
title('Tok na povrchu {:.2f} mW/m2'.format(1000 * k * (t[1] - t[0]) / dx));"
"plot(-1000*qset, qs)
xlabel('Mantle heat flow [mW/m2]')
ylabel('Surface heat flow [mW/m2]');"
"for iteration in range(20):
    prediction = input.dot(weights)
    error = (prediction - goal_prediction) ** 2
    delta = prediction - goal_prediction
    weights = weights - (alpha * (input * delta))
    
    print(""Error:"" + str(error) + "" Prediction:"" + str(prediction))"
"for iteration in range(40):
    error_for_all_lights = 0
    for row_index in range(len(walk_vs_stop)):
        input = streetlights[row_index]
        goal_prediction = walk_vs_stop[row_index]
        prediction = input.dot(weights)
        error = (goal_prediction - prediction) ** 2
        error_for_all_lights += error
        delta = prediction - goal_prediction
        weights = weights - (alpha * (input * delta))
        print(""Prediction:"" + str(prediction))

    print(""Error:"" + str(error_for_all_lights) + ""\n"")"
"plt.imshow(chelsea)
#plt.axis('off')"
img
chelsea
"img_adapteq = img_as_float(skie.equalize_adapthist(img, clip_limit=0.03))"
"@interact(hist_type=list(hist_types.keys()))
def display_result(hist_type):
    result = hist_types[hist_type]

    # We display the processed grayscale image on the left.
    plt.subplot(121)
    plt.imshow(result, cmap='gray')
    plt.axis('off')

    # We display the histogram on the right.
    plt.subplot(122)
    plt.hist(result.ravel(), bins=np.linspace(0., 1., 256),
             histtype='step', color='black')

    plt.show()"
"def asset_profile1(ca0):
    
    A = np.zeros(61)

    cb0 = ca0
    Q0 = 3*ca0
    A_pre = 0
    for t in range(60):
        A_next = (1 + r)*A_pre + Y[t].sum() - (ca0 + cb0 + Q0)*((1 + r)*beta)**t
        A_pre = A_next

        A[t+1] = A_next
    
    return A

def term_cond1(ca0):
    
    return abs(asset_profile1(ca0)[-1])

rslt = minimize(term_cond1, x0=1000, method='Nelder-Mead')
print(rslt.success, rslt.x[0])
plt.plot(range(1, 62), asset_profile1(rslt.x[0]))"
"def asset_profile2(theta, ca0):
    
    A = np.zeros(61)
    
    cb0 = (1-theta)*ca0/(2*theta)
    Q0 = 3*ca0/(2*theta)
    A_pre = 0
    for t in range(60):
        A_next = (1 + r)*A_pre + Y[t].sum() - (ca0 + cb0 + Q0)*((1 + r)*beta)**t
        A_pre = A_next

        A[t+1] = A_next
    
    return A

def term_cond2(theta, ca0):
    
    return abs(asset_profile2(theta, ca0)[-1])

thetas = [0.3, 0.5, 0.7]
A = pd.DataFrame(columns=thetas)

for theta in thetas:
    
    eval_term_cond = functools.partial(term_cond2, theta)
    
    rslt = minimize(eval_term_cond, x0=1000, method='Nelder-Mead')
    print(rslt.success, rslt.x[0])
    A[theta] = asset_profile2(theta, rslt.x[0])
    
A.index = range(1, 62)
A.plot(xlim=[0, 61], ylim=[0, 2000000])"
"P_K = P_K.astype('int')
A_K = asset_grid[P_K]

# Setting dead households to missing
A_K[S == 3] = np.nan

# Taking averages, skipping missing households
A_avg = np.nanmean(A_K, axis = 1)

# Plotting average wealth over lifecycle for living households
%matplotlib inline

import matplotlib.pyplot as plt
plt.plot(range(20, T+20), A_avg)"
"interact(factorit, n=(2,40));"
"t, x_t = solve_lorenz(angle=10, N=10)"
"w = interactive(solve_lorenz, angle=(0.,360.), N=(0,50), σ=(0.0,50.0), ρ=(0.0,50.0))
display(w)"
w.kwargs
xyz_avg.shape
"plt.hist(xyz_avg[:,0])
plt.title('Average $x(t)$')"
"plt.hist(xyz_avg[:,1])
plt.title('Average $y(t)$')"
"# load the data
iris = datasets.load_iris()

print(iris.keys())"
"print(iris.feature_names)
# only print the first 10 samples
print(iris.data[:10])
print('We have %d data samples with %d features'%(iris.data.shape[0], iris.data.shape[1]))"
print(iris.DESCR)
"print(digits.data)
print('The targets are:')
print(digits.target_names)"
"## plot the first 64 samples, and get a sense of the data
fig = plt.figure(figsize = (8,8))
fig.subplots_adjust(left=0, right=1, bottom=0, top=1, hspace=0.05, wspace=0.05)
for i in range(64):
    ax = fig.add_subplot(8, 8, i+1, xticks=[], yticks=[])
    ax.imshow(digits.images[i],cmap=plt.cm.binary,interpolation='nearest')
    ax.text(0, 7, str(digits.target[i]))"
"boston = datasets.load_boston()
print(boston.DESCR)"
boston.feature_names
"# let's just plot the average number of rooms per dwelling with the price
plt.figure(figsize = (10,8))
plt.plot(boston.data[:,5], boston.target, 'o')
plt.xlabel('Number of rooms')
plt.ylabel('Price (thousands)')"
print(model)
"plt.figure(figsize = (10,8))
plt.plot(x,y,'o')"
"plt.figure(figsize = (10,8))
plt.plot(x,y_noise,'o')"
"# The input data for sklearn is 2D: (samples == 10 x features == 1)
X = x[:, np.newaxis]
print(X)
print(y_noise)"
"# model fitting is via the fit function
model.fit(X, y_noise)"
"# underscore at the end indicates a fit parameter
print(model.coef_)
print(model.intercept_)"
"plt.figure(figsize = (10,8))
plt.plot(x,y_noise,'o')
plt.plot(x,predicted, label = 'Prediction')
plt.legend()"
"import warnings
warnings.filterwarnings('ignore')
from IPython.display import YouTubeVideo
YouTubeVideo(""OO8Jfs9uZnc"")"
"from IPython.display import YouTubeVideo
YouTubeVideo(""hKCqske0rAE"")"
"from IPython.display import YouTubeVideo
YouTubeVideo(""BTRssTnhZVU"")"
"import warnings
warnings.filterwarnings('ignore')
import matplotlib.pyplot as plt

nx.draw_circular(simple_relation, with_labels=True, directed = True)
plt.show()"
nx.transitive_closure(simple_relation)
"tc = nx.transitive_closure(simple_relation)
tc.edges()"
"nx.draw_circular(tc, with_labels=True, directed = True)
plt.show()"
"# What this does, is list all the edges in the edge list for ""tc"" 
# as long as the edge was not in the edge list for simple_relation. 

[edge for edge in tc.edges() if edge not in simple_relation.edges()]"
"import warnings
warnings.filterwarnings('ignore')
import networkx as nx
import matplotlib.pyplot as plt

# Converting the list of pairs to actual networkX digraphs: 

divides_relation = nx.DiGraph(divides)
next_to_relation = nx.DiGraph(next_to)
leq_relation = nx.DiGraph(less_than_eq)
slt_relation = nx.DiGraph(strict_less_than)

# Put these into a list: 
examples = [divides_relation, next_to_relation, leq_relation, slt_relation]

# Look at the dictionaries: 
for r in examples:
    print(nx.to_dict_of_lists(r))"
"nx.draw_circular(divides_relation, with_labels=True)
plt.show()"
"nx.draw_circular(next_to_relation, with_labels=True)
plt.show()"
"nx.draw_circular(slt_relation, with_labels=True)
plt.show()"
"not_symmetric = [(1,2), (2,1), (1,3), (3,1), (2,4), (4,2), (2,5), (3,6), (4,7), (5,8), (6,9), (7,10)]
not_symmetric_relation = nx.DiGraph(not_symmetric)
nx.draw_circular(not_symmetric_relation, with_labels=True)
plt.show()"
"nx.draw_circular(divides_relation, with_labels=True)
plt.show()"
"nx.draw_circular(next_to_relation, with_labels=True)
plt.show()"
"nx.draw_circular(leq_relation, with_labels=True)
plt.show()"
"nx.draw_circular(slt_relation, with_labels=True)
plt.show()"
"import warnings
warnings.filterwarnings('ignore')
from IPython.display import YouTubeVideo
YouTubeVideo(""YyLaRffCdk4"")"
"from IPython.display import YouTubeVideo
YouTubeVideo(""rcJTZXZJVUs"")"
"import warnings
warnings.filterwarnings('ignore')
import warnings
warnings.filterwarnings('ignore')
import networkx as nx
import matplotlib.pyplot as plt

U = [2,3,4,6,8,9,12]

# Add (x,y) to the edge list if x divides y
poset_edges = [(x,y) for x in U for y in U if y % x == 0]

poset = nx.DiGraph(poset_edges)
nx.draw(poset, with_labels=True)
plt.show()
"
"from IPython.display import YouTubeVideo
from datetime import timedelta
start=int(timedelta(minutes=9, seconds=55).total_seconds())
YouTubeVideo(""R36F8CWAi2k"", start=start)"
"import warnings
warnings.filterwarnings('ignore')
import warnings
warnings.filterwarnings('ignore')
import networkx as nx
import matplotlib.pyplot as plt

t = nx.random_powerlaw_tree(8)

nx.draw(t)
plt.show()"
"import warnings
warnings.filterwarnings('ignore')
import warnings
warnings.filterwarnings('ignore')

import networkx as nx
import matplotlib.pyplot as plt

chapsnat_edges = [('Ronald', 'Scott'), ('Ronald', 'Trish'), ('Scott', 'Uriah'), ('Scott', 'Trish'), 
                  ('Uriah', 'Ronald'), ('Uriah', 'Scott'), ('Uriah', 'Trish'), ('Uriah', 'Victoria'), 
                  ('Victoria', 'Uriah'), ('Victoria', 'Scott')]

chapsnat = nx.DiGraph(chapsnat_edges)
nx.draw_circular(chapsnat, with_labels = True, node_size = 2500)
plt.show()"
"bsquared = [('Ronald', 'Uriah'), ('Ronald', 'Trish'), 
 ('Scott', 'Ronald'), ('Scott', 'Scott'), ('Scott', 'Trish'), ('Scott', 'Victoria'), 
 ('Uriah', 'Scott'), ('Uriah', 'Trish'), ('Uriah', 'Uriah'), 
 ('Victoria', 'Ronald'), ('Victoria', 'Scott'), ('Victoria', 'Trish'), ('Victoria', 'Victoria'), ('Victoria', 'Uriah')]

bsquaredgraph = nx.DiGraph(bsquared)
nx.draw_circular(bsquaredgraph, with_labels = True, node_size = 1000)
plt.show()"
"from IPython.display import YouTubeVideo
YouTubeVideo(""G7rGep_v-EM"")"
"# Let's first set up the edge list for our relation. 

chapsnat_edges = [('Ronald', 'Scott'), ('Ronald', 'Trish'), ('Scott', 'Uriah'), ('Scott', 'Trish'), 
                  ('Uriah', 'Ronald'), ('Uriah', 'Scott'), ('Uriah', 'Trish'), ('Uriah', 'Victoria'), 
                  ('Victoria', 'Uriah'), ('Victoria', 'Scott')]

# Here's how to tell networkX you have a directed graph.
# It's exactly the same process as undirected graphs except you use DiGraph intead of Graph. 

chapsnat = nx.DiGraph(chapsnat_edges)

# Once we have this defined, we can visualize using Matplotlib: 

nx.draw_circular(chapsnat, with_labels = True, node_size = 2000)
plt.show()"
nx.to_dict_of_lists(chapsnat)
chapsnat.edges()
print(nx.adjacency_matrix(chapsnat))
"g = nx.Graph({0: [1, 3, 4, 5, 6, 7],
 1: [0, 3, 4, 5, 6, 7],
 2: [3, 5, 6, 7],
 3: [0, 1, 2, 4, 6],
 4: [0, 1, 3, 5, 7],
 5: [0, 1, 2, 4, 6, 7],
 6: [0, 1, 2, 3, 5, 7],
 7: [0, 1, 2, 4, 5, 6]})

nx.draw_circular(g, with_labels=True)
plt.show()"
nx.coloring.greedy_color(g)
"# This is the default greedy algorithm -- the strategy does not 
# actually need to be stated. 
nx.coloring.greedy_color(g, strategy=nx.coloring.strategy_largest_first)"
"# This one randomly selects vertices one at a time and builds a 
# proper coloring. Notice the result is different here. 
nx.coloring.greedy_color(g, strategy=nx.coloring.strategy_random_sequential)"
"nx.coloring.greedy_color(g, strategy=nx.coloring.strategy_connected_sequential)"
"nx.coloring.greedy_color(g, strategy=nx.coloring.strategy_independent_set)"
"nx.coloring.greedy_color(g, strategy=nx.coloring.strategy_smallest_last)"
"nx.coloring.greedy_color(g, strategy=nx.coloring.strategy_random_sequential)"
"# First let's remind ourselves what the graph looked like

nx.draw_circular(g, with_labels=True)
plt.show()"
"# And the coloring: 

nx.coloring.greedy_color(g)"
"# The color classes are {0,2}, {1}, {3,5}, {4,6}, and {7}. 
# Here is some code that will assign those red, blue, yellow, green, and purple. 

# Specify the layout; in this case ""circular"" 
pos = nx.circular_layout(g)

# Now draw just the nodes, one color class at a time. 
# Here we have to specify the layout (`pos`) and the list of nodes. 
nx.draw_networkx_nodes(g, pos, nodelist=[0,2], node_color='red')
nx.draw_networkx_nodes(g, pos, nodelist=[1], node_color='blue')
nx.draw_networkx_nodes(g, pos, nodelist=[3,5], node_color='yellow')
nx.draw_networkx_nodes(g, pos, nodelist=[4,6], node_color='green')
nx.draw_networkx_nodes(g, pos, nodelist=[7], node_color='purple')

# Now draw the edges and labels: 
nx.draw_networkx_edges(g, pos)
nx.draw_networkx_labels(g, pos)

# Turn off the axes: 
plt.axis('off')

# Then plot the whole thing: 
plt.show()"
"# Specify the layout; in this case with a ""random"" layout.
pos = nx.random_layout(k44)

# Now draw just the nodes, one color class at a time. 
# Here we have to specify the layout (`pos`) and the list of nodes. 
nx.draw_networkx_nodes(k44, pos, nodelist=[0,1,2,3], node_color='red')
nx.draw_networkx_nodes(k44, pos, nodelist=[4,5,6,7], node_color='blue')

# Now draw the edges and labels: 
nx.draw_networkx_edges(k44, pos)
nx.draw_networkx_labels(k44, pos)

# Turn off the axes: 
plt.axis('off')

# Then plot the whole thing: 
plt.show()"
"# For MTH 325 students: Using Python to visualize graphs...

# Jupyter has access to the huge collection of Python libraries that are available: 
import networkx as nx
import matplotlib.pyplot as plt

# Let's plot the complete graph on 20 vertices: 
k20 = nx.complete_graph(20)  
nx.draw(k20, pos = nx.circular_layout(k20), edge_color=""red"", 
        node_color=""cyan"", with_labels = True)
plt.show()"
"from IPython.display import YouTubeVideo
YouTubeVideo(""T4WXs6niloU"")"
"from IPython.display import YouTubeVideo
YouTubeVideo(""tz4UwhQs0Z8"")"
"from IPython.display import YouTubeVideo
YouTubeVideo(""woEVRMADIck"")"
"import warnings
warnings.filterwarnings('ignore')
from IPython.display import YouTubeVideo
YouTubeVideo(""aNQV45Wichw"")"
"from IPython.display import YouTubeVideo
YouTubeVideo(""txdmCgThR6o"")"
"import warnings
warnings.filterwarnings('ignore')
from IPython.display import YouTubeVideo
YouTubeVideo(""n-bJB_7QbQU"")"
"from IPython.display import YouTubeVideo
YouTubeVideo(""ML-g2xLYruE"")"
"from IPython.display import YouTubeVideo
YouTubeVideo(""6O1s3_GsSHo"")"
"import warnings
warnings.filterwarnings('ignore')
from IPython.display import YouTubeVideo
YouTubeVideo(""4c6Bg2GJvQw"")"
"from IPython.display import YouTubeVideo
YouTubeVideo(""GvOBBcnIjlw"")"
"from IPython.display import YouTubeVideo
YouTubeVideo(""Rwm-NW9Y5iM"")"
"import warnings
warnings.filterwarnings('ignore')
from IPython.display import YouTubeVideo
YouTubeVideo(""OO8Jfs9uZnc"")"
"from IPython.display import YouTubeVideo
YouTubeVideo(""hKCqske0rAE"")"
"from IPython.display import YouTubeVideo
YouTubeVideo(""BTRssTnhZVU"")"
"import warnings
warnings.filterwarnings('ignore')
from IPython.display import YouTubeVideo
YouTubeVideo(""Z9sYIWHIvNc"")"
"fig = plt.figure()
ax = fig.add_subplot(1,1,1)

ax.scatter(diamondData.carats, diamondData.price)
   # draw linear regression line
x = [0.1,0.4]
y = [-259.6 + 3721 * i for i in x] 
ax.plot(x, y)
   # alternatively, plot the fitted values
#y_hat = simpleModel.fittedvalues
#ax.plot(diamondData.carats, y_hat)
   # pretty-up the plot
fig.suptitle(""Relation between diamonds' price and weight"")
ax.set_ylabel('Price [SIN $]')
ax.set_xlabel('Weight [carat]')
ax.grid(True)"
simpleModel.params[0] + 0.2*simpleModel.params[1]
simpleModel.predict(newDiamond)
simpleModel.predict(newDiamonds)
max (abs (y - y_hat))
max(abs(residuals))
"fig = plt.figure()
ax = fig.add_subplot(1,1,1)
ax.plot(simpleModel.fittedvalues, residuals, 'o') # as round marks

   # pretty-up the plot
ax.plot ((0, 1200), (0,0)) # draw also a line at y=0
fig.suptitle(""Residuals versus fitted prices"")
ax.set_ylabel('Residuals [SIN $]')
ax.set_xlabel('Price [SIN $]')
ax.grid(True)"
sum(residuals)
np.mean(residuals)
plt.hist(residuals)
"sm.qqplot(residuals, fit=True, line = '45')"
"n = len(y)
MSE = sum(residuals**2) / (n-2)
RMSE = np.sqrt(MSE)
RMSE"
max(simpleModel.resid / RMSE)
simpleModel.rsquared
"fig = plt.figure()
ax = fig.add_subplot(1,1,1)
ax.plot(simpleModel.fittedvalues, diamondData.price, 'o') # as round marks
   # identity line
plt.plot(simpleModel.fittedvalues, simpleModel.fittedvalues, '--') 
   # pretty-up the plot
fig.suptitle(""Relation between estimated and actual diamonds' prices"")
ax.set_ylabel('Estimated Price [SIN $]')
ax.set_xlabel('Actual Price [SIN $]')
ax.grid(True)"
1 - (1-simpleModel.rsquared)*((n-1)/simpleModel.df_resid)
simpleModel.rsquared_adj
"  # calculate standard error for beta1
seBeta1 = sigma / np.sqrt(ssx)
seBeta1"
"tBeta1 = beta1 / seBeta1
tBeta1"
"print (""##            Estimate     Std. Error    t-value        p-value"")
print (""Intercept: "", beta0, seBeta0, tBeta0, pBeta0)
print (""Carats:     "", beta1, seBeta1, tBeta1, pBeta1)"
[beta1 + i*t_value*seBeta1 for i in limits]
"fig, ax = plt.subplots()
ax.plot(x,y, 'o', label=""data"")
ax.plot(x, simpleModel.fittedvalues, 'g-', label=""OLS"")

ax.plot(x, interval_u, 'c--', label = ""Intervals"")
ax.plot(x, interval_l, 'c--')
   # pretty-up the plot
fig.suptitle(""OLS Linear Regression with confidence intervals"")
ax.set_ylabel('Predicted Price [SIN $]')
ax.set_xlabel('Weight [Carat]')
ax.grid(True)
ax.legend(loc='best')"
simpleModel.summary()
dir(simpleModel)
"df = pd.DataFrame(scale(iris.data), columns=iris.feature_names)
df.head()"
"pca = PCA(n_components=4)
pca.fit(df)
var = pca.explained_variance_ratio_.cumsum()
var"
plot(var)
pca.fit(d)
"var = pca.explained_variance_ratio_.cumsum()
var"
plot(var)
"import warnings
warnings.filterwarnings('ignore')
%matplotlib inline
from IPython.display import Image
Image(filename='figures/exp_setup.png', width = 240)"
"Image(filename='figures/fit_W.png', width = 240)"
"Image(filename='figures/fit_RH.png', width = 240)"
"plt.figure(figsize=(8, 8))

# Question A
plt.subplot(4, 1, 1)
a, b = 4, 4
prior = beta(a, b)
posterior = update_beta(prior, N=1, z=1)
prior_posterior_plot(prior, posterior, text='First flip, head')

# Question B
plt.subplot(4, 1, 2)
prior = posterior
posterior = update_beta(prior, N=1, z=1)
prior_posterior_plot(prior, posterior, text='Second flip, head again')

# Question C
plt.subplot(4, 1, 3)
prior = posterior
posterior = update_beta(prior, N=1, z=0)
prior_posterior_plot(prior, posterior, text='3rd flip, tail')

# Question D
plt.subplot(4, 1, 4)
a, b = 4, 4
prior = beta(a, b)
posterior = update_beta(prior, N=3, z=2)
prior_posterior_plot(prior, posterior, text='3 flips, different order (THH)')

plt.legend(loc='best');"
"plt.figure(figsize=(8, 5))

# Question A
plt.subplot(2, 1, 1)
a, b = 1, 1
prior = beta(a, b)
posterior = update_beta(prior, N=100, z=58)
prior_posterior_plot(prior, posterior, hdi_tails=distribution_hdi(posterior),
                     text='From uniform to 100 flips,\n58 heads')

# Question B
plt.subplot(2, 1, 2)
prior = posterior
posterior = update_beta(prior, N=100, z=57)
prior_posterior_plot(prior, posterior, hdi_tails=distribution_hdi(posterior),
                     text='Additional 100 flips,\n57 heads')

plt.legend(loc='best');"
"a = b = 1
prior = beta(a, b)
posterior = update_beta(prior, 50, 40)
text = '''
Prior: a = b = 1 (uniform)
50 participants, 40 choose F
'''.strip()
prior_posterior_plot(prior, posterior, text=text,
                     hdi_tails=distribution_hdi(posterior))

plt.legend(loc='best');"
"a = b = 1
prior = beta(a, b)
posterior = update_beta(prior, 50, 15)
text = '''
Prior: a = b = 1 (uniform)
50 participants, 15 choose F
'''.strip()
prior_posterior_plot(prior, posterior, text=text,
                     hdi_tails=distribution_hdi(posterior))

plt.legend(loc='best');"
"a = b = 0.1
prior = beta(a, b)
posterior = update_beta(prior, N=5, z=4)
text = '''
Prior: a = b = .1
5 flips, 4 heads
'''.strip()
prior_posterior_plot(prior, posterior, text=text)

plt.legend(loc='best');"
"plt.figure(figsize=(8, 5))

# Question A
plt.subplot(2, 1, 1)
a = b = 50  # Strong prior belief that the coin is fair
prior = beta(a, b)
posterior = update_beta(prior, N=10, z=9)
text = '''Update strong prior with
controversial data.
Prior: a = b = 50
N = 10, z = 9
'''.strip()
prior_posterior_plot(prior, posterior, text=text)
plt.legend(loc=2)

# Question B
plt.subplot(2, 1, 2)
a = b = .1  # Strong prior belief that the coin is unfair
prior = beta(a, b)
posterior = update_beta(prior, N=10, z=9)
text = '''Different prior, same data.
Prior: a = b = .1
N = 10, z = 9
'''.strip()
prior_posterior_plot(prior, posterior, text=text)"
"N = 500
p_head = .8

coin_flips = np.random.rand(N) < p_head
running_avarage = np.cumsum(coin_flips) / (np.arange(N) + 1)
print('End proportion: {}'.format(running_avarage[-1]))

# Plot
plt.title('Running proportion of heads')
plt.gca().set(xscale='log')
plt.plot(running_avarage)
plt.ylim(0, 1.1)
plt.xlabel('Flip number')
plt.ylabel('Proportion heads')
# Plot proportion line
plt.plot((0, N), (p_head, p_head), linestyle='dashed');"
"dx = 0.01
x = np.arange(0, 1, dx)
plt.plot(x, p(x))
plt.xlabel('$x$')
plt.ylabel('$y = 6x(1 - x)$')
plt.title('Probability density function');"
"plt.plot(base, norm_pdf(base))
plt.fill_between(x, 0, norm_pdf(x), alpha=0.3)
plt.title('Normal distribution probability density function')
plt.xticks([-1, 0, 1], [""$\mu - alpha$"", ""$\mu$"", ""$\mu + alpha$""])
plt.yticks([], []);"
"x = np.linspace(-3, 3, 100)
y = x ** 3

plt.plot(x, y)
plt.xlabel('$x$')
plt.ylabel('$y = x ^ 3$')
plt.title('Qubic function');"
"import warnings
warnings.filterwarnings('ignore')
%pylab inline
from urllib.request import urlopen
import datetime
import numpy as np
import matplotlib
matplotlib.rcParams['font.size'] = 15
from matplotlib import pyplot as plt

stableURL = 'https://raw.githubusercontent.com/jradavenport/aas225-gender/master/data.csv'
raw = urlopen(stableURL).read().splitlines()

num = []
time = []
sid = []
speaker = []
qs = []
comments = []

for i in range(len(raw)):
    splitline = str(raw[i]).split(';')[0].split(',')
    splitline = [j for j in splitline if j != ' ' and j != '']
    
    # If all columns filled and there is only one entry (M, F) for speaker gender:
    if len(splitline) >= 5 and len(splitline[3]) == 1 and ('M' in splitline[4].upper() or 'F' in splitline[4].upper()):
        iscleansid = splitline[2].replace('.', '').isdigit()
        if iscleansid:
            # Fix sid if necessary
            if len(splitline[2]) != 6:
                splitsid = splitline[2].split('.')
                splitline[2] = '.'.join([splitsid[0], splitsid[1].zfill(2)])
            
            # If this talk ID has not been added:
            if splitline[2] not in sid:
                num.append(splitline[0])
                time.append(splitline[1])
                sid.append(splitline[2])
                speaker.append(splitline[3].upper())
                qs.append(splitline[4].upper())
                comments.append(','.join(splitline[5:]))
            
            # If this talk ID has been added already:
            if splitline[2] in sid:
                # Check if the number of questions in new entry is longer. 
                # If so, replace earlier entry with the longer one.
                if len(qs[sid.index(splitline[2])]) < len(splitline[4].upper()):
                    replaceindex = sid.index(splitline[2])
                    num[replaceindex] = splitline[0]
                    time[replaceindex] = splitline[1]
                    sid[replaceindex] = splitline[2]
                    speaker[replaceindex] = splitline[3].upper()
                    qs[replaceindex] = splitline[4].upper()
                    comments[replaceindex] = ','.join(splitline[5:])"
"def annotateformat(number):
    if number < 1:
        return '%.2f' % number
    else: 
        return '%d' % number

def nicehist(axis, female, male, title):
    axis.bar([0, 1], [female, male], color='k', width=0.5)
    axis.set_xticklabels(['Female', 'Male'])
    axis.set_xticks([0.25, 1.25])
    axis.set_xlim([-0.2, 1.7])
    
    axis.annotate(annotateformat(female), xy=(0.25, female), va='bottom', ha='center')
    axis.annotate(annotateformat(male), xy=(1.25, male), va='bottom', ha='center')
    axis.set_title(title)    
    axis.spines['right'].set_color('none')
    axis.spines['top'].set_color('none')
    axis.xaxis.set_ticks_position('bottom')
    axis.yaxis.set_ticks_position('left')

Nfemalespeakers = len([i for i in speaker if i == 'F'])
Nmalespeakers = len([i for i in speaker if i == 'M'])

fig, ax = plt.subplots(1)
nicehist(ax, Nfemalespeakers, Nmalespeakers, 
         'Gender of Speakers ({0} Total)'.format(Nfemalespeakers+Nmalespeakers))
plt.show()

print('Male speakers/All speakers= {0:.1f}%'
      .format(100*float(Nmalespeakers)/(Nfemalespeakers + Nmalespeakers)))
print('Female speakers/All speakers = {0:.1f}%'
      .format(100*float(Nfemalespeakers)/(Nfemalespeakers + Nmalespeakers)))"
"allqs = ''.join([i for i in qs if 'M' in i or 'F' in i])
Nfemaleqs = len([i for i in allqs if i == 'F'])
Nmaleqs = len([i for i in allqs if i == 'M'])
    
fig, ax = plt.subplots(1)
nicehist(ax, Nfemaleqs, Nmaleqs, 'Gender of Question-Askers (%d Total)' % (Nfemaleqs+Nmaleqs))

print('Male qs/All qs = {0:.1f}%'.format((100*float(Nmaleqs)/(Nfemaleqs + Nmaleqs))))
print('Female qs/All qs = {0:.1f}%'.format((100*float(Nfemaleqs)/(Nfemaleqs + Nmaleqs))))"
"fig, ax = plt.subplots(1)
Nbins = 7

ax.hist([len(i) for i in qs], Nbins, histtype='stepfilled', color='w', lw=2)
ax.set_title('Number of Questions Per Talk:')
ax.set_xlabel('Number of Questions')
plt.show()

malespeaker_Nqs = [len(qs[i]) for i in range(len(num)) if speaker[i] == 'M']
femalespeaker_Nqs = [len(qs[i]) for i in range(len(num)) if speaker[i] == 'F']

fig, ax = plt.subplots(1)
ax.hist(malespeaker_Nqs, Nbins, histtype='step', color='b', lw=2, label='Male Speaker', range=[1,8])
ax.hist(femalespeaker_Nqs, Nbins, histtype='step', color='r', lw=2, label='Female Speaker', range=[1,8])
ax.set_title('Number of Questions Per Talk:')
ax.set_xlabel('Number of Questions')
ax.legend()
plt.show()"
"Nmalesfirst = len([i for i in qs if i[0] == 'M'])
Nfemalesfirst = len([i for i in qs if i[0] == 'F'])

fig, ax = plt.subplots(1)
nicehist(ax, Nfemalesfirst, Nmalesfirst, 
         'Gender of First Question-Askers ({0} Total)'.format(Nfemalesfirst+Nmalesfirst))
plt.show()



print('Male first qs/All qs = {0:.1f}%'.format((100*float(Nmalesfirst)/(Nfemalesfirst + Nmalesfirst))))
print('Female first qs/All qs = {0:.1f}%'.format((100*float(Nfemalesfirst)/(Nfemalesfirst + Nmalesfirst))))"
"malesfirst = [i for i in qs if i[0] == 'M']
malesfirst_percentagefemaleafter = [float(i.count('F'))/len(i) for i in malesfirst[1:] \
                                    if len(i) >= 2]

femalesfirst = [i for i in qs if i[0] == 'F']
femalesfirst_percentagefemaleafter = [float(i.count('F'))/len(i) for i in femalesfirst[1:] 
                                      if len(i) >= 2]

fig, ax = plt.subplots(1, 2, figsize=(16,8))
Nbins = 5

ax[0].hist(malesfirst_percentagefemaleafter, Nbins, color='k', 
           range=[0,1], histtype='step', lw=2, 
           weights=len(malesfirst_percentagefemaleafter)*[1./len(malesfirst_percentagefemaleafter)])
ax[0].set_title('Male Asks First Question')

ax[1].hist(femalesfirst_percentagefemaleafter, Nbins, color='k', 
           range=[0,1], histtype='step', lw=2,
           weights=len(femalesfirst_percentagefemaleafter)*[1./len(femalesfirst_percentagefemaleafter)])

ax[1].set_title('Female Asks First Question')

for axes in ax:
    axes.set_xlim([0, 1])
    axes.set_xlabel(""Fraction subsequent Q's asked by females"")
    axes.set_ylabel(""Fraction of all talks"")
    axes.set_ylim([0, 1])
plt.show()"
"fig, ax = plt.subplots(1, figsize=(10,8))
Nbins = 5
ax.hist(malesfirst_percentagefemaleafter, Nbins, color='b', 
           range=[0,1], histtype='step', lw=3, 
           weights=len(malesfirst_percentagefemaleafter)*[1./len(malesfirst_percentagefemaleafter)],
           label='Male Asks First Question')

ax.hist(femalesfirst_percentagefemaleafter, Nbins, color='r', 
           range=[0,1], histtype='step', lw=3,
           weights=len(femalesfirst_percentagefemaleafter)*[1./len(femalesfirst_percentagefemaleafter)],
           label='Female Asks First Question')

ax.set_xlim([0, 1])
ax.set_ylim([0, 1])
ax.set_xlabel(""Fraction subsequent Q's asked by females"")
ax.set_ylabel(""Fraction of all talks"")
ax.spines['right'].set_color('none')
ax.spines['top'].set_color('none')
ax.xaxis.set_ticks_position('bottom')
ax.yaxis.set_ticks_position('left')
ax.legend()
plt.show()"
"malesfirst_lenafter = float(len(''.join(malesfirst)))
malesfirst_Nfemaleafter = sum([i.count('F') for i in malesfirst[1:]
                                    if len(i) >= 2])/malesfirst_lenafter
femalesfirst_lenafter = float(len(''.join(femalesfirst)))
femalesfirst_Nfemaleafter = sum([i.count('F') for i in femalesfirst[1:]
                                      if len(i) >= 2])/femalesfirst_lenafter

fig, ax = plt.subplots(1, 2, figsize=(16,8))

nicehist(ax[0], malesfirst_Nfemaleafter, 1-malesfirst_Nfemaleafter,
         'Male Asks First Q ({0} Total)'.format(len(''.join(malesfirst))))

nicehist(ax[1], femalesfirst_Nfemaleafter, 1-femalesfirst_Nfemaleafter,
         'Female Asks First Q (%d Total)'.format(len(''.join(femalesfirst))))

for axes in ax:
    axes.set_ylim([0,1])

plt.show()"
"malespeaker_qs = [qs[i] for i in range(len(num)) if speaker[i] == 'M']
femalespeaker_qs = [qs[i] for i in range(len(num)) if speaker[i] == 'F']

malespeaker_Nfemaleqs = ''.join(malespeaker_qs).count('F')
malespeaker_Nquestions = len(''.join(malespeaker_qs))
malespeaker_percentagefemaleqs = malespeaker_Nfemaleqs/float(malespeaker_Nquestions)

femalespeaker_Nfemaleqs = ''.join(femalespeaker_qs).count('F')
femalespeaker_Nquestions = len(''.join(femalespeaker_qs))
femalespeaker_percentagefemaleqs = femalespeaker_Nfemaleqs/float(femalespeaker_Nquestions)

fig, ax = plt.subplots(1, 2, figsize=(16,8))

nicehist(ax[0], malespeaker_percentagefemaleqs, 1-malespeaker_percentagefemaleqs, 
         'Male Speaker ({0} Total)'.format(malespeaker_Nquestions))

nicehist(ax[1], femalespeaker_percentagefemaleqs, 1-femalespeaker_percentagefemaleqs,
          'Female Speaker ({0} Total)'.format(femalespeaker_Nquestions))
for axes in ax:
    axes.set_xlabel(""Fraction Q's asked"")
    axes.set_ylim([0,1])


plt.show()"
"def getfirstindex(searchstring, letter):
    if letter in searchstring:
        return searchstring.index(letter) + 1
    return None
    
malespeaker_orderfemale = [getfirstindex(i, ""F"") for i in malespeaker_qs if 'F' in i]
femalespeaker_orderfemale = [getfirstindex(i, ""F"") for i in femalespeaker_qs if 'F' in i]

fig, ax = plt.subplots(1, figsize=(10,8))
Nbins = 3
ax.hist(malespeaker_orderfemale, Nbins, color='b', 
           range=[1,3], histtype='step', lw=3, 
           label='Male Speaker')

ax.hist(femalespeaker_orderfemale, Nbins, color='r', 
           range=[1,3], histtype='step', lw=3,
           label='Female Speaker')

ax.set_xlim([1, 3])
ax.set_xticks([1,2,3])
#ax.set_ylim([0, 1])
ax.set_xlabel(""First Female Question Position in Sequence"")
#ax.set_ylabel(""Fraction of all talks"")
ax.set_title('In talks when a female asked a question:')
ax.spines['right'].set_color('none')
ax.spines['top'].set_color('none')
ax.xaxis.set_ticks_position('bottom')
ax.yaxis.set_ticks_position('left')
ax.legend()
plt.show()"
"N_qs = np.array([len(i) for i in qs])
percentagefemaleqs = np.array([i.count('F')/float(len(i)) for i in qs])
order = np.argsort(N_qs)

#plt.plot(N_qs, percentagefemaleqs, '.')

for i in range(1, max(N_qs)):
    considertalks = i == N_qs
    medianpercentagefemaleqs = np.median(percentagefemaleqs[considertalks])
    stdpercentagefemaleqs = np.std(percentagefemaleqs[considertalks])
    plt.errorbar(i, medianpercentagefemaleqs, yerr=stdpercentagefemaleqs, fmt='o', color='k')
plt.ylim([0, 1.0])
plt.xlabel('Number of questions asked')
plt.ylabel('Fraction of Qs asked by females')
plt.show()"
"import warnings
warnings.filterwarnings('ignore')
%matplotlib inline
import matplotlib.pyplot as plt
plt.style.use('fivethirtyeight')
plt.rcParams['font.family'] = 'serif'
plt.rcParams['font.serif'] = 'Ubuntu'
plt.rcParams['font.monospace'] = 'Ubuntu Mono'
plt.rcParams['font.size'] = 14
plt.rcParams['axes.labelsize'] = 14
plt.rcParams['axes.labelweight'] = 'bold'
plt.rcParams['axes.titlesize'] = 14
plt.rcParams['xtick.labelsize'] = 12
plt.rcParams['ytick.labelsize'] = 12
plt.rcParams['legend.fontsize'] = 14
plt.rcParams['figure.titlesize'] = 18

import c_ve_3param

f = range(0, 600)
ve_params = [(10e3, 2e3, 6666.7, 'Phantom 1'), (15e3, 4e3, 5500, 'Phantom 2'), (20e3, 4e3, 4000, 'Phantom 3')] # G0, Ginf, beta
label_positions = [(450, 2.0, 'Phantom 1'), (375,3.1, 'Phantom 2'), (200, 3.4, 'Phantom 3')]
c140_yoffset = [0.1, 0.1, 0.2]
markers_on = [140]
plt.figure(figsize=(10, 8))
for n, ve in enumerate(ve_params):
    c = c_ve_3param.calc(f, ve[0], ve[1], ve[2])
    h = plt.plot(f, c, '-o', label=ve[3], markevery=markers_on, markersize=12)
    plt.xticks(range(0, 600, 140), range(0, 600, 140))
    plt.xlabel('Frequency (Hz)')
    plt.ylabel('Phase Velocity (m/s)')
    plt.title('c @ 140 Hz annotated on plot')
    c140 = c_ve_3param.calc(140, ve[0], ve[1], ve[2])
    plt.annotate(""%.1f"" % c140, xy=(140, c140), xycoords='data', xytext=(140, c140+c140_yoffset[n]), textcoords='data', color=h[0].get_color())
    plt.text(label_positions[n][0], label_positions[n][1], label_positions[n][2], color=h[0].get_color())"
"from collections import Counter

Counter(message)"
"import warnings
warnings.filterwarnings('ignore')
from scipy.stats import norm
from numpy import linspace
from pylab import plot,show,hist,figure,title

%matplotlib inline

from matplotlib import pyplot as plt
plt.style.use('ggplot')
_ = plt.figure(figsize=(16,8))"
"from scipy.stats import norm,rayleigh

samp = rayleigh.rvs(loc=5,scale=2,size=150) # samples generation

param = rayleigh.fit(samp) # distribution fitting

x = linspace(5,13,100)
# fitted distribution
pdf_fitted = rayleigh.pdf(x,loc=param[0],scale=param[1])
# original distribution
pdf = rayleigh.pdf(x,loc=5,scale=2)

plt.figure(figsize=(16,8))
title('Rayleigh distribution')
plot(x,pdf_fitted,'r-',x,pdf,'b-')
_ = hist(samp,normed=1,alpha=.3,bins=16)"
"# picking 150 of from a normal distrubution
# with mean 0 and standard deviation 1
samp = norm.rvs(loc=0,scale=1,size=150) 

param = norm.fit(samp) # distribution fitting

# now, param[0] and param[1] are the mean and 
# the standard deviation of the fitted distribution
x = linspace(-5,5,100)
# fitted distribution
pdf_fitted = norm.pdf(x,loc=param[0],scale=param[1])
# original distribution
pdf = norm.pdf(x)

plt.figure(figsize=(16,8))
title('Normal distribution')
plot(x,pdf_fitted,'r-',x,pdf,'b-')
_ = hist(samp,normed=1,alpha=.3, bins=16)"
"# probability of symbols
p = np.array( [ .25, .2, .2, .15, .07, .05, .025, .025, .02, .01] )
    
# get codeword lengths by rounding towards 0 and mapping to int
# (type conversion being necessary to use cw_lengths as indices)
cw_lengths = np.ceil( - np.log2( p ) )
cw_lengths = cw_lengths.astype( int )

# show probabilities and codewords lengths
print( 'Probabilities:' )
print( 'p = {}\n'.format( p ) )

print( 'Codeword lengths:' )
print( 'L(x_n) = {}'.format( cw_lengths ) )"
"plt.axis('square');
plt.scatter(complex_noisy_samples.real, complex_noisy_samples.imag, s=6, alpha=0.3)
plt.xlim(-3000, 3000); plt.ylim(-3000, 3000)
plt.xlabel('In-phase component'); plt.ylabel('Quadrature component')
plt.tight_layout()"
"seaborn.jointplot(complex_noisy_samples.real, complex_noisy_samples.imag, kind=""reg"", size=6, joint_kws={""scatter_kws"": {""s"":6, ""alpha"":0.3}})
plt.xlim(-3000, 3000); plt.ylim(-3000, 3000)
plt.xlabel('In-phase component');  plt.ylabel('Quadrature component')"
"t = np.arange(num_samples) * T_S  # vector of sampling time instances
plt.plot(t*1e3, complex_noisy_samples.real, t*1e3, complex_noisy_samples.imag, alpha=0.7)
plt.title(""Time Domain of I an Q components"")
plt.xlabel('Time / ms'); plt.ylabel('Amplitude'); plt.legend(('inphase', 'quadrature'));"
"plt.subplot(121)
plt.acorr(complex_noisy_samples.real, usevlines=True, maxlags=50)
plt.ylabel('$\phi_{\Re\Re}$'); plt.xlabel('lag / Samples'); plt.axis('tight')

plt.subplot(122)
plt.acorr(complex_noisy_samples.imag, usevlines=True, maxlags=50)
plt.ylabel('$\phi_{\Im\Im}$'); plt.xlabel('lag / Samples'); plt.axis('tight');"
"# Plot normalized histogram
plt.hist(complex_noisy_samples.real, bins=40, normed=True, alpha=0.5);
plt.xlabel('Amplitude'); plt.ylabel('Probability')

# Plot normal distribution
x = np.linspace(-3000, 3000, 100)
_ = plt.plot(x,stats.norm.pdf(x,0,sigma))"
"freqs, Pxx = signal.welch(complex_noisy_samples,
                          fs=f_S, nfft=1024, noverlap=0,
                          window=""hanning"", scaling=""density"",
                          return_onesided=False)
freqs = np.fft.fftshift(freqs); Pxx = np.fft.fftshift(Pxx)
# Plot PSD, use logarithmic scale:
plt.plot(freqs / 1000, 10*np.log10(np.abs(Pxx)))
plt.ylim(-70, 10)
plt.ylabel('$\Phi_{XX}(f)$ [dB]'); plt.xlabel('$f$ / kHz');"
"cutoff_freq = 1e5  # cutoff frequency of lowpass filter: 100 kHz
numtaps = 51      # number of filter taps 

# FIR filter design:
lpass_taps = signal.firwin(numtaps, cutoff_freq, nyq=f_nyquist)  #  Get filter taps
freq_norm, response = signal.freqz(lpass_taps) # filter response in frequency domain
freq = freq_norm * f_nyquist / np.pi

# Plot frequency response:
plt.plot(freq / 1e3, 10*np.log10(np.abs(response))) 
plt.title('Frequency response of lowpass filter'); plt.ylabel('$H(f)$ [dB]'); plt.xlabel('$f$ / kHz');"
"# Filter noise with lowpass:
filtered_x = signal.lfilter(lpass_taps, 1.0, complex_noisy_samples)
# Calculate PSD:
freqs, Pxx = signal.welch(filtered_x,
                          nfft=1024, fs=f_S, window=""hanning"", noverlap=0, scaling=""density"", return_onesided=False)
plt.plot(np.fft.fftshift(freqs),
         10*np.log10(np.abs(np.fft.fftshift(Pxx))))
# Plot PSD, use logarithmic scale:
plt.title('PSD of low-pass filtered Gaussian noise');
plt.axis('tight'); plt.ylim(-70, 10); plt.ylabel('$P_{XX}(f)$'); plt.xlabel('$f$ / kHz');"
"plt.acorr(filtered_x.real, usevlines=False, maxlags=50, marker=None, linestyle='-')
plt.acorr(filtered_x.imag, usevlines=False, maxlags=50, marker=None, linestyle='-')
plt.xlabel('lag / Samples')
plt.legend(('inphase', 'quadrature'));"
"# Take every 5th element of filtered signal
factor = 5; filt_x_dwnsampled = filtered_x[::factor]
plt.acorr(filt_x_dwnsampled.real, usevlines=False, maxlags=50, marker=None, linestyle='-')
plt.acorr(filt_x_dwnsampled.imag, usevlines=False, maxlags=50, marker=None, linestyle='-')
plt.title('Autocorrelation function of downsampled signal')
plt.xlabel('lag / Samples'); plt.axis('tight'); plt.legend(('inphase', 'quadrature'));"
"freqs, Pxx = signal.welch(filt_x_dwnsampled,
             fs=f_S/factor,nfft=1024, window=""hanning"", noverlap=0, scaling=""density"", return_onesided=False)
# Plot PSD, use logarithmic scale:
plt.plot(np.fft.fftshift(freqs),
         10*np.log10(np.abs(np.fft.fftshift(Pxx))))
plt.axis('tight'); plt.ylim(-70, 10)
plt.ylabel('$P_{XX}$'); plt.xlabel('$f$ / kHz');"
"# Calculate impulse response and frequency response from definition:
for idx,r in enumerate(np.nditer(r_values)):
    # time domain:
    g[idx,:] = np.sinc(t)*np.cos(np.pi*t*r)/(1-np.power(2*r*t,2)) # np.sinc = sin(pi*x)/(pi*x)
    # frequency domain:
    for idx2,myf in enumerate(np.nditer(f)):
        if np.abs(myf) < (1-r)/2:
            G[idx,idx2] = 1
        elif np.abs(myf) < (1+r)/2:
            G[idx,idx2] = 0.5*(1+np.cos(np.pi/r*(np.abs(myf)-(1-r)/2)))
            
# plotting:
plt.subplot(121) # Time domain
plt.plot(np.transpose(t_mat), np.transpose(g), alpha=0.7)
plt.ylim(-0.25, 1.1)
plt.xlabel('t / T'); plt.ylabel('g(t)'); plt.legend(('r=0','r=0.5','r=1.0'))

plt.subplot(122) # Frequency Domain
plt.plot(np.transpose(f_mat), np.transpose(G), alpha=0.7)
plt.ylim(-0.25, 1.1)
plt.xlabel('fT'); plt.ylabel('G(f)'); plt.legend(('r=0','r=0.5','r=1.0'),loc=8);"
"Image('images/ligo-press.png', width='80%')"
"Image(""images/oreilly-socialweb.png"", width=""80%"")"
"Image(""images/p4sp-clipped.png"", width=""60%"")"
"Image(""images/oreilly-beta.png"", width=""80%"")"
"Image(""images/oreilly-oriole.png"", width=""80%"")"
"Image(""images/nature-genetics.png"", width=""80%"")"
"Image(""images/reinhart-rogoff-replication.png"", width=""80%"")"
"Image(""images/python-data-science-handbook.png"")"
"Image(""images/intro-ml-python.png"")"
"df1 = pd.read_csv('data/police_locals.csv')
df1 = df1.replace('**',np.nan)
df1['all'] = df1['all'].astype('float')
df1['white'] = df1['white'].astype('float')
df1['non-white'] = df1['non-white'].astype('float')
df1['black'] = df1['black'].astype('float')
df1['hispanic'] = df1['hispanic'].astype('float')"
"sns.boxplot(x='ethnicity', y='percent', data=df2);"
"from IPython.display import Image
Image('images/538_local_police.png', width=600)"
"Image(""images/nbviewer1.png"", width=""80%"")"
"Image(""images/mybinder.png"", width=""80%"")"
"Image(""images/jupyterhub-admin.png"", width=""80%"")"
"from IPython.display import Image
Image(""images/calpoly_logo.png"", width=400)"
"Image(""images/jupyter_logo.png"", width=400)"
"Image(""images/ipython_logo.png"", width=400)"
"Image(""images/buzzfeed-article.png"", width=""80%"")"
"Image(""images/buzzfeed-repo.png"", width=""80%"")"
"Image(""images/latimes-article.png"", width=""80%"")"
"Image(""images/latimes-repo.png"", width=""80%"")"
Image('images/JupyterBlocksA.png')
Image('images/JupyterBlocksB.png')
Image('images/JupyterBlocksC.png')
Image('images/JupyterBlocksD.png')
"Image(""images/quantopian-research.png"", width=""90%"")"
Image('images/JupyterBlocksE.png')
"Image(""images/ipython_logo.png"", width=400)"
"Image(""images/jupyter_logo.png"", width=400)"
"Image('images/lego-filebrowser.png', width='80%')"
"Image('images/lego-terminal.png', width='80%')"
"Image('images/lego-texteditor.png', width='80%')"
"Image('images/lego-output.png', width='80%')"
bayarea16.isin(['manifold'])
"matchAgency(nyc,emdf)"
"matchAgency(pilgrims,emdf)"
"raw='./data/it-list.csv'
# raw='/Users/ganesha/Documents/src/dataviz/inc5000/csv_data/it-list.txt'
df = pd.read_csv(raw)
df"
"df[""TAGS""].apply(tagSeries)"
plt.plot(trace2)
"# check filenames of the data we are exploring:
glob.glob('./data/*.xls')"
"# Import the excel file and call it xls_file
xls_file = pd.ExcelFile('./data/Table01—Estimated Revenue 2007 through 2015.xls')
xls_file"
"# Load the xls file's Sheet1 as a dataframe
df = xls_file.parse('Sheet1')
df"
"ds_train_x[[""maxPlayerLevel"", ""numberOfAttemptedLevels""]].hist(bins=10, figsize = (15,5))"
"# different view of histograms
ds_train_x.hist(bins=10, figsize = (20,20), normed=1)"
"# different view of histograms
ds_train_x.hist(bins=10, figsize = (20,20))"
"ds_train_y.hist(bins=10, figsize = (10,10))
#ds_train_y.hist(bins=10, figsize = (10,10), normed=1)"
"clf = clf.fit(X, y)
print (clf.coef_)
print (clf.intercept_)"
"clfSVC = SVC()
clfSVC = clfSVC.fit(X, y)
print ('Intercept_: {0}\n'.format(clfSVC.intercept_))
print (clfSVC)"
"from sklearn.linear_model import LogisticRegression

clfLR = LogisticRegression().fit(X, y)
print ('Intercept_: {0}\n'.format(clfLR.intercept_))
print (clfLR)"
pca.components_  
"print (pca.explained_variance_ratio_)  
print (pca.explained_variance_ratio_.sum())"
"plot_2D(X_pca, iris.target, iris.target_names)"
"np.round(kmeans.cluster_centers_, decimals=2)"
"plot_2D(X_pca, kmeans.labels_, [""c0"", ""c1"", ""c2""])"
"import warnings
warnings.filterwarnings('ignore')
!head -20 /Users/sr320/data-genomic/tentacle/OlyO-v4-transcriptome.fasta"
"!fgrep -c "">"" /Users/sr320/data-genomic/tentacle/OlyO-v4-transcriptome.fasta"
"import warnings
warnings.filterwarnings('ignore')
!head /Users/sr320/Dropbox/hummingbird-ipython-nbs/OlyO-v6/blastout_query.part-01.fasta_sp"
"!cat /Users/sr320/Dropbox/hummingbird-ipython-nbs/OlyO-v6/blastout_*fasta_sp \
> ./data/blastout_sp"
!head /Users/sr320/Dropbox/hummingbird-ipython-nbs/OlyO-v6/error-blastout_query.part-10.fasta_sp
!wc -l ./data/blastout_sp
!wc -l /Users/sr320/Dropbox/hummingbird-ipython-nbs/OlyO-v6/blastout_query.part-01.fasta_sp
"!cat /Users/sr320/Dropbox/hummingbird-ipython-nbs/OlyO-v6/error-*fasta_sp \
> ./data/error_blastout_sp"
!wc -l ./data/error_blastout_sp
"!sed 's/|c/_c/g' ./data/blastout_sp \
> ./data/blastout_sp_rn
!head ./data/blastout_sp_rn"
"!cat /Users/sr320/Dropbox/hummingbird-ipython-nbs/OlyO-v6/blastout_*fasta_nt \
> ./data/blastout_nt"
!wc -l ./data/blastout_nt
"!cat /Users/sr320/Dropbox/hummingbird-ipython-nbs/OlyO-v6/err*fasta_nt \
> ./data/error_blastout_nt"
!wc -l /Users/sr320/Dropbox/hummingbird-ipython-nbs/OlyO-v6/blastout_query.part-10.fasta_nt
"!fgrep ""Bacteria"" ./data/blastout_nt | wc -l"
"plt.figure(1)
plt.subplot(111)
plt.plot(t,a)
plt.title(""Synthetized time history"")
plt.xlabel(""Time [s]"")
plt.ylabel(""Acceleration [g]"")
plt.grid(True)

plt.figure(2)
plt.subplot(211)
plt.plot(f, A, 'rs')
plt.title(""Amplitude and Phase Angle @ frequency"")
plt.ylabel(""Amplitude [g]"")
plt.grid(True)

plt.subplot(212)
plt.plot(f, fi, 'gs')
plt.xlabel(""Frequency [Hz]"")
plt.ylabel(""Phase angle [rad]"")
plt.grid(True)

plt.figure(3)
plt.subplot(111)
plt.semilogy(range(len(f)), f, 'y^')
plt.grid(True)
plt.title('Frequency change by 1/6 octave')
plt.xlabel('step')
plt.ylabel('Frequency [Hz]')"
"import warnings
warnings.filterwarnings('ignore')
#importing all required modules
#important otherwise pop-up window may not work
%matplotlib inline 
import numpy as np
import scipy as sp
import matplotlib as mpl
import matplotlib.pyplot as plt
import math
import cmath
import seaborn"
"#definition of module of current density J (otherwise only real component will be plotted)
modJ = [abs(J[i]) for i in range(0, len(J))]
rJ = [J[i].real for i in range(0, len(J))]
iJ = [J[i].imag for i in range(0, len(J))]

#plot preparation
fig, ax = plt.subplots(2, figsize=(10,10))

ax[0].plot(x*10**3,modJ, label=""|J| [A/mm2]"", color = ""green"")
ax[0].plot(x*10**3,rJ, label=""Re{J} [A/mm2]"", color = ""red"")
ax[0].set_ylabel(""Current density [A/mm2]"")
ax[0].set_xlabel(""Conductor width [mm]"")
ax[0].set_title(""Current density distribution along conductor width"")
#ax.axis([-5, 5, 0.9, 1.3])
ax[0].legend()

ax[1].plot(x*10**3,iJ, label=""Im{J} [A/mm2]"", color = ""blue"")
ax[1].set_ylabel(""Current density [A/mm2]"")
ax[1].set_xlabel(""Conductor width [mm]"")
ax[1].legend()
plt.tight_layout()"
"#Plotting
import seaborn
import matplotlib.pyplot as plt
%matplotlib inline

#conductor height vector
y = np.linspace(0, 100, n)

#plot preparation
fig, ax = plt.subplots(2, figsize=(10,10))

ax[0].plot(y,modJ, label=""|J| [A/mm2]"", color = ""green"")
ax[0].set_ylabel(""Current density [A/mm2]"")
ax[0].set_xlabel(""Conductor height [mm]"")
ax[0].set_title(""Current density distribution along conductor width"")
ax[0].legend()

ax[1].plot(y,phJ, label=""Arg{J} [deg]"", color = ""blue"")
ax[1].set_ylabel(""Current density phase [deg]"")
ax[1].set_xlabel(""Conductor height [mm]"")
ax[1].legend()
plt.tight_layout()"
"Pac, Pdc, k"
"plt.plot(t,ua,t,ub,t,uc)
plt.title(""Three phase voltages"")
plt.xlabel(""Time [s]"")
plt.ylabel(""Voltage [V]"")
plt.axis([0, 0.02, -400, 400])
plt.grid(True)"
"kresl(t, ia, iadc, iau, ""Phase A"")
kresl(t, ib, ibdc, ibu, ""Phase B"")
kresl(t, ic, icdc, icu, ""Phase C"")"
"interact(current, u=(100,400), alpha=(0,phi))"
"p = figure(plot_width=400, plot_height=400)
p.line(t,ia, color=""navy"", line_width=2, legend=""final current"")
p.line(t,iau, color=""red"", line_width=1, legend=""periodic component"")
p.line(t,iadc, color=""green"", line_width=1, legend=""aperiodic component"")
show(p)"
"# after integration vectors t and uc had different lengths so I need to append one item
np.append(uc, uc[999])"
"fig, ax = plt.subplots(nrows=2, ncols=1, figsize=(8,8))

ax[0].plot(t,ia, label=""Current"")
ax[0].set_ylabel(""Current [A]"")
ax[0].set_xlabel(""Time [s]"")
ax[0].set_title(""Current in R-L-C circuit during switch-on"")
ax[0].legend()

ax[1].plot(t,ua, label=""Supply voltage"", color=""green"")
ax[1].plot(t,uc, label=""Capacitor voltage"", color=""orange"")
ax[1].set_ylabel(""Voltage [V]"")
ax[1].set_xlabel(""Time [s]"")
ax[1].set_title(""Supply voltage"")
ax[1].legend()

fig.tight_layout()"
"fig, ax = plt.subplots(nrows=2, ncols=1, figsize=(8,8))

ax[0].plot(t,ib, label=""Current"")
ax[0].set_ylabel(""Current [A]"")
ax[0].set_xlabel(""Time [s]"")
ax[0].set_title(""Current in R-L-C circuit during switch-on"")
ax[0].legend()

ax[1].plot(t,ub, label=""Line voltage"", color=""green"")
ax[1].plot(t,uc2, label=""Capacitor voltage"", color=""orange"")
ax[1].set_ylabel(""Voltage [V]"")
ax[1].set_xlabel(""Time [s]"")
ax[1].set_title(""Supply voltage"")
ax[1].legend()

fig.tight_layout()"
"plt.figure(figsize=(10,5))
plt.plot(t,ia, label=""Current"")
plt.ylabel(""Current [A]"")
plt.xlabel(""Time [s]"")
plt.title(""Current in R-L circuit during switch-on"")
plt.legend();"
"plt.figure(figsize=(10,5))
plt.plot(t, uR, label='R')
plt.plot(t, U, label='supply')
plt.plot(t, uL, label='L')
plt.legend();"
"#half past x
#y past x
#quarter till x
#y till x
hours_eng = {0: ""Midnight"", 1:""one"", 2: ""two"", 3:""three"", 4:""Four"", 5:""Five"", 6:""Six"",
             7:""Seven"", 8:""Eight"", 9:""Nine"",10:""Ten"",11:""Eleven"",12:""Noon""}
min_eng = {1:""One"", 2:""Two"", 3:""Three"", 4:""Four"", 5:""Five"", 6:""Six"", 7:""Seven"", 8:""Eight"", 9:""Nine"", 10:""Ten"",
          11:""Eleven"", 12:""Twolve"", 13:""Thirteen"", 14:""Fourteen"", 15:""Quarter"", 16:""Sixteen"", 17:""Seventeen"", 18:""Eighteen"", 19:""Nineteen"",20:""Twinty"",
          21:""Twinty-one"", 22:""Twinty-two"", 23:""Twinty-three"", 24:""Twinty-four"", 25:""Twinty-five"",
          26:""Twinty-six"", 27:""Twinty-seven"", 28:""Twinty-eight"", 29:""Twinty-nine"", 30:""Half""}

def print_time(time_str):
    time_str = time_str.replace("":"", """")[::-1]
    minu = int(time_str[:2][::-1])
    hour = int(time_str[2:][::-1])
    #print(minu, hour)
    pend = ""past""
    if minu > 30:
        minu = 60 -minu
        hour = ( hour + 1) % 24
        pend = ""till""
    hour = hour % 24
    result = min_eng[minu] + "" "" + pend + "" "" + hours_eng[hour - ((hour-1)//12 == 1) * 12] #noon is not past midnight
    return result
    
print([print_time(str(i)+"":25"") for i in range(24)])"
"g = 9.8

def draw_graph(x, y):
    plot(x, y)
    xlabel('x-coordinate')
    ylabel('y-coordinate')
    title('Projectile motion at different initial velocities and angles')
    
def draw_markers(x,y):
    plot((x),(y),'o',color='y')
    
def frange(start, final, interval):

    numbers = []
    while start < final:
        numbers.append(start)
        start = start + interval
    
    return numbers

def draw_trajectory(u, theta, t_flight, t_time):
    # list of x and y co-ordinates
    x = []
    y = []
    intervals = frange(0, t_flight, 0.001)
    for t in intervals:
        x.append(u*math.cos(theta)*t)
        y.append(u*math.sin(theta)*t - 0.5*g*t*t)

    #create the graph
    draw_graph(x, y)
    time_step = min(int(t_time/0.001), len(intervals)) -1
    draw_markers(x[time_step], y[time_step])

def new_trajectory(v, theta, at_time):
    #num_trajectories = 3
    velocity = v
    angle = math.radians(float(theta))
    
    t_flight = 2*velocity*math.sin(angle)/g
    S_x = velocity*math.cos(angle)*t_flight
    S_y = velocity*math.sin(angle)*(t_flight/2) - (1/2)*g*(t_flight/2)**2
    draw_trajectory(velocity, angle, t_flight, at_time)

    # Add a legend and show the graph
    legends = []

    legends.append('{0} - {1}'.format(velocity, math.degrees(angle)))
    legend(legends)
    xlim(0, 10)
    ylim(0, 10)
    show()
#new_trajectory()
interact(new_trajectory, 
         v=FloatSlider(min=0.1, max=10.0, step=0.5, value=9.0), 
         theta= FloatSlider(min=0.1, max=90.0, step=0.5, value=45.0),
        at_time=FloatSlider(min=0.1, max=2.0, value=2.0))

"
"# horizontal from general dynamics of IPM

z,X0 = 0.8,0.2
th0,r0 = np.arctan2(X0,z),np.sqrt(z**2+X0**2)
x0 = np.array([th0,r0,0.0,0.0]) # x = [th,r,w,rp]

m,g = 50,9.81 # uIPMFreeFallFcn # uIPMConstantLengthFcn # uIPMHorizontalFcn
t = np.linspace(0,0.8,41)
sols = []
for fun in {uIPMFreeFallFcn, uIPMConstantLengthFcn, uIPMHorizontalFcn, uIPMFallFcn}:
    pars = (m,g,fun,(m,g),)
    # odeint solver
    sols.append(odeint(dynGeneralIPM, x0, t, args=(pars,)))

plt.figure(1,figsize=(10,10))
for i,sol in enumerate(sols):
    plt.subplot(2,2,i+1),
    plt.plot(sol[:,1]*np.sin(sol[:,0]),sol[:,1]*np.cos(sol[:,0]),'o-')
    plt.axis('equal'),plt.axis((0,1.8,0.0,1.2))

plt.show()
"
"#
x0s = [[-0.151, 0.467],
       [  -0.2, 0.791],
       [   0.2,-0.791],
       [ 0.151,-0.467]]
z,g = 0.8,9.81
t = np.linspace(0,0.8,41)
pars = (g,z,lambda t: 0)
# odeint solver
plt.figure(1,) #figsize=(5,5)
for i,x0 in enumerate(x0s):
    sol = odeint(dynLIPM, x0, t, args=(pars,))
    E = (x0[1]**2-g/z*x0[0]**2)/2
    if E<0: 
        xrev,vrev = np.sqrt(-2*z*E/g), np.nan
    else: 
        xrev,vrev = np.nan, np.sqrt(2*E)
    print('Energia: {0:.3f}, vtop: {0:.3f}, xrev: {0:.3f}: '.format(E,vrev,xrev))
    plt.subplot(2,2,i+1),plt.plot(t,sol[:,0],'o-')
    plt.title('Energia: {0:.3f}J'.format(E))

plt.show()
"
"#
x0 = [  -0.2, 0.791]
z,g = 0.8,9.81
t = np.linspace(0,3.2,1601)
zmpFcn = lambda t: 0.4*stepFcn(t,0.8)+0.40*stepFcn(t,1.59)+0.305*stepFcn(t,2.4)
pars = (g,z,zmpFcn)
# odeint solver
sol = odeint(dynLIPM, x0, t, args=(pars,))
E = (x0[1]**2-g/z*x0[0]**2)/2
if E<0: 
    xrev,vrev = np.sqrt(-2*z*E/g), np.nan
else: 
    xrev,vrev = np.nan, np.sqrt(2*E)
print('Energia: {0:.3f}, vtop: {0:.3f}, xrev: {0:.3f}: '.format(E,vrev,xrev))
plt.plot(t,sol[:,0]-zmpFcn(t),'o-'),plt.grid('on')
plt.title('Energia: {0:.3f}J'.format(E))

plt.show()
"
"#
# x0 = [  0.00157, 0.0] 
x0 = [  0.0, 0.00546926689437855903172525] 
z,g = 0.8,9.81
dt,tmax = 0.01,7.5
n = int(tmax/dt)+1
t = np.linspace(0,tmax,n)
zmpFcn = lambda t: 0.3*stepFcn(t,1.5)-0.3*stepFcn(t,3.0)+0.3*stepFcn(t,4.5)-0.3*stepFcn(t,6.0)
pars = (g,z,zmpFcn)
# odeint solver
sol = odeint(dynLIPM, x0, t, args=(pars,))
E = (x0[1]**2-g/z*x0[0]**2)/2
if E<0: 
    xrev,vrev = np.sqrt(-2*z*E/g), np.nan
else: 
    xrev,vrev = np.nan, np.sqrt(2*E)
print('Energia: {0:.3f}, vtop: {0:.3f}, xrev: {0:.3f}: '.format(E,vrev,xrev))
plt.plot(t,sol[:,0],'o-',t,zmpFcn(t),'--'),plt.grid('on')
plt.title('Energia: {0:.3f}J'.format(E))
plt.axis((0,7.5,0.0,0.4))

plt.show()
"
"dt, tmax = 0.01, 3.0
n = int(tmax/dt+1)
t = np.linspace(0,tmax,n)
ai = -z/(g*dt**2)
bi = 2*z/(g*dt**2)+1
ci = ai
A = np.zeros((n,n))
for i in range(0,n):
    if i==0:
        A[0,0:2] = [bi,ci]
    elif i==n-1:
        A[n-1,n-2:n] = [ai,bi]
    else:
        A[i,i-1:i+2] = [ai,bi,ci]
        
zmpFcn = lambda t: 0.3*stepFcn(t,1.5)
print(ai,bi,ci)
# posicion
#p = zmpFcn(t)
#p[0] = p[0]-ai*0.3
#p[n-1] = p[n-1]-ci*0.3
# velocidad
p = zmpFcn(t)
p[0] += +ai*(0.0)*dt
p[n-1] += -ci*(0.0)*dt
A[0,0] += ai
A[n-1,n-1] += ci
x_com = scipy.linalg.solve_triangular(A,p)
plt.plot(t,zmpFcn(t),'--',t,x_com,'o-'),plt.grid('on')#
plt.axis([0,3.0,-0.1,0.4])

print(x_com[-1])"
"zc, g = (0.814,9.81)
xc = -0.0
xppc = xc*g/zc
x0 = np.array([xc,0.0,-xppc])
dt,tmax = 0.01,5
n = int(tmax/dt+1)
t = np.linspace(0,tmax,n)

A,B,a,w = (np.array([[0,1,0],[0,0,1],[0,0,0]]),
           np.array([[0,0,1]]).reshape((3,1)),
           [ 1.5],        # times
           [ 0.3])  # step-amplitud
#           [ 1.8, 2.6, 3.4, 4.2, 5.0],        # times
#           [ 0.1,-0.2, 0.2,-0.2, 0.1])  # step-amplitud

kp,kd = 10,40
yOutFcn = lambda x,t,pars: x[0]-pars[0]/pars[1]*x[2] 
pars = (kp,kd,w,a,zc,g,yOutFcn)
uFcn = lambda x,t,pars: \
    pars[0]*(arrayOfStepFcn(t,(pars[2],pars[3])) - pars[6](x,t,(pars[4],pars[5]))) +\
    pars[1]*(0 - x[1])

# odeint solver
sol = odeint(dynCartTable, x0, t, args=((A,B,uFcn,pars),))
y_zmp = sol[:,0]-zc/g*sol[:,2]
y_d = arrayOfStepFcn(t,(w,a))

plt.plot(t,y_d,t,y_zmp,t,sol[:,0])
#plt.grid('on'),plt.axis([0,tmax,-0.1,0.4])
plt.show()"
"# PI-controller
def dynSys(x,t,params):
    A,B,uFcn,pars = params
    xp = np.dot(A,x.reshape((2,1)))+B*uFcn(x,t,pars)
    return xp.reshape((2,))

def dynSysWithIAction(x,t,params):
    A,B,fun,pars = params
    kp,ki,kd = pars
    xp = np.zeros_like(x)
    xsys = x[0:2]
    e = (stepFcn(t)-xsys[0])*stepFcn(t,10)
    xp[0:2] = dynSys(xsys,t,(A,B,fun,(kp,kd)))+np.array([0,ki*x[2]])
    xp[2] = e
    return xp.reshape((3,))

t = np.linspace(0,20,10001)
x0 = np.array([0,0,0])
A,B = np.array([[0,1],[-1,-0.7]]),np.array([0.,1]).reshape((2,1))

fun = lambda x,t,pars: pars[0]*(stepFcn(t,0)-x[0])+pars[1]*(0-x[1])

kp,ki,kd = 10,10,1
pars = (kp,ki,kd)
# odeint solver
sol = odeint(dynSysWithIAction, x0, t, args=((A,B,fun,pars),))

plt.plot(t,sol[:,0]),plt.grid('on')
plt.show()"
"x = linspace(0, 100)
y = 4 * x + 5
plot(x, y, 'r')"
"x = linspace(0, 100)
y = 4 * x 
plot(x, y, 'r')"
"x = linspace(0, 100)
y = x 
plot(x, y, 'r')"
"x = linspace(0, 10)
y1 = 3 * (x ** 2)
y2 = x + 5
y3 = 2 ** x
plot(x, y1, 'r', label='$3n^2$')
plot(x, y2, 'b', label='$n+5$')
plot(x, y3, 'g', label='$2^n$')
legend()"
"x = linspace(0, 20)
yg = x ** 2
yf = 5 * x + 7
plot(x, yg, 'r', label='g(n)')
plot(x, yf, 'b', label='f(n)')
legend()"
"x = linspace(0, 20)
yg = 6 * (x ** 2)
yf = 5 * x + 7
plot(x, yg, 'r', label='g(n)')
plot(x, yf, 'b', label='f(n)')
legend()"
"x = linspace(1, 100)
x_const = linspace(1, 1)

fig, axes = plt.subplots(3, 4, figsize=(18, 10))

axes[0,0].plot(x, x_const)
axes[0,0].set_title(""$O(1)$"")

axes[0,1].plot(x, log(x))
axes[0,1].set_title(""$O(\log n)$"")

axes[0,2].plot(x, log(x) ** 2)
axes[0,2].set_title(""$O((log n)^c)$"")

axes[0,3].plot(x, x)
axes[0,3].set_title(""$O(n)$"")

axes[1,0].plot(x, x * log(x))
axes[1,0].set_title(""$O(n \log n)$"")

axes[1,1].plot(x, x ** 2)
axes[1,1].set_title(""$O(n^2)$"")

axes[1,2].plot(x, x ** 3)
axes[1,2].set_title(""$O(n^3)$"")

axes[1,3].plot(x, x ** 2)
axes[1,3].set_title(""$O(n^c)$"")

axes[2,0].plot(x, scipy.power(2, x))
axes[2,0].set_title(""$O(c ^ n)$"")

axes[2,1].plot(x, scipy.misc.factorial(x))
axes[2,1].set_title(""$O(n!)$"")

axes[2,2].plot(x, scipy.power(x,x))
axes[2,2].set_title(""$O(n^n)$"")"
"# Plotting

# create figure
fig, axes = plt.subplots(2, 1, sharex=True, figsize=(8, 8))

# plot histogram
ax = axes[0]
ax.hist(y, np.arange(-44, 43, 2))
# decorate
ax.set_title('Newcomb\'s measurements')
ax.set_ylabel('count')
ax.set_xlabel('$\mu$')
plt.setp(axes[0].get_xticklabels(), visible=True)

# plot the posterior of mu
ax = axes[1]
ax.plot(t1, pm_mu)
# plot the posterior of mu in the filtered case
ax.plot(t1, pm_mu_pos)
# Plot the currently accepted true value
ax.axvline(33, color='k', linestyle='--')
ax.legend(
    ('posterior of $\mu$',
     'posterior of $\mu$ given $y > 0$',
     '""true value""'),
    loc='upper left'
)
ax.set_title('Normal model')
ax.set_xlabel('$\mu$')
ax.set_yticks(())
# set bottom to zero
ax.set_ylim((0, ax.set_ylim()[1]))

fig.tight_layout()"
"print('Joint and marginal posterior distributions')

# create figure
fig, axes = plt.subplots(
    2, 2,
    figsize=(9, 9),
    gridspec_kw=dict(
        width_ratios=[2, 1],
        height_ratios=[1, 2]
    )
)

# plot the joint distribution
ax = axes[1, 0]
# plot the samples from the joint posterior
samps = ax.scatter(mu, sigma, 5, color=plot_tools.lighten('C2', 0.3))
# plot the contour plot of the exact posterior (c_levels is used to give
# a vector of linearly spaced values at which levels contours are drawn)
c_levels = np.linspace(1e-5, Z.max(), 6)[:-1]
ax.contour(t1, t2, Z, c_levels, colors='C0')
# decorate
ax.set_xlim(tl1)
ax.set_ylim(tl2)
ax.set_xlabel('$\mu$', fontsize='14')
ax.set_ylabel('$\sigma$', fontsize='14')

# plot the marginal of mu
ax = axes[0, 0]
# exact
ax.plot(
    t1,
    pm_mu,
    color=plot_tools.lighten('C1'),
    linewidth=5
)
# empirical
ax.plot(
    t1,
    pk_mu,
    color='k',
    dashes=(5, 7),
    linewidth=1.5
)
# decorate
ax.set_xlim(tl1)
ax.set_yticks(())
ax.set_xticklabels(())

# plot the marginal of sigma
ax = axes[1, 1]
# exact
exact_line, = ax.plot(
    pm_sigma,
    t2,
    color=plot_tools.lighten('C1'),
    linewidth=5
)
# empirical
empirical_line, = ax.plot(
    pk_sigma,
    t2,
    color='k',
    dashes=(5, 7),
    linewidth=1.5
)
# decorate
ax.set_ylim(tl2)
ax.set_xticks(())
ax.set_yticklabels(())

# hide last ax
ax = axes[0, 1]
ax.axis('off')
# make legend there
ax.legend(
    (
        samps,
        plt.Line2D([], [], color='C0'),
        exact_line,
        empirical_line
    ),
    (
        'samples',
        'exact joint contour',
        'exact marginal',
        'empirical marginal'
    ),
    loc='center',
)

fig.tight_layout()"
"# create figure
fig, axes = plt.subplots(
    1, 2,
    figsize=(9, 6),
    gridspec_kw=dict(width_ratios=[2, 1])
)

# plot the joint distribution
ax = axes[0]
# plot the contour plot of the exact posterior (c_levels is used to give
# a vector of linearly spaced values at which levels contours are drawn)
c_levels = np.linspace(1e-5, Z.max(), 6)[:-1]
ax.contour(t1, t2, Z, c_levels, colors='C0')

# plot the first sample
line1, = ax.plot(tl1, [sigma[0], sigma[0]], 'k')
line2, = ax.plot(
    t1,
    sigma[0] + stats.norm.pdf(t1, my, np.sqrt(sigma2[0]/n))*100,
    color='C1',
    linestyle='dashed',
    linewidth=2
)
scat = ax.scatter(mu[0], sigma[0], 40, color='r')

# decorate
ax.set_xlim(tl1)
ax.set_ylim(tl2)
ax.set_xlabel('$\mu$', fontsize=14)
ax.set_ylabel('$\sigma$', fontsize=14)
ax.set_title('joint posterior')
ax.legend(
    (plt.Line2D([], [], color='C0'), line1, line2, scat),
    ('exact contour plot',
     'sample from the marginal of $\\sigma$',
     'conditional distribution of $\\mu$',
     'sample from joint posterior'),
    loc='upper center'
)

# plot the marginal of sigma
ax = axes[1]
ax.plot(pm_sigma, t2)
# decorate
ax.set_ylim(tl2)
ax.set_title('marginal of $\sigma$')
ax.set_xticks(());
ax.set_yticklabels(())

fig.tight_layout()"
"# calculate conditional pdfs for each sample
condpdfs = stats.norm.pdf(t1, my, np.sqrt(sigma2/n)[:,np.newaxis])

# create figure
fig, axes = plt.subplots(2, 1, figsize=(6, 9), sharex=True)

# plot some of them
ax = axes[0]
ax.plot(t1, condpdfs[:25].T, 'C2', alpha=0.25)
ax.set_xlim(tl1)
ylims_from_0 = ax.set_ylim()  # set same y_lims to the next plot
ax.set_title('conditional distribution of $\\mu$ for first 25 samples')
ax.set_yticks(())

# plot their mean
ax = axes[1]
ax.plot(
    t1,
    np.mean(condpdfs, axis=0),
    color=plot_tools.lighten('C1'),
    linewidth=5,
    label='average of sampled conditionals'
)
ax.plot(
    t1,
    pm_mu,
    color='k',
    dashes=(5, 7),
    linewidth=1.5,
    label='exact marginal of $\\mu$'
)
ax.set_yticks(())
ax.set_xlim(tl1)
ax.set_ylim(ylims_from_0)
ax.legend(loc='upper center')

fig.tight_layout()"
"# calculate predictive pdf for the first given mu and sigma sample
ynewdists = stats.norm.pdf(xynew, mu[0], sigma[0])

# create figure
fig, axes = plt.subplots(3, 1, figsize=(6, 12))

# plot the joint distribution
ax = axes[0]
# plot the samples from the joint posterior
samps = ax.scatter(mu, sigma, 5, color=plot_tools.lighten('C2', 0.3))
# plot the contour plot of the exact posterior (c_levels is used to give
# a vector of linearly spaced values at which levels contours are drawn)
c_levels = np.linspace(1e-5, Z.max(), 6)[:-1]
ax.contour(t1, t2, Z, c_levels, colors='C0')
# decorate
ax.set_xlim(tl1)
ax.set_ylim(tl2)
ax.set_xlabel('$\mu$', fontsize=14)
ax.set_ylabel('$\sigma$', fontsize=14)
ax.legend(
    (plt.Line2D([], [], color='C0'), samps),
    ('exact joint posterior contour', 'joint posterior samples'),
    loc='center left',
    bbox_to_anchor=(1, 0.5)
)
# highlight the first sample
ax.scatter(mu[0], sigma[0], 40, 'r')

# plot first ynew
ax = axes[1]
# plot the distribution and the respective sample
line1, = ax.plot(xynew, ynewdists)
ax1_hs = ax.scatter(ynew[0], 0.02*np.max(ynewdists), 40, 'r')
# decorate
ylims_from_1 = ax.set_ylim()
ax.set_xlim(tlynew)
ax.set_xlabel('$\widetilde{y}$', fontsize=14)
ax.set_yticks(())
ax.legend(
    (line1, ax1_hs),
    ('pred.dist. given the posterior sample',
    'sample from the predictive distribution'),
    loc='center left',
    bbox_to_anchor=(1, 0.5)
)

# plot all ynews
ax = axes[2]
ax.scatter(
    ynew,
    (0.02 + 0.05*rng.rand(*ynew.shape))*np.max(ynewdists),
    10,
    color='C2',
    alpha=0.2,
    label='samples from the predictive distribution'
)
ax.plot(
    xynew,
    p_new,
    linewidth=1.5,
    label='exact predictive distribution'
)
ax.set_ylim(ylims_from_1)
ax.set_xlim(tlynew)
ax.set_xlabel('$\widetilde{y}$', fontsize=14)
ax.set_yticks(())
ax.legend(loc='center left', bbox_to_anchor=(1, 0.5))

fig.tight_layout()"
"# Plot the probabilities, using 'seats' as the index as a bar chart
pd.Series(aisle_probability, index=seats).plot(kind='bar', # set y to range between 0 and 1
                                                    ylim=[0,1],
                                                    # set the figure size
                                                    figsize=[10,6],
                                                    # set the figure title
                                                    title='Probabilty of being an Aisle Seat in Economy Class')"
"# train['species'] contains the actual species names. Before we can use it,
# we need to convert each species name into a digit. So, in this case there
# are three species, which have been coded as 0, 1, or 2.
y = pd.factorize(train['species'])[0]

# View target
y"
"# Create a random forest Classifier. By convention, clf means 'Classifier'
clf = RandomForestClassifier(n_jobs=2, random_state=0)

# Train the Classifier to take the training features and learn how they relate
# to the training y (the species)
clf.fit(train[features], y)"
"# Apply the Classifier we trained to the test data (which, remember, it has never seen before)
clf.predict(test[features])"
"# View the predicted probabilities of the first 10 observations
clf.predict_proba(test[features])[0:10]"
"# View the PREDICTED species for the first five observations
preds[0:5]"
"seaborn.lmplot('test_1', 'test_2', data=training_data, fit_reg=False,hue=""outcome"", scatter_kws={""marker"": ""D"",""s"": 100})"
"trained_model.score(X, y)"
trained_model.predict_proba(x_test)
"data = {'name': ['Jason', 'Molly', 'Tina', 'Jake', 'Amy'], 
        'year': [2012, 2012, 2013, 2014, 2014], 
        'reports': [4, 24, 31, 2, 3]}
df = pd.DataFrame(data, index = ['Cochice', 'Pima', 'Santa Cruz', 'Maricopa', 'Yuma'])
df"
"df.drop(['Cochice', 'Pima'])"
df[df.name != 'Tina']
df.drop(df.index[2])
"df.drop(df.index[[2,3]])"
df.drop(df.index[-2])
df[:3] #keep top 3
df[:-3] #drop bottom 3 
"data = {'name': ['Jason', 'Molly', 'Tina', 'Jake', 'Amy'], 
        'year': [2012, 2012, 2013, 2014, 2014], 
        'reports': [1, 2, 1, 2, 3],
        'coverage': [2, 2, 3, 3, 3]}
df = pd.DataFrame(data, index = ['Cochice', 'Pima', 'Santa Cruz', 'Maricopa', 'Yuma'])
df"
"df.sort_values(by='reports', ascending=0)"
"df.sort_values(by=['coverage', 'reports'])"
"# Create a figure
plt.figure(figsize=(10,8))

# Create a scatterplot of,
            # attacker size in year 298 as the x axis
plt.scatter(df['attacker_size'][df['year'] == 298], 
            # attacker size in year 298 as the y axis
            df['defender_size'][df['year'] == 298], 
            # the marker as
            marker='x', 
            # the color
            color='b',
            # the alpha
            alpha=0.7,
            # with size
            s = 124,
            # labelled this
            label='Year 298')
            
            # attacker size in year 299 as the x axis
plt.scatter(df['attacker_size'][df['year'] == 299], 
            # defender size in year 299 as the y axis
            df['defender_size'][df['year'] == 299], 
            # the marker as
            marker='o', 
            # the color
            color='r', 
            # the alpha
            alpha=0.7,
            # with size
            s = 124,
            # labelled this
            label='Year 299')

            # attacker size in year 300 as the x axis
plt.scatter(df['attacker_size'][df['year'] == 300], 
            # defender size in year 300 as the y axis
            df['defender_size'][df['year'] == 300], 
            # the marker as
            marker='^', 
            # the color
            color='g', 
            # the alpha
            alpha=0.7, 
            # with size
            s = 124,
            # labelled this
            label='Year 300')

# Chart title
plt.title('Battles Of The War Of The Five Kings')

# y label
plt.ylabel('Defender Size')

# x label
plt.xlabel('Attacker Size')

# and a legend
plt.legend(loc='upper right')

# set the figure boundaries
plt.xlim([min(df['attacker_size'])-1000, max(df['attacker_size'])+1000])
plt.ylim([min(df['defender_size'])-1000, max(df['defender_size'])+1000])

plt.show()"
"data = {'name': ['Jason', 'Molly', 'Tina', 'Jake', 'Amy'], 
        'year': [2012, 2012, 2013, 2014, 2014], 
        'reports': [4, 24, 31, 2, 3]}
df = pd.DataFrame(data, index = ['Cochice', 'Pima', 'Santa Cruz', 'Maricopa', 'Yuma'])
df"
"# Create a variable
next_year = []

# For each row in df.years,
for row in df['year']:
    # Add 1 to the row and append it to next_year
    next_year.append(row + 1)

# Create df.next_year
df['next_year'] = next_year

# View the dataframe
df"
df
"# Create an example dataframe
data = {'name': ['Jason', 'Molly', 'Tina', 'Jake', 'Amy'], 
        'year': [2012, 2012, 2013, 2014, 2014], 
        'reports': [4, 24, 31, 2, 3]}
df = pd.DataFrame(data, index = ['Cochice', 'Pima', 'Santa Cruz', 'Maricopa', 'Yuma'])
df"
"# Create a list of colors (from iWantHue)
colors = [""#E13F29"", ""#D69A80"", ""#D63B59"", ""#AE5552"", ""#CB5C3B"", ""#EB8076"", ""#96624E""]

# Create a pie chart
plt.pie(
    # using data total)arrests
    df['total_arrests'],
    # with the labels being officer names
    labels=df['officer_name'],
    # with no shadows
    shadow=False,
    # with colors
    colors=colors,
    # with one slide exploded out
    explode=(0, 0, 0, 0, 0.15),
    # with the start angle at 90%
    startangle=90,
    # with the percent listed as a fraction
    autopct='%1.1f%%',
    )

# View the plot drop above
plt.axis('equal')

# View the plot
plt.tight_layout()
plt.show()"
"plt.scatter(df.preTestScore, df.postTestScore
, s=df.age)"
"plt.scatter(df.preTestScore, df.postTestScore, s=300, c=df.female)"
"pyplot.plot([1.6, 2.7])"
"# Create a list of unique values by turning the
# pandas column into a set
list(set(df.trucks))"
"# Create a figure with a single subplot
f, ax = plt.subplots(1, figsize=(10,5))

# Set bar width at 1
bar_width = 1

# positions of the left bar-boundaries
bar_l = [i for i in range(len(df['pre_score']))] 

# positions of the x-axis ticks (center of the bars as bar labels)
tick_pos = [i+(bar_width/2) for i in bar_l] 

# Create the total score for each participant
totals = [i+j+k for i,j,k in zip(df['pre_score'], df['mid_score'], df['post_score'])]

# Create the percentage of the total score the pre_score value for each participant was
pre_rel = [i / j * 100 for  i,j in zip(df['pre_score'], totals)]

# Create the percentage of the total score the mid_score value for each participant was
mid_rel = [i / j * 100 for  i,j in zip(df['mid_score'], totals)]

# Create the percentage of the total score the post_score value for each participant was
post_rel = [i / j * 100 for  i,j in zip(df['post_score'], totals)]

# Create a bar chart in position bar_1
ax.bar(bar_l, 
       # using pre_rel data
       pre_rel, 
       # labeled 
       label='Pre Score', 
       # with alpha
       alpha=0.9, 
       # with color
       color='#019600',
       # with bar width
       width=bar_width,
       # with border color
       edgecolor='white'
       )

# Create a bar chart in position bar_1
ax.bar(bar_l, 
       # using mid_rel data
       mid_rel, 
       # with pre_rel
       bottom=pre_rel, 
       # labeled 
       label='Mid Score', 
       # with alpha
       alpha=0.9, 
       # with color
       color='#3C5F5A', 
       # with bar width
       width=bar_width,
       # with border color
       edgecolor='white'
       )

# Create a bar chart in position bar_1
ax.bar(bar_l, 
       # using post_rel data
       post_rel, 
       # with pre_rel and mid_rel on bottom
       bottom=[i+j for i,j in zip(pre_rel, mid_rel)], 
       # labeled 
       label='Post Score',
       # with alpha
       alpha=0.9, 
       # with color
       color='#219AD8', 
       # with bar width
       width=bar_width,
       # with border color
       edgecolor='white'
       )

# Set the ticks to be first names
plt.xticks(tick_pos, df['first_name'])
ax.set_ylabel(""Percentage"")
ax.set_xlabel("""")

# Let the borders of the graphic
plt.xlim([min(tick_pos)-bar_width, max(tick_pos)+bar_width])
plt.ylim(-10, 110)

# rotate axis labels
plt.setp(plt.gca().get_xticklabels(), rotation=45, horizontalalignment='right')

# shot plot
plt.show()"
"# Create dataframe
data = {'name': ['Jason', 'Molly', 'Tina', 'Jake', 'Amy'], 
        'year': [2012, 2012, 2013, 2014, 2014], 
        'reports': [4, 24, 31, 2, 3],
        'coverage': [25, 94, 57, 62, 70]}
df = pd.DataFrame(data, index = ['Cochice', 'Pima', 'Santa Cruz', 'Maricopa', 'Yuma'])
df"
"# Create a new column that is the rank of the value of coverage in ascending order
df['coverageRanked'] = df['coverage'].rank(ascending=1)
df"
"sns.tsplot([df.deaths_regiment_1, df.deaths_regiment_2, df.deaths_regiment_3, df.deaths_regiment_4,
            df.deaths_regiment_5, df.deaths_regiment_6, df.deaths_regiment_7], color=""indianred"")"
"sns.tsplot([df.deaths_regiment_1, df.deaths_regiment_2, df.deaths_regiment_3, df.deaths_regiment_4,
            df.deaths_regiment_5, df.deaths_regiment_6, df.deaths_regiment_7], err_style=""ci_bars"", interpolate=False)"
"# Create the general blog and the ""subplots"" i.e. the bars
f, ax1 = plt.subplots(1, figsize=(10,5))

# Set the bar width
bar_width = 0.75

# positions of the left bar-boundaries
bar_l = [i+1 for i in range(len(df['pre_score']))] 

# positions of the x-axis ticks (center of the bars as bar labels)
tick_pos = [i+(bar_width/2) for i in bar_l] 

# Create a bar plot, in position bar_1
ax1.bar(bar_l, 
        # using the pre_score data
        df['pre_score'], 
        # set the width
        width=bar_width,
        # with the label pre score
        label='Pre Score', 
        # with alpha 0.5
        alpha=0.5, 
        # with color
        color='#F4561D')

# Create a bar plot, in position bar_1
ax1.bar(bar_l, 
        # using the mid_score data
        df['mid_score'], 
        # set the width
        width=bar_width,
        # with pre_score on the bottom
        bottom=df['pre_score'], 
        # with the label mid score
        label='Mid Score', 
        # with alpha 0.5
        alpha=0.5, 
        # with color
        color='#F1911E')

# Create a bar plot, in position bar_1
ax1.bar(bar_l, 
        # using the post_score data
        df['post_score'], 
        # set the width
        width=bar_width,
        # with pre_score and mid_score on the bottom
        bottom=[i+j for i,j in zip(df['pre_score'],df['mid_score'])], 
        # with the label post score
        label='Post Score', 
        # with alpha 0.5
        alpha=0.5, 
        # with color
        color='#F1BD1A')

# set the x ticks with names
plt.xticks(tick_pos, df['first_name'])

# Set the label and legends
ax1.set_ylabel(""Total Score"")
ax1.set_xlabel(""Test Subject"")
plt.legend(loc='upper left')

# Set a buffer around the edge
plt.xlim([min(tick_pos)-bar_width, max(tick_pos)+bar_width])"
df.head()
"sns.lmplot('x', 'y', data=df, fit_reg=False)"
sns.kdeplot(df.y)
"sns.kdeplot(df.y, df.x)"
sns.distplot(df.x)
"plt.hist(df.x, alpha=.3)
sns.rugplot(df.x);"
"sns.boxplot([df.y, df.x])"
"sns.violinplot([df.y, df.x])"
"sns.heatmap([df.y, df.x], annot=True, fmt=""d"")"
sns.clustermap(df)
"# Create an example dataframe
data = {'NAME': ['Jason', 'Molly', 'Tina', 'Jake', 'Amy'], 
        'YEAR': [2012, 2012, 2013, 2014, 2014], 
        'REPORTS': [4, 24, 31, 2, 3]}
df = pd.DataFrame(data, index = ['Cochice', 'Pima', 'Santa Cruz', 'Maricopa', 'Yuma'])
df"
df
"# Create an example dataframe
data = {'name': ['Jason', 'Molly', 'Tina', 'Jake', 'Amy'], 
        'year': [2012, 2012, 2013, 2014, 2014], 
        'reports': [4, 24, 31, 2, 3]}
df = pd.DataFrame(data, index = ['Cochice', 'Pima', 'Santa Cruz', 'Maricopa', 'Yuma'])
df"
"#Grab DataFrame rows where column has certain values
df[df.name.isin(value_list)]"
"#Grab DataFrame rows where column doesn't have certain values
df[~df.name.isin(value_list)]"
df.resample('D').sum().plot()
"# Create a dataframe
data = {'county': ['Cochice', 'Pima', 'Santa Cruz', 'Maricopa', 'Yuma'], 
        'year': [2012, 2012, 2013, 2014, 2014], 
        'reports': [4, 24, 31, 2, 3]}
df = pd.DataFrame(data)
df"
"# Change the order (the index) of the rows
df.reindex([4, 3, 2, 1, 0])"
"# Print the code to create the dataframe
print('==============================')
print('RUN THE CODE BELOW THIS LINE')
print('==============================')
print('raw_data =', df.to_dict(orient='list'))
print('df = pd.DataFrame(raw_data, columns = ' + str(list(df_original)) + ')')"
"sns.palplot(sns.color_palette(""deep"", 10))"
"sns.palplot(sns.color_palette(""muted"", 10))"
"sns.palplot(sns.color_palette(""bright"", 10))"
"sns.palplot(sns.color_palette(""dark"", 10))"
"sns.palplot(sns.color_palette(""colorblind"", 10))"
"sns.palplot(sns.color_palette(""Paired"", 10))"
"sns.palplot(sns.color_palette(""BuGn"", 10))"
"sns.palplot(sns.color_palette(""GnBu"", 10))"
"sns.palplot(sns.color_palette(""OrRd"", 10))"
"sns.palplot(sns.color_palette(""PuBu"", 10))"
"sns.palplot(sns.color_palette(""YlGn"", 10))"
"sns.palplot(sns.color_palette(""YlGnBu"", 10))"
"sns.palplot(sns.color_palette(""YlOrBr"", 10))"
"sns.palplot(sns.color_palette(""YlOrRd"", 10))"
"sns.palplot(sns.color_palette(""BrBG"", 10))"
"sns.palplot(sns.color_palette(""PiYG"", 10))"
"sns.palplot(sns.color_palette(""PRGn"", 10))"
"sns.palplot(sns.color_palette(""PuOr"", 10))"
"sns.palplot(sns.color_palette(""RdBu"", 10))"
"sns.palplot(sns.color_palette(""RdGy"", 10))"
"sns.palplot(sns.color_palette(""RdYlBu"", 10))"
"sns.palplot(sns.color_palette(""RdYlGn"", 10))"
"sns.palplot(sns.color_palette(""Spectral"", 10))"
"flatui = [""#9b59b6"", ""#3498db"", ""#95a5a6"", ""#e74c3c"", ""#34495e"", ""#2ecc71""]
sns.set_palette(flatui)
sns.palplot(sns.color_palette())"
"sns.tsplot([df.deaths_regiment_1, df.deaths_regiment_2, df.deaths_regiment_3, df.deaths_regiment_4,
            df.deaths_regiment_5, df.deaths_regiment_6, df.deaths_regiment_7], color=""#34495e"")"
"# Setting the positions and width for the bars
pos = list(range(len(df['pre_score']))) 
width = 0.25 
    
# Plotting the bars
fig, ax = plt.subplots(figsize=(10,5))

# Create a bar with pre_score data,
# in position pos,
plt.bar(pos, 
        #using df['pre_score'] data,
        df['pre_score'], 
        # of width
        width, 
        # with alpha 0.5
        alpha=0.5, 
        # with color
        color='#EE3224', 
        # with label the first value in first_name
        label=df['first_name'][0]) 

# Create a bar with mid_score data,
# in position pos + some width buffer,
plt.bar([p + width for p in pos], 
        #using df['mid_score'] data,
        df['mid_score'],
        # of width
        width, 
        # with alpha 0.5
        alpha=0.5, 
        # with color
        color='#F78F1E', 
        # with label the second value in first_name
        label=df['first_name'][1]) 

# Create a bar with post_score data,
# in position pos + some width buffer,
plt.bar([p + width*2 for p in pos], 
        #using df['post_score'] data,
        df['post_score'], 
        # of width
        width, 
        # with alpha 0.5
        alpha=0.5, 
        # with color
        color='#FFC222', 
        # with label the third value in first_name
        label=df['first_name'][2]) 

# Set the y axis label
ax.set_ylabel('Score')

# Set the chart's title
ax.set_title('Test Subject Scores')

# Set the position of the x ticks
ax.set_xticks([p + 1.5 * width for p in pos])

# Set the labels for the x ticks
ax.set_xticklabels(df['first_name'])

# Setting the x-axis and y-axis limits
plt.xlim(min(pos)-width, max(pos)+width*4)
plt.ylim([0, max(df['pre_score'] + df['mid_score'] + df['post_score'])] )

# Adding the legend and showing the plot
plt.legend(['Pre Score', 'Mid Score', 'Post Score'], loc='upper left')
plt.grid()
plt.show()"
"# Create a list of the mean scores for each variable
mean_values = [df['pre_score'].mean(), df['mid_score'].mean(), df['post_score'].mean()]

# Create a list of variances, which are set at .25 above and below the score
variance = [df['pre_score'].mean() * 0.25, df['pre_score'].mean() * 0.25, df['pre_score'].mean() * 0.25]

# Set the bar labels
bar_labels = ['Pre Score', 'Mid Score', 'Post Score']

# Create the x position of the bars
x_pos = list(range(len(bar_labels)))

# Create the plot bars
# In x position
plt.bar(x_pos,
        # using the data from the mean_values
        mean_values, 
        # with a y-error lines set at variance
        yerr=variance, 
        # aligned in the center
        align='center',
        # with color
        color='#FFC222',
        # alpha 0.5
        alpha=0.5)

# add a grid
plt.grid()

# set height of the y-axis
max_y = max(zip(mean_values, variance)) # returns a tuple, here: (3, 5)
plt.ylim([0, (max_y[0] + max_y[1]) * 1.1])

# set axes labels and title
plt.ylabel('Score')
plt.xticks(x_pos, bar_labels)
plt.title('Mean Scores For Each Test')

plt.show()"
"# Let's take a took at the data
df.head()"
"# Group the data by month, and take the mean for each group (i.e. each month)
df.resample('M').mean()"
"# Group the data by month, and take the sum for each group (i.e. each month)
df.resample('M').sum()"
"# View the index
df.index"
"# Summarize the results by regiment
df.sum(level='regiment')"
"# Create a groupby variable that groups preTestScores by regiment
groupby_regiment = df['preTestScore'].groupby(df['regiment'])
groupby_regiment"
df['postTestScore'].groupby(df['categories']).apply(get_stats).unstack()
"# Create a new variable called 'header' from the first row of the dataset
header = df.iloc[0]"
"df = pd.DataFrame()

df['german_army'] = np.random.randint(low=20000, high=30000, size=100)
df['allied_army'] = np.random.randint(low=20000, high=40000, size=100)
df.index = pd.date_range('1/1/2014', periods=100, freq='H')

df.head()"
"df.truncate(before='1/2/2014', after='1/3/2014')"
df.head()
df.shift(1).head()
df.shift(-1).tail()
df.resample('D').sum()
df.resample('D').mean()
df.resample('D').median()
df.resample('D').median()
df.resample('D').first()
df.resample('D').last()
df.resample('D').ohlc()
"matches = df['email'].str.match(pattern, flags=re.IGNORECASE)
matches"
matches.str[1]
"c = plt.scatter(theta, r, c=colors, s=area, cmap=plt.cm.RdYlGn)"
"c1 = plt.scatter(theta, r, c=colors, s=area, cmap=plt.cm.Blues)"
"c2 = plt.scatter(theta, r, c=colors, s=area, cmap=plt.cm.BrBG)"
"c3 = plt.scatter(theta, r, c=colors, s=area, cmap=plt.cm.Greens)"
"c4 = plt.scatter(theta, r, c=colors, s=area, cmap=plt.cm.RdGy)"
"c5 = plt.scatter(theta, r, c=colors, s=area, cmap=plt.cm.YlOrRd)"
"c6 = plt.scatter(theta, r, c=colors, s=area, cmap=plt.cm.autumn)"
"c7 = plt.scatter(theta, r, c=colors, s=area, cmap=plt.cm.binary)"
"c8 = plt.scatter(theta, r, c=colors, s=area, cmap=plt.cm.gist_earth)"
"c9 = plt.scatter(theta, r, c=colors, s=area, cmap=plt.cm.gist_heat)"
"c10 = plt.scatter(theta, r, c=colors, s=area, cmap=plt.cm.hot)"
"c11 = plt.scatter(theta, r, c=colors, s=area, cmap=plt.cm.spring)"
"c12 = plt.scatter(theta, r, c=colors, s=area, cmap=plt.cm.summer)"
"c12 = plt.scatter(theta, r, c=colors, s=area, cmap=plt.cm.winter)"
"c13 = plt.scatter(theta, r, c=colors, s=area, cmap=plt.cm.bone)"
"c14 = plt.scatter(theta, r, c=colors, s=area, cmap=plt.cm.cool)"
"c15 = plt.scatter(theta, r, c=colors, s=area, cmap=plt.cm.YlGn)"
"c16 = plt.scatter(theta, r, c=colors, s=area, cmap=plt.cm.RdBu)"
"c17 = plt.scatter(theta, r, c=colors, s=area, cmap=plt.cm.PuOr)"
"c18 = plt.scatter(theta, r, c=colors, s=area, cmap=plt.cm.Oranges)"
"data = {'name': ['Jason', 'Molly', 'Tina', 'Jake', 'Amy'], 
        'year': [2012, 2012, 2013, 2014, 2014], 
        'reports': [4, 24, 31, 2, 3],
        'coverage': [25, 94, 57, 62, 70]}
df = pd.DataFrame(data, index = ['Cochice', 'Pima', 'Santa Cruz', 'Maricopa', 'Yuma'])
df"
"# Drop the string variable so that applymap() can run
df = df.drop('name', axis=1)

# Return the square root of every cell in the dataframe
df.applymap(np.sqrt)"
df.applymap(times100)
"print('year 1 battles:',  year_1[0:3])
print('year 2 battles:', year_2[0:3])
print('year 3 battles:', year_3[0:3])"
centroids
variance
identified
distance
"print(cluster_1[0:3])
print(cluster_2[0:3])
print(cluster_3[0:3])"
"# create a scatter plot there the x-axis is the first column of battles
# the y-axis is the second column of battles, the size is 100, and
# the color of each point is determined by the indentified variable
plt.scatter(battles[:,0], battles[:,1], s=100, c=identified)"
df.take(np.random.permutation(len(df))[:2])
"# Join the dummy variables to the main dataframe
df_new = pd.concat([df, df_sex], axis=1)
df_new"
"# Alterative for joining the new columns
df_new = df.join(df_sex)
df_new"
"# Create an example dataframe
data = {'Platoon': ['A','A','A','A','A','A','B','B','B','B','B','C','C','C','C','C'],
       'Casualties': [1,4,5,7,5,5,6,1,4,5,6,7,4,6,4,6]}
df = pd.DataFrame(data)
df"
"# Group df by df.platoon, then apply a rolling mean lambda function to df.casualties
df.groupby('Platoon')['Casualties'].apply(lambda x:x.rolling(center=False,window=2).mean())"
"data = {'name': ['Jason', 'Molly', 'Tina', 'Jake', 'Amy'], 
        'age': [42, 52, 36, 24, 73], 
        'preTestScore': [4, 24, 31, 2, 3],
        'postTestScore': [25, 94, 57, 62, 70]}
df = pd.DataFrame(data, columns = ['name', 'age', 'preTestScore', 'postTestScore'])
df"
df['preTestScore'].mean()
df['preTestScore'].var()
df['preTestScore'].skew()
df['preTestScore'].kurt()
df.corr()
df.cov()
"# Create an example dataframe
data = {'name': ['Jason', 'Molly'], 
        'country': [['Syria', 'Lebanon'],['Spain', 'Morocco']]}
df = pd.DataFrame(data)
df"
df[df['country'].map(lambda country: 'Syria' in country)]
"data = {'county': ['Cochice', 'Pima', 'Santa Cruz', 'Maricopa', 'Yuma'], 
        'year': [2012, 2012, 2013, 2014, 2014], 
        'reports': [4, 24, 31, 2, 3]}
df = pd.DataFrame(data)
df"
df
"
df.reset_index()"
"import warnings
warnings.filterwarnings('ignore')
from urllib.request import urlopen

url = 'http://jakevdp.github.com/downloads/notebooks/XKCD_plots.ipynb'
response = urlopen(url).read().decode()
response[0:60] + ' ...'"
print(body[:400] + '...')
"print(""Resources:"", resources.keys())
print(""Metadata:"", resources['metadata'].keys())
print(""Inlining:"", resources['inlining'].keys())
print(""Extension:"", resources['output_extension'])"
"# Import the RST exproter
from nbconvert import RSTExporter
# Instantiate it
rst_exporter = RSTExporter()
# Convert the notebook to RST format
(body, resources) = rst_exporter.from_notebook_node(jake_notebook)

print(body[:970] + '...')
print('[.....]')
print(body[800:1200] + '...')"
"from IPython.display import Image
Image(data=resources['outputs']['output_3_0.png'], format='png')"
"(_, resources)          = html_exporter.from_notebook_node(jake_notebook)
(_, resources_with_fig) = html_exporter_with_figs.from_notebook_node(jake_notebook)

print(""resources without figures:"")
print(sorted(resources.keys()))

print(""\nresources with extracted figures (notice that there's one more field called 'outputs'):"")
print(sorted(resources_with_fig.keys()))

print(""\nthe actual figures are:"")
print(sorted(resources_with_fig['outputs'].keys()))"
"# Create a new config object that configures both the new preprocessor, as well as the exporter
c =  Config()
c.PelicanSubCell.start = 4
c.PelicanSubCell.end = 6
c.RSTExporter.preprocessors = [PelicanSubCell]

# Create our new, customized exporter that uses our custom preprocessor
pelican = RSTExporter(config=c)

# Process the notebook
print(pelican.from_notebook_node(jake_notebook)[0])"
"import warnings
warnings.filterwarnings('ignore')
import time

time.time()"
"import warnings
warnings.filterwarnings('ignore')
%matplotlib inline

import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
np.seterr(invalid='ignore') # Workaround"
"female_age_mean = round(df[df.Sex=='female']['Age'].mean())
male_age_mean = round(df[df.Sex=='male']['Age'].mean())

print('女性の平均年齢は{0}歳、男性は{1}歳です。この平均年齢で補間します。'.format(female_age_mean, male_age_mean))"
round(df[df.Sex=='male']['Age'].mean())
"filled_df['Survived'].plot(alpha=0.6, kind='hist', bins=2)
plt.xlabel('Survived')
plt.ylabel('N')"
"fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(8, 4))

for i, sex in enumerate(['male', 'female']):
    filled_df['Survived'][filled_df.Sex==sex].hist(alpha=0.5, bins=2, ax=axes[i])
    axes[i].set_title(sex)

fig.subplots_adjust(hspace=0.3)
fig.tight_layout()"
"plt.hist([filled_df[(filled_df.Survived==0) & (filled_df.Sex=='male')]['Age'], filled_df[(filled_df.Survived==1) & (filled_df.Sex=='male')]['Age']],
          alpha=0.6, range=(1,80), bins=10, stacked=True,
          label=('Died', 'Survived'))
plt.legend()
plt.xlabel('Age')
plt.ylabel('N')
plt.title('male')"
"plt.hist([filled_df[(filled_df.Survived==0) & (filled_df.Sex=='female')]['Age'],
          filled_df[(filled_df.Survived==1) & (filled_df.Sex=='female')]['Age']],
          alpha=0.6, range=(1,80), bins=10, stacked=True,
          label=('Died', 'Survived'))
plt.legend()
plt.xlabel('Age')
plt.ylabel('N')
plt.title('female')"
"fig = plt.figure(figsize=[15, 5])

ax1 = fig.add_subplot(121)

plt.hist([filled_df[(filled_df.Survived==0) & (filled_df.Sex=='female')]['Age'],
          filled_df[(filled_df.Survived==1) & (filled_df.Sex=='female')]['Age']],
          alpha=0.6, range=(1,80), bins=10, stacked=True,
          label=('Died', 'Survived'))

plt.xlabel('Age')
plt.yticks([0, 40, 80, 120])
plt.ylabel('N')
plt.title('female')
plt.legend()

ax2 = fig.add_subplot(122)

plt.hist([filled_df[(filled_df.Survived==0) & (filled_df.Sex=='male')]['Age'],
          filled_df[(filled_df.Survived==1) & (filled_df.Sex=='male')]['Age']],
          alpha=0.6, range=(1,80), bins=10, stacked=True,
          label=('Died', 'Survived'))

plt.xlabel('Age')
plt.yticks([0, 40, 80, 120])
plt.ylabel('N')
plt.title('male')
plt.legend()

plt.show()"
"mean_age = df['Age'].mean()

for pclass in [1, 2, 3]:
    fig, axes = plt.subplots(nrows=2, ncols=2, figsize=[10, 10])

    sex_n=0
    for sex in ['male', 'female']:
        for survived in [0, 1]:
                fig = filled_df[((filled_df.Survived==survived) & (filled_df.Sex==sex) & (filled_df.Pclass==pclass) )].Age.hist(alpha=0.6, bins=10, ax=axes[sex_n][survived])
                fig.set_xlabel(""Age"")    
                fig.set_ylabel('N ('+sex+str(survived)+' )')  
                axes[sex_n][survived].set_ylim(0,70)
                fig.set_title('Pclass = {0} / mean_age = {1}'.format(pclass, round(mean_age)))
                
        sex_n += 1
    plt.subplots_adjust(hspace=0.5)
    plt.show()"
YouTubeVideo('w16iUU6IA5E')
YouTubeVideo('UQhOyZXHkxI')
YouTubeVideo('Pul4cqoPLbE')
"# Calculate pi by calculating area of quarter circle

def estimate_pi(n_attempts = 10000):
    """""" Estimate pi from area of quarter circle """""" 
    count_successes = 0
    for i in range(n_attempts):
        x_rand = np.random.random()
        y_rand = np.random.random()
        dist = math.sqrt( x_rand **2 + y_rand**2 )
        if dist <= 1.:
            count_successes += 1
    return float(count_successes) / n_attempts


print( math.pi / 4. )
print( estimate_pi(10) )"
"print( math.pi / 4. )
print( estimate_pi(100000) )"
"import numpy as np

def prob_of_outcome_tossing_n_dice(sum_dots, n, n_attempts = 1000):
    """""" Probability of the sum of the dots in n dice being sum_dots""""""
    count_successes = 0
    for a in range(n_attempts):
        # Toss dice
        sum_temp = 0
        for i in range(n):
            sum_temp += np.random.randint(6) + 1
        if sum_temp == sum_dots:
            count_successes += 1
    
    return float(count_successes) / n_attempts

print( prob_of_outcome_tossing_n_dice(3, 2, 10) )
print( prob_of_outcome_tossing_n_dice(3, 2, 100) )
print( prob_of_outcome_tossing_n_dice(3, 2) )
print( prob_of_outcome_tossing_n_dice(3, 2, 1000000) )"
"import scipy.stats as stats

othello_entropy = calc_entropy(othellos_dialogue)
iago_entropy = calc_entropy(iagos_dialogue)

stats.ttest_ind([othello_entropy], [iago_entropy] )"
"def synthetic_dialogue_generator(dialogue):
    import random
    new_dialogue = []
    for i in range(len(dialogue)):
        new_dialogue.append( random.choice(dialogue) )
    return new_dialogue

synthetic_dialogue = synthetic_dialogue_generator(othellos_dialogue)
print( len(synthetic_dialogue) )
print(synthetic_dialogue[:10])"
"from IPython.display import Image
Image(filename='bhcurve.png',width=600)"
"from IPython.display import Image
Image(filename='srm.png',width=600)"
"from IPython.display import Image
Image(filename='magnetic.png',width=300)"
"from IPython.display import Image
Image(filename='core.png', width=500, height=500)"
"from IPython.display import Image
Image(filename='skindepthreason.png', width=200, height=200)"
"from IPython.display import Image
Image(filename='skindepth.png', width=200, height=200)
"
"import warnings
warnings.filterwarnings('ignore')
import matplotlib.pyplot as plt
%matplotlib inline

plt.title('axvline/axhline, vlines/hlines, axvspan/axhspan')

# Vertical elements
plt.axvline(x=-0.5, ymin=.25, color='red')
plt.vlines(x=[0.6, 0.8], ymin=-.5, ymax=[0.75, 1.25], colors='blue')
plt.axvspan(xmin=-1.25, xmax=-.75, alpha=.5, color='green')

# Horizontal elements
plt.axhline(y=0.5, xmax=.75, color='orange')
plt.hlines(y=[0.7, 1.1], xmin=[.25, .5], xmax=0.75, color='black')
plt.axhspan(ymin=-1., ymax=0, alpha=.5, color='purple')

plt.ylim(-1.5);"
"fig = plt.figure(figsize=(15,7))
ax = fig.add_subplot(1, 1, 1, xticks=[], yticks=[],
                     title=""Flow Diagram of two connected, non-circular systems"")
sankey = Sankey(ax=ax, scale=0.01, unit='', format='%G', gap=2,
                radius=0.01, shoulder=0, offset=1, head_angle=160, margin=2.4)

sankey.add(patchlabel=""Flow A"",
           flows=[1000,1525 ,-2025],
           orientations=[-1, 1, 0],
           labels=['A', 'B', ''],
           trunklength=10,
           prior= None,
           connect=(0,0),
           alpha=0.2, lw=4.0)
        
s2 = sankey.add(patchlabel=""Flow B"",
           flows=[2025, -500, -450, -310,
                  -205, -400, -160],
           orientations=[0, 1, 1, 1,
                         1 , 0, -1],
           labels=['', 'B2', 'Housing', 'Food',
                   'Transportation', 'Health Care', 'Other Necessities'],
           trunklength=10,
           pathlengths=[10, 3, 3, 3,
                        3, 3, 5],
           prior= 0,
           connect=(2,0),
           rotation = 0,
           alpha=0.2, lw=4.0, color='r')  # Arguments to matplotlib.patches.PathPatch()

diagrams = sankey.finish()
diagrams[0].texts[2].remove()"
"plt.plot(t, inSignal, label='Input')
plt.plot(tout, outSignal, label='Output')
plt.legend(loc='lower right')
plt.ylim(0, 1.05)
plt.xlabel('Time [sec]')"
"showData(sampleRate, x, 'Cosine wave')

# Clip the data
y = x
y[1:199] = 0
y[400:1001] = 0
showData(sampleRate, y, 'Clipped Signal')

# Window the clipped data
z = y;
window = np.hamming(201)
z[199:400] = z[199:400]*window
showData(sampleRate, z, 'Clipped & Windowed Signal')"
"def findTargetSumWays_1(nums, S):
    """"""
    :type nums: Tuple[int]
    :type S: int
    :rtype: int
    """"""
    if not nums:
        if S == 0:
            return 1
        else:
            return 0
    return findTargetSumWays_1(nums[1:], S+nums[0]) + findTargetSumWays_1(nums[1:], S-nums[0]) 

%time findTargetSumWays_1(small_test_nums, small_test_S)"
"def findTargetSumWays_2(nums, S):
    if not nums:
        return 0
    dic = {nums[0]: 1, -nums[0]: 1} if nums[0] != 0 else {0: 2}
    for i in range(1, len(nums)):
        tdic = {}
        for d in dic:
            tdic[d + nums[i]] = tdic.get(d + nums[i], 0) + dic.get(d, 0)
            tdic[d - nums[i]] = tdic.get(d - nums[i], 0) + dic.get(d, 0)
        dic = tdic
    return dic.get(S, 0)

%time findTargetSumWays_2(big_test_nums, big_test_S)"
"@lru_cache(10000000)
def findTargetSumWays_3(nums, S):
    if not nums:
        if S == 0:
            return 1
        else:
            return 0
    return findTargetSumWays_3(nums[1:], S+nums[0]) + findTargetSumWays_3(nums[1:], S-nums[0]) 

%time findTargetSumWays_3(big_test_nums, big_test_S)"
"# load data
f, ferr, r, rerr = np.genfromtxt(""../data/flickers.dat"").T

# fit a line
C = np.diag(ferr)
AT = np.vstack((r, np.ones_like(r)))
ATA = np.dot(AT, AT.T)
m, c = np.linalg.solve(ATA, np.dot(AT, f))
print(m, c)

# plot data
xs = np.linspace(min(r), max(r), 100)
ys = m * xs + c
plt.errorbar(r, f, xerr=rerr, yerr=ferr, fmt=""k."", capsize=0)
plt.plot(xs, ys)
plt.ylabel(""log flicker"")
plt.xlabel(""log rho"")"
"resids = f - (m * r + c)
plt.errorbar(r, resids, xerr=rerr, yerr=ferr, fmt=""k."", capsize=0)"
"# fit a line
C = np.diag(ferr)
AT = np.vstack((l, np.ones_like(l)))
ATA = np.dot(AT, AT.T)
m, c = np.linalg.solve(ATA, np.dot(AT, f))
print(m, c)

# plot data
xs = np.linspace(min(l), max(l), 100)
ys = m * xs + c
plt.errorbar(l, f, xerr=lerr, yerr=ferr, fmt=""k."", capsize=0)
plt.plot(xs, ys)
plt.ylabel(""log flicker"")
plt.xlabel(""log g"")"
"resids = f - (m * l + c)
plt.errorbar(f, resids, xerr=lerr, yerr=ferr, fmt=""k."", capsize=0)"
"plt.hist(resids)
print(np.std(resids))
print(""errorbars underestimated by a factor of"", np.std(resids/ferr))"
"plt.hist(resids)
print(np.std(resids))
print(np.std(resids/ferr))"
"data = pd.DataFrame(np.random.uniform(-1, 1, (len(simulators), len(metrics))), columns=sorted(metrics), index=simulators)

styles = [
    dict(selector=""th"", props=[(""font-size"", ""100%""),
                               (""padding"", ""20px""),
                               (""width"", ""200px""),
                               (""border"", '0px'),
                               (""text-align"", ""center"")]),
    dict(selector=""tr"", props=[(""border"", '0px')]),
    dict(selector=""td"", props=[(""font-size"", ""100%""),
                               (""padding"", ""20px""),
                               (""text-align"", ""center"")]),
]

(data.style
     .set_table_styles(styles)
     .background_gradient(cmap='seismic', low=5.0, high=5.0)
     .set_precision(3))
"
"d = HBox(scales)
d"
"import warnings
warnings.filterwarnings('ignore')
%matplotlib inline

import sys

import numpy as np
import scipy as sp
import pandas as pd
import matplotlib as mpl

from numpy.fft import rfft, rfftfreq
from scipy.signal import spectrogram
import matplotlib.pyplot as plt

print(sys.version)
for module in (np, sp, pd, mpl):
    print('{:.<15}{}'.format(module.__name__, module.__version__))"
"Ns = 128
tt = np.arange(Ns)/Ns
Ac = 2.
fc = 2.
sc = Ac*(np.sin(2.*np.pi*fc*tt)+np.sin(2.*2.*np.pi*fc*tt))
sm = sc
se = sm-sc
FDE = FDEindex(sc, sm)
RMS = rms(se)
print('FDE index={:.2f}, RMS error={:.2f}'.format(FDE, RMS))
fig, ax = plt.subplots(1, 2, figsize=(12,4))
ax[0].plot(tt, sc, label='Computed')
ax[0].plot(tt, sm, label='Measured')
ax[0].legend()
ax[0].grid(b=True)
ax[1].plot(tt, se, label='Error')
ax[1].legend()
ax[1].grid(b=True)"
"sm1 = np.roll(sc, 1)
se1 = sm1-sc
FDE1 = FDEindex(sc, sm1)
RMS1 = rms(se1)
print('FDE index={:.2f}, RMS error={:.2f}'.format(FDE1, RMS1))
fig, ax = plt.subplots(1, 2, figsize=(12,4))
ax[0].plot(tt, sc, label='Computed')
ax[0].plot(tt, sm1, label='Measured')
ax[0].legend()
ax[0].grid(b=True)
ax[1].plot(tt, se1, label='Error')
ax[1].legend()
ax[1].grid(b=True)"
"sm2 = -sc
se2 = sm2-sc
FDE2 = FDEindex(sc, sm2)
RMS2 = rms(se2)
print('FDE index={:.2f}, RMS error={:.2f}'.format(FDE2, RMS2))
fig, ax = plt.subplots(1, 2, figsize=(12,4))
ax[0].plot(tt, sc, label='Computed')
ax[0].plot(tt, sm2, label='Measured')
ax[0].legend()
ax[0].grid(b=True)
ax[1].plot(tt, se2, label='Error')
ax[1].legend()
ax[1].grid(b=True)"
"sm3 = sm+(np.random.random(Ns)-0.5)
se3 = sm3-sc
FDE3 = FDEindex(sc, sm3)
RMS3 = rms(se3)
print('FDE index={:.2f}, RMS error={:.2f}'.format(FDE3, RMS3))
fig, ax = plt.subplots(1, 2, figsize=(12,4))
ax[0].plot(tt, sc, label='Computed')
ax[0].plot(tt, sm3, label='Measured')
ax[0].legend()
ax[0].grid(b=True)
ax[1].plot(tt, se3, label='Error')
ax[1].legend()
ax[1].grid(b=True)"
"data = np.array(((FDE, RMS), (FDE1, RMS1), (FDE2, RMS2), (FDE3, RMS3)))
index = ('Equal', 'Delay 1', 'Times -1', 'Add UWN')
columns = ('FDE index', 'RMS error')
df = pd.DataFrame(data, index, columns)
print(df)"
"ff = rfftfreq(Ns, d=tt[1]-tt[0])
SCOMP = rfft(sc)/Ns
SMEAS = rfft(sm3)/Ns
SERR = SMEAS-SCOMP
fig, ax = plt.subplots(1,2, figsize=(12,4))
ax[0].plot(ff, np.absolute(SCOMP), label='Computed')
ax[0].plot(ff, np.absolute(SMEAS), label='Measured')
ax[0].legend()
ax[0].grid(b=True)
ax[1].plot(ff, np.absolute(SERR), label='Error')
ax[1].legend()
ax[1].grid(b=True)"
"import warnings
warnings.filterwarnings('ignore')
# IPython magic commands
%matplotlib inline

# Python standard library
import sys
import os.path

# 3rd party modules
import numpy as np
import scipy as sp
import matplotlib as mpl

from scipy import signal
import matplotlib.pyplot as plt

print(sys.version)
for module in (np, sp, mpl):
    print('{:.<15}{}'.format(module.__name__, module.__version__))"
"fig, ax = plt.subplots(2, 2, figsize=(18,12))
for order in range(3):
    # digital filter
    b, a = signal.butter(2*(order+1), wc, analog=False)
    w, h = signal.freqz(b, a)
    ax[0][0].plot(w*fs/(2*np.pi), 20 * np.log10(abs(h)), label=f'order={2*(order+1)}')
    ax[0][0].axvline(fc, color='green')
    ax[1][0].plot(w*fs/(2*np.pi), np.unwrap(np.angle(h))*180/np.pi, label=f'order={2*(order+1)}')
    ax[1][0].axvline(fc, color='green')
    # analog filter
    b, a = signal.butter(2*(order+1), fc, analog=True)
    w, h = signal.freqs(b, a)
    ax[0][1].semilogx(w, 20 * np.log10(abs(h)), label=f'order={2*(order+1)}')
    ax[0][1].axvline(fc, color='green')
    ax[1][1].semilogx(w, np.unwrap(np.angle(h))*180/np.pi, label=f'order={2*(order+1)}')
    ax[1][1].axvline(fc, color='green')
format_plots(ax, 'Butterworth')"
"fig, ax = plt.subplots(2, 2, figsize=(18,12))
rp = 5 # maximum ripple allowed below unity gain in the passband, specified in decibels as a positive number
for order in range(3):
    # digital filter
    b, a = signal.cheby1(2*(order+1), rp, wc, analog=False)
    w, h = signal.freqz(b, a)
    ax[0][0].plot(w*fs/(2*np.pi), 20 * np.log10(abs(h)), label=f'order={2*(order+1)}')
    ax[0][0].axvline(fc, color='green')
    ax[0][0].axhline(-rp, color='green')
    ax[1][0].plot(w*fs/(2*np.pi), np.unwrap(np.angle(h))*180/np.pi, label=f'order={2*(order+1)}')
    ax[1][0].axvline(fc, color='green')
    # analog filter
    b, a = signal.cheby1(2*(order+1), rp, fc, analog=True)
    w, h = signal.freqs(b, a)
    ax[0][1].semilogx(w, 20 * np.log10(abs(h)), label=f'order={2*(order+1)}')
    ax[0][1].axvline(fc, color='green')
    ax[0][1].axhline(-rp, color='green')
    ax[1][1].semilogx(w, np.unwrap(np.angle(h))*180/np.pi, label=f'order={2*(order+1)}')
    ax[1][1].axvline(fc, color='green')
format_plots(ax, f'Chebyshev Type I (rp={rp})')"
"fig, ax = plt.subplots(2, 2, figsize=(18,12))
rs = 40 # minimum attenuation required in the stop band, specified in decibels as a positive number
for order in range(3):
    # digital filter
    b, a = signal.cheby2(2*(order+1), rs, wc, analog=False)
    w, h = signal.freqz(b, a)
    ax[0][0].plot(w*fs/(2*np.pi), 20 * np.log10(abs(h)), label=f'order={2*(order+1)}')
    ax[0][0].axvline(fc, color='green')
    ax[0][0].axhline(-rs, color='green')
    ax[1][0].plot(w*fs/(2*np.pi), np.unwrap(np.angle(h))*180/np.pi, label=f'order={2*(order+1)}')
    ax[1][0].axvline(fc, color='green')
    # analog filter
    b, a = signal.cheby2(2*(order+1), rs, fc, analog=True)
    w, h = signal.freqs(b, a)
    ax[0][1].semilogx(w, 20 * np.log10(abs(h)), label=f'order={2*(order+1)}')
    ax[0][1].axvline(fc, color='green')
    ax[0][1].axhline(-rs, color='green')
    ax[1][1].semilogx(w, np.unwrap(np.angle(h))*180/np.pi, label=f'order={2*(order+1)}')
    ax[1][1].axvline(fc, color='green')
format_plots(ax, f'Chebyshev Type II (rs={rs})')"
"fig, ax = plt.subplots(2, 2, figsize=(18,12))
rp = 5 # maximum ripple allowed below unity gain in the passband, specified in decibels as a positive number
rs = 40 # minimum attenuation required in the stop band, specified in decibels as a positive number
for order in range(3):
    # digital filter
    b, a = signal.ellip(2*(order+1), rp, rs, wc, analog=False)
    w, h = signal.freqz(b, a)
    ax[0][0].plot(w*fs/(2*np.pi), 20 * np.log10(abs(h)), label=f'order={2*(order+1)}')
    ax[0][0].axvline(fc, color='green')
    ax[0][0].axhline(-rp, color='green')
    ax[0][0].axhline(-rs, color='green')
    ax[1][0].plot(w*fs/(2*np.pi), np.unwrap(np.angle(h))*180/np.pi, label=f'order={2*(order+1)}')
    ax[1][0].axvline(fc, color='green')
    # analog filter
    b, a = signal.ellip(2*(order+1), rp, rs, fc, analog=True)
    w, h = signal.freqs(b, a)
    ax[0][1].semilogx(w, 20 * np.log10(abs(h)), label=f'order={2*(order+1)}')
    ax[0][1].axvline(fc, color='green')
    ax[0][1].axhline(-rp, color='green')
    ax[0][1].axhline(-rs, color='green')
    ax[1][1].semilogx(w, np.unwrap(np.angle(h))*180/np.pi, label=f'order={2*(order+1)}')
    ax[1][1].axvline(fc, color='green')
format_plots(ax, f'Elliptic (rp={rp}, rs={rs})')"
"fig, ax = plt.subplots(2, 2, figsize=(18,12))
for order in range(3):
    # digital filter
    b, a = signal.bessel(2*(order+1), wc, analog=False)
    w, h = signal.freqz(b, a)
    ax[0][0].plot(w*fs/(2*np.pi), 20 * np.log10(abs(h)), label=f'order={2*(order+1)}')
    ax[0][0].axvline(fc, color='green')
    ax[1][0].plot(w*fs/(2*np.pi), np.unwrap(np.angle(h))*180/np.pi, label=f'order={2*(order+1)}')
    ax[1][0].axvline(fc, color='green')
    # analog filter
    b, a = signal.bessel(2*(order+1), fc, analog=True)
    w, h = signal.freqs(b, a)
    ax[0][1].semilogx(w, 20 * np.log10(abs(h)), label=f'order={2*(order+1)}')
    ax[0][1].axvline(fc, color='green')
    ax[1][1].semilogx(w, np.unwrap(np.angle(h))*180/np.pi, label=f'order={2*(order+1)}')
    ax[1][1].axvline(fc, color='green')
format_plots(ax, 'Bessel')"
"fig, ax = plt.subplots(2, 2, figsize=(18,12))
for Q in range(3): # quality factor
    # Notch filter
    b, a = signal.iirnotch(wc, 10*(Q+1))
    w, h = signal.freqz(b, a)
    ax[0][0].plot(w*fs/(2*np.pi), 20 * np.log10(abs(h)), label=f'Q={10*(Q+1)}')
    ax[0][0].axvline(fc, color='green')
    ax[1][0].plot(w*fs/(2*np.pi), np.unwrap(np.angle(h))*180/np.pi, label=f'Q={10*(Q+1)}')
    ax[1][0].axvline(fc, color='green')
    # Peak filter
    b, a = signal.iirpeak(wc, 10*(Q+1))
    w, h = signal.freqz(b, a)
    ax[0][1].plot(w*fs/(2*np.pi), 20 * np.log10(abs(h)), label=f'Q={10*(Q+1)}')
    ax[0][1].axvline(fc, color='green')
    ax[1][1].plot(w*fs/(2*np.pi), np.unwrap(np.angle(h))*180/np.pi, label=f'Q={10*(Q+1)}')
    ax[1][1].axvline(fc, color='green')
format_plots(ax, 'Digital', column_name=('notch', 'peak'))"
"print('System: {}'.format(sys.version))
for package in (np, sp, mpl, pd):
    print('Package: {} {}'.format(package.__name__, package.__version__))"
"x = np.linspace(rv.ppf(0.001), rv.ppf(0.999), 1000)
fig, ax = plt.subplots(1, 1)
ax.plot(x, rv.pdf(x), label='frozen pdf')
ax.plot(x, rv.cdf(x), lw=2, label='frozen cdf')
ax.axhline(0.5, ls=':')
ax.axhline(1.0, ls=':')
ax.axvline(mean, ls='-.', label='mean')
ax.legend(loc='best', frameon=False)
plt.show()"
"Nsamples = [u*d for d in (10,100,1000) for u in (1,2,5)]
ysamples = []
fig, ax = plt.subplots(3, 3, sharex=True, sharey=True, figsize=(8,6))
for row in range(3):
    for col in range(3):
        N = Nsamples[3*row+col]
        ax[row,col].plot(x, rv.pdf(x), 'k-', lw=2)
        y = rv.rvs(size=N)
        b = int(math.sqrt(N))
        ysamples.append(y)
        ax[row,col].hist(y, bins=b, normed=True, histtype='stepfilled', alpha=0.2)
        ax[row,col].set_title('{} samples, {} bins'.format(N, b))
plt.show()"
"fig, ax = plt.subplots(3, 3, sharex=True, sharey=True, figsize=(8,6))
for row in range(3):
    for col in range(3):
        N = Nsamples[3*row+col]
        y = ysamples[3*row+col]
        ax[row,col].plot(x, rv.cdf(x), 'k-', lw=2)
        yn = np.sort(y)
        xn = np.linspace(0., 1., num=N)
        ax[row,col].plot(yn, xn, 'o', alpha=0.2)
        ax[row,col].axhline(0.5, ls=':')
        ax[row,col].axvline(mean, ls=':')
        ax[row,col].set_title('{} samples'.format(N))
plt.show()"
"Dsamples = []
psamples = []
for N,y in zip(Nsamples, ysamples):
    D, pvalue = stats.kstest(y, 'norm', args=(mean, std))
    Dsamples.append(D)
    psamples.append(pvalue)
    print('{:4d} samples: D={}, pvalue={}'.format(N, D, pvalue))"
"fig, ax = plt.subplots(1, 1)
ax.scatter(psamples, Dsamples, s=Nsamples, alpha=0.2)
for p,D,N in zip(psamples, Dsamples, Nsamples):
    ax.text(p, D, str(N), ha='center', va='center')
ax.set_xlabel('p-value')
ax.set_ylabel('D')
ax.set_title('K-S test results')
plt.show()"
"for N,y in zip(Nsamples, ysamples):
    A2, critical_values, significance_level = stats.anderson(y)
    print('{:4d} samples: A2={}'.format(N, A2), critical_values, significance_level)"
"Wsamples = []
psamples = []
for N,y in zip(Nsamples, ysamples):
    W, pvalue = stats.shapiro(y)
    Wsamples.append(W)
    psamples.append(pvalue)
    print('{:4d} samples: W={}, pvalue={}'.format(N, W, pvalue))"
"fig, ax = plt.subplots(1, 1)
ax.scatter(psamples, Wsamples, s=Nsamples, alpha=0.2)
for p,W,N in zip(psamples, Wsamples, Nsamples):
    ax.text(p, W, str(N), ha='center', va='center')
ax.set_xlabel('p-value')
ax.set_ylabel('W')
ax.set_title('S-W test results')
plt.show()"
"import warnings
warnings.filterwarnings('ignore')
import sys
import numpy as np
import scipy as sp
import matplotlib as mpl

print('System: {}'.format(sys.version))
print('numpy version: {}'.format(np.__version__))
print('scipy version: {}'.format(sp.__version__))
print('matplotlib version: {}'.format(mpl.__version__))"
"MM = np.matrix(np.diag([1., 2.]))
print(MM)"
"C2 = np.matrix([[0.1, 0.2], [0.2, 0.2]])
print(C2)"
"W2, F1 = LA.eig(LA.solve(MM,KK)) # eigenanalysis
ix = np.argsort(np.absolute(W2)) # sort eigenvalues in ascending order
W2 = W2[ix] # sorted eigenvalues
F1 = F1[:,ix] # sorted eigenvectors
print(np.round_(W2, 4))
print(np.round_(F1, 4))"
print(np.sqrt(W2))
"print(LA.norm(F1, axis=0))"
"fig, ax = plt.subplots(1, 2, subplot_kw=dict(polar=True))
for mode in range(2):
    ax[mode].set_title('Mode #{}'.format(mode+1))
    for dof in range(2):
        r = np.array([0, np.absolute(F1[dof,mode])])
        t = np.array([0, np.angle(F1[dof,mode])])
        ax[mode].plot(t, r, 'o-', label='DOF #{}'.format(dof+1))
plt.legend(loc='lower left', bbox_to_anchor=(1., 0.))
plt.show()"
"print(np.round_(F1.T*C1*F1, 4))"
"w1, v1 = LA.eig(LA.solve(A,B))
ix = np.argsort(np.absolute(w1))
w1 = w1[ix]
v1 = v1[:,ix]
print(np.round_(w1, 4))
print(np.round_(v1, 4))"
"zw = -w1.real # damping coefficient time angular frequency
wD = w1.imag # damped angular frequency
zn = 1./np.sqrt(1.+(wD/-zw)**2) # the minus sign is formally correct!
wn = zw/zn # undamped angular frequency
print('Angular frequency: {}'.format(wn[[0,2]]))
print('Damping coefficient: {}'.format(zn[[0,2]]))"
"print(LA.norm(v1[:,::2], axis=0))"
"AA = v1[:2,[0,2]]
AB = AA.conjugate()
BA = np.multiply(AA,w1[[0,2]])
BB = BA.conjugate()
v1_new = np.bmat([[AA, AB], [BA, BB]])
print(np.round_(v1_new[:,[0,2,1,3]], 4))"
"fig, ax = plt.subplots(1, 2, subplot_kw=dict(polar=True))
for mode in range(2):
    ax[mode].set_title('Mode #{}'.format(mode+1))
    for dof in range(2):
        r = np.array([0, np.absolute(v1[dof,2*mode])])
        t = np.array([0, np.angle(v1[dof,2*mode])])
        ax[mode].plot(t, r, 'o-', label='DOF #{}'.format(dof+1))
plt.legend(loc='lower left', bbox_to_anchor=(1., 0.))
plt.show()"
"print(np.round_(F1.T*C2*F1, 4))"
"A = np.bmat([[np.zeros_like(MM), MM], [MM, C2]])
print(A)"
"w2, v2 = LA.eig(LA.solve(A,B))
ix = np.argsort(np.absolute(w2))
w2 = w2[ix]
v2 = v2[:,ix]
print(np.round_(w2, 4))
print(np.round_(v2, 4))"
"zw = -w2.real # damping coefficient times angular frequency
wD = w2.imag # damped angular frequency
zn = 1./np.sqrt(1.+(wD/-zw)**2) # the minus sign is formally correct!
wn = zw/zn # undamped angular frequency
print('Angular frequency: {}'.format(wn[[0,2]]))
print('Damping coefficient: {}'.format(zn[[0,2]]))"
"print(LA.norm(v2[:,[0,2]], axis=0))"
"AA = v2[:2,[0,2]]
AB = AA.conjugate()
BA = np.multiply(AA,w2[[0,2]])
BB = BA.conjugate()
v2_new = np.bmat([[AA, AB], [BA, BB]])
print(np.round_(v2_new[:,[0,2,1,3]], 4))"
"fig, ax = plt.subplots(1, 2, subplot_kw=dict(polar=True))
for mode in range(2):
    ax[mode].set_title('Mode #{}'.format(mode+1))
    for dof in range(2):
        r = np.array([0, np.absolute(v2[dof,2*mode])])
        t = np.array([0, np.angle(v2[dof,2*mode])])
        ax[mode].plot(t, r, 'o-', label='DOF #{}'.format(dof+1))
plt.legend(loc='lower left', bbox_to_anchor=(1., 0.))
plt.show()"
"import warnings
warnings.filterwarnings('ignore')
import sys
import numpy as np
import scipy as sp
import matplotlib as mpl

print('System: {}'.format(sys.version))
print('numpy version: {}'.format(np.__version__))
print('scipy version: {}'.format(sp.__version__))
print('matplotlib version: {}'.format(mpl.__version__))"
"W2, F1 = LA.eig(LA.solve(MM,KK)) # eigenanalysis
ix = np.argsort(np.absolute(W2)) # sort eigenvalues in ascending order
W2 = W2[ix] # sorted eigenvalues
F1 = F1[:,ix] # sorted eigenvectors
print(np.round_(W2, 4))
print(np.round_(F1, 4))"
print(np.sqrt(W2))
"print(LA.norm(F1, axis=0))"
"A = np.bmat([[np.zeros_like(MM), np.identity(MM.shape[0])], [LA.solve(-MM,KK), LA.solve(-MM,C0)]])
print(A)"
"w0, v0 = LA.eig(A)
ix = np.argsort(np.absolute(w0))
w0 = w0[ix]
v0 = v0[:,ix]
print(np.round_(w0, 4))
print(np.round_(v0, 4))"
"print(w0[[0,2]].imag)"
"print(LA.norm(v0[:,[0,2]], axis=0))"
"AA = v0[:2,[0,2]]
AB = AA.conjugate()
BA = np.multiply(AA,w0[[0,2]])
BB = BA.conjugate()
v0_new = np.bmat([[AA, AB], [BA, BB]])
print(np.round_(v0_new[:,[0,2,1,3]], 4))"
"fig, ax = plt.subplots(1, 2, subplot_kw=dict(polar=True))
for mode in range(2):
    ax[mode].set_title('Mode #{}'.format(mode+1))
    for dof in range(2):
        r = np.array([0, np.absolute(v0[dof,2*mode])])
        t = np.array([0, np.angle(v0[dof,2*mode])])
        ax[mode].plot(t, r, 'o-', label='DOF #{}'.format(dof+1))
plt.legend(loc='lower left', bbox_to_anchor=(1., 0.))
plt.show()"
"print(np.round_(F1.T*C1*F1, 4))"
"w1, v1 = LA.eig(A)
ix = np.argsort(np.absolute(w1))
w1 = w1[ix]
v1 = v1[:,ix]
print(np.round_(w1, 4))
print(np.round_(v1, 4))"
"print(np.round_(w1[[0,2]], 4))"
"zw = -w1.real # damping coefficient time angular frequency
wD = w1.imag # damped angular frequency
zn = 1./np.sqrt(1.+(wD/-zw)**2) # the minus sign is formally correct!
wn = zw/zn # undamped angular frequency
print('Angular frequency: {}'.format(wn[[0,2]]))
print('Damping coefficient: {}'.format(zn[[0,2]]))"
"print(LA.norm(v1[:,[0,2]], axis=0))"
"fig, ax = plt.subplots(1, 2, subplot_kw=dict(polar=True))
for mode in range(2):
    ax[mode].set_title('Mode #{}'.format(mode+1))
    for dof in range(2):
        r = np.array([0, np.absolute(v1[dof,2*mode])])
        t = np.array([0, np.angle(v1[dof,2*mode])])
        ax[mode].plot(t, r, 'o-', label='DOF #{}'.format(dof+1))
plt.legend(loc='lower left', bbox_to_anchor=(1., 0.))
plt.show()"
"print(np.round_(F1.T*C2*F1, 4))"
"w2, v2 = LA.eig(A)
ix = np.argsort(np.absolute(w2))
w2 = w2[ix]
v2 = v2[:,ix]
print(np.round_(w2, 4))
print(np.round_(v2, 4))"
"zw = -w2.real # damping coefficient time angular frequency
wD = w2.imag # damped angular frequency
zn = 1./np.sqrt(1.+(wD/-zw)**2) # the minus sign is formally correct!
wn = zw/zn # undamped angular frequency
print('Angular frequency: {}'.format(wn[[0,2]]))
print('Damping coefficient: {}'.format(zn[[0,2]]))"
"print(LA.norm(v2[:,[0,2]], axis=0))"
"AA = v2[:2,[0,2]]
AB = AA.conjugate()
BA = np.multiply(AA,w2[[0,2]])
BB = BA.conjugate()
v2_new = np.bmat([[AA, AB], [BA, BB]])
print(np.round_(v2_new[:,[0,2,1,3]], 4))"
"fig, ax = plt.subplots(1, 2, subplot_kw=dict(polar=True))
for mode in range(2):
    ax[mode].set_title('Mode #{}'.format(mode+1))
    for dof in range(2):
        r = np.array([0, np.absolute(v2[dof,2*mode])])
        t = np.array([0, np.angle(v2[dof,2*mode])])
        ax[mode].plot(t, r, 'o-', label='DOF #{}'.format(dof+1))
plt.legend(loc='lower left', bbox_to_anchor=(1., 0.))
plt.show()"
"import warnings
warnings.filterwarnings('ignore')
import sys
import math
import numpy as np
import matplotlib as mpl
import matplotlib.pyplot as plt
%matplotlib inline
print('System: {}'.format(sys.version))
for package in (np, mpl):
    print('Package: {} {}'.format(package.__name__, package.__version__))"
"MM = np.matrix(np.diag([1.,2.,3.]))
KK = np.matrix([[2,-1,0],[-1,2,-1],[0,-1,1]])*1000.
print(MM)
print(KK)"
"W2, F1 = np.linalg.eig(KK.I@MM)
print(W2)
print(F1)"
"ix = np.argsort(W2)[::-1]
W2 = 1./W2[ix]
F1 = F1[:,ix]
Wn = np.sqrt(W2)
print(W2)
print(Wn)
print(F1)"
"Mn = np.diag(Fn.T@MM@Fn)
Kn = np.diag(Fn.T@KK@Fn)
print(Mn)
print(Kn)"
"sp = np.matrix([[0.], [1.], [0.]])
wp = 2.*np.pi*4.
print(sp)
print(wp)"
"tt = np.arange(1000)*0.005
pt = np.sin(wp*tt)
plt.figure()
plt.plot(tt, pt)
plt.xlabel('Tempo (s)')
plt.ylabel('Força (kN)')
plt.show()"
"qn_t = []
plt.figure()
for n in range(3):
    an = Gn[n]/Kn[n]
    bn = wp/Wn[n]
    q = qn(an[0,0], wp, bn, 0.05, tt)
    qn_t.append(q)
    plt.plot(tt, q, label='an={:.2e},bn={:.2f}'.format(an[0,0], bn))
plt.legend()
plt.xlabel('Tempo (s)')
plt.ylabel('Deslocamento modal (m)')
plt.show()"
"plt.figure()
u_t = Fn@qn_t
for n in range(3):
    plt.plot(tt, u_t[n].T, label='{:.2e}'.format(np.max(u_t[n])))
plt.legend()
plt.xlabel('Tempo (s)')
plt.ylabel('Deslocamento (m)')
plt.show()"
"ust = KK.I@sp
print(ust)"
"qn_st_t = []
plt.figure()
for n in range(3):
    an = Fn.T[n]@MM@ust/Mn[n]
    bn = wp/Wn[n]
    qst = qn(an[0,0], wp, bn, 0.05, tt)
    qn_st_t.append(qst)
    plt.plot(tt, qst, label='an={:.2e},bn={:.2f}'.format(an[0,0], bn))
plt.legend()
plt.xlabel('Tempo (s)')
plt.ylabel('Deslocamento modal (m)')
plt.show()"
"plt.figure()
u_t = Fn@qn_st_t
for n in range(3):
    plt.plot(tt, u_t[n].T, label='{:.2e}'.format(np.max(u_t[n])))
plt.legend()
plt.xlabel('Tempo (s)')
plt.ylabel('Deslocamento (m)')
plt.show()"
"f = (y2-y)/(y2-y1)*(Q11*(x2-x)/(x2-x1)+Q21*(x-x1)/(x2-x1))+(y-y1)/(y2-y1)*(Q12*(x2-x)/(x2-x1)+Q22*(x-x1)/(x2-x1))
f"
"sympy.simplify(sympy.diff(f,x))"
"sympy.simplify(sympy.diff(f,y))"
"f1 = (1-eta)/2*(Q11*(1-csi)/2+Q21*(csi+1)/2)+(eta+1)/2*(Q12*(1-csi)/2+Q22*(csi+1)/2)
f1"
"sympy.simplify(sympy.diff(f1,csi))"
"sympy.simplify(sympy.diff(f1,eta))"
"# Likelihood function of a Bernoulli trial (coin toss)
p_list = linspace(0,1,500)
l_list = [L(1,10,p) for p in p_list]
plt.figure(figsize=(4,2))
plt.rcParams['font.size']=15
plt.plot(p_list,l_list)
plt.xlabel('$p$')
plt.ylabel('$L(p|k)$')
plt.yticks([0,0.1,0.2,0.3,0.4]);"
"plt.figure(figsize=(5,4))
plt.subplot(211)
plt.plot(p_list,gradient(l_list))
plt.plot(p_list, zeros_like(p_list), 'k--')
for i in range(len(candidate_idx)):
    plt.plot(p_list[candidate_idx[i]], 
             gradient(l_list)[candidate_idx[i]], 'ro')
#plt.plot(p_list)
plt.ylim([-0.01,0.01])
plt.yticks([-0.01, 0, 0.01])
plt.ylabel('$\dot{L}$')

plt.subplot(212)
plt.plot(p_list,gradient(gradient(l_list)))
for i in range(len(candidate_idx)):
    plt.plot(p_list[candidate_idx[i]], 
             gradient(gradient(l_list))[candidate_idx[i]], 'ro')

plt.ylim([-0.001, 0.001])
plt.yticks([-0.001, 0, 0.001])
plt.ylabel('$\ddot{L}$')
plt.xlabel('$p$')
plt.tight_layout()"
"n = 10
k = 1
plt.figure(figsize=(4,4))
plt.subplot(211)
plt.plot(p_list, k*log(p_list)+(n-k)*log(1.-p_list) )
plt.ylim([-20,0])
plt.ylabel('$L(p\,|\,k)$')
plt.xlabel('$p$')

plt.subplot(212)
phi_list = log(p_list/(1+p_list))
plt.plot(phi_list, k*phi_list + n*log(1./(1.+exp(phi_list))))
plt.yticks([-3,-4,-5, -6])
#plt.xticks([-5,-4,-3,-2,-1])
plt.ylabel('$L(\\phi\,|\,k)$')
plt.xlabel('$\\phi$')
plt.tight_layout()"
"n = 10
k = 1
ll_phi = lambda phi: k*phi + n*log(1./(1.+exp(phi)))
soln = fsolve(ll_phi, -1)
print(soln)
soln_phi = exp(soln)/(1.+exp(soln))
print(soln_phi)"
"def f(x): return 2*np.exp(x) - (x+0.5)**2
def df(x): return 2*np.exp(x) - 2*(x+0.5)
def d2f(x): return 2*np.exp(x) - 2

# Mediante prueba y error, llegamos a este intervalo donde f cambia
# el signo pero f'>0 y 
a,b = -2, 0 
print(""[a,b] = [%f,%f]"" % (a,b))

x = np.linspace(a, b,100)
plt.plot(x, f(x), c=""red"", linewidth=4)
plt.plot(x, df(x), c=""gray"", lw=2)
plt.plot(x, d2f(x), c=""orange"")

plt.grid()
plt.axhline(c='k');
plt.legend([r""$f(x)$"",r""$f'(x)$"",r""$f''(x)$""])"
"x0 = -2 # Inicialización sugerida por la regla de Fourier
x, niter = newton(f,df,x0)
print(""Aproximación por el m. de Newton:"", x)
print(""Número de iteraciones:"", niter) 
print(""Resíduo: |f(x)| = %1.16f"" % abs(f(x)))"
"import warnings
warnings.filterwarnings('ignore')
%pylab inline"
"f = lambda x: x-3*np.arctan(x)+1
x = linspace(-6,4,num=300)
plot(x,f(x), lw=2)
axhline(c='k'); axvline(c='k')
grid()"
"df = lambda x: 1-3/(1+x**2)
df(0)
lista_x = newton(f,df,x0=2)
print(""Lista de aproximaciones:"", lista_x)
print(""Número de iteraciones: %d "" % len(lista_x))
print(""Última aproximación: %1.6f"" % lista_x[-1])"
"a,b=2,4
x=linspace(2,4)

subplot(2,1,1)
plot(x,df(x),label=""f'(x)"")
legend()
grid()

subplot(2,1,2)
d2f = lambda x: 6*x/(1+x**2)**2
plot(x,d2f(x), label=""f''(x)"")
legend()
grid()"
"g = lambda x: 1.0/(2+x)

x = np.linspace(0,1)
plt.plot(x, x, ""--"", color=""black"", linewidth=2, label=""$y=x$"")
plt.plot(x, g(x), color=""red"", linewidth=4, label=""$y=g(x)=(2+x)^{-1}$"")
plt.legend()
plt.grid()
plt.show()"
"y = 2*sin(x)*cos(x);
plot(x,y);
grid()"
arr
arr
"np.linspace( 0, 2, 9 )   # 0 到 2 总共 9 位等距间隔"
"A = np.random.randn(500)
hist(A)"
np.std(A)    # 方差
np.mean(A)   # 均值
"N = 1000
x = rnd.randn(N)
a = 0.2

y = [x[0]]
for n in range(N - 1):
    y.append(a*y[-1] + x[n])

plt.figure()
plt.plot(y, '.-', alpha=0.5)
plt.plot(x, '.-', color='r', alpha=0.5)
plt.xlabel('Time index n')"
"# Auto-Correlation function
R = []
sigsq_x = 1
for el in range(N):
    R.append(sigsq_x*(a**el)/(1 - a**2))
plt.figure()
plt.plot(R, '.-')
"
"from IPython.display import YouTubeVideo
YouTubeVideo('phnEFFL5K_A')"
"# In R, I exported the dataset from package 'ISLR' to an Excel file
df = pd.read_excel('Data/Default.xlsx')

# Note: factorize() returns two objects: a label array and an array with the unique values.
# We are only interested in the first object. 
df['default2'] = df.default.factorize()[0]
df['student2'] = df.student.factorize()[0]
df.head(3)"
"fig = plt.figure(figsize=(12,5))
gs = mpl.gridspec.GridSpec(1, 4)
ax1 = plt.subplot(gs[0,:-2])
ax2 = plt.subplot(gs[0,-2])
ax3 = plt.subplot(gs[0,-1])

# Take a fraction of the samples where target value (default) is 'no'
df_no = df[df.default2 == 0].sample(frac=0.15)
# Take all samples  where target value is 'yes'
df_yes = df[df.default2 == 1]
df_ = df_no.append(df_yes)

ax1.scatter(df_[df_.default == 'Yes'].balance, df_[df_.default == 'Yes'].income, s=40, c='orange', marker='+',
            linewidths=1)
ax1.scatter(df_[df_.default == 'No'].balance, df_[df_.default == 'No'].income, s=40, marker='o', linewidths='1',
            edgecolors='lightblue', facecolors='white', alpha=.6)

ax1.set_ylim(ymin=0)
ax1.set_ylabel('Income')
ax1.set_xlim(xmin=-100)
ax1.set_xlabel('Balance')

c_palette = {'No':'lightblue', 'Yes':'orange'}
sns.boxplot('default', 'balance', data=df, orient='v', ax=ax2, palette=c_palette)
sns.boxplot('default', 'income', data=df, orient='v', ax=ax3, palette=c_palette)
gs.tight_layout(plt.gcf())"
"X_train = df.balance.values.reshape(-1,1) 
y = df.default2

# Create array of test data. Calculate the classification probability
# and predicted classification.
X_test = np.arange(df.balance.min(), df.balance.max()).reshape(-1,1)

clf = skl_lm.LogisticRegression(solver='newton-cg')
clf.fit(X_train,y)
prob = clf.predict_proba(X_test)

fig, (ax1, ax2) = plt.subplots(1,2, figsize=(12,5))
# Left plot
sns.regplot(df.balance, df.default2, order=1, ci=None,
            scatter_kws={'color':'orange'},
            line_kws={'color':'lightblue', 'lw':2}, ax=ax1)
# Right plot
ax2.scatter(X_train, y, color='orange')
ax2.plot(X_test, prob[:,1], color='lightblue')

for ax in fig.axes:
    ax.hlines(1, xmin=ax.xaxis.get_data_interval()[0],
              xmax=ax.xaxis.get_data_interval()[1], linestyles='dashed', lw=1)
    ax.hlines(0, xmin=ax.xaxis.get_data_interval()[0],
              xmax=ax.xaxis.get_data_interval()[1], linestyles='dashed', lw=1)
    ax.set_ylabel('Probability of default')
    ax.set_xlabel('Balance')
    ax.set_yticks([0, 0.25, 0.5, 0.75, 1.])
    ax.set_xlim(xmin=-100)"
"# Using newton-cg solver, the coefficients are equal/closest to the ones in the book. 
# I do not know the details on the differences between the solvers.
clf = skl_lm.LogisticRegression(solver='newton-cg')
X_train = df.balance.values.reshape(-1,1)
clf.fit(X_train,y)
print(clf)
print('classes: ',clf.classes_)
print('coefficients: ',clf.coef_)
print('intercept :', clf.intercept_)"
"# balance and default vectors for students
X_train = df[df.student == 'Yes'].balance.values.reshape(df[df.student == 'Yes'].balance.size,1) 
y = df[df.student == 'Yes'].default2

# balance and default vectors for non-students
X_train2 = df[df.student == 'No'].balance.values.reshape(df[df.student == 'No'].balance.size,1) 
y2 = df[df.student == 'No'].default2

# Vector with balance values for plotting
X_test = np.arange(df.balance.min(), df.balance.max()).reshape(-1,1)

clf = skl_lm.LogisticRegression(solver='newton-cg')
clf2 = skl_lm.LogisticRegression(solver='newton-cg')

clf.fit(X_train,y)
clf2.fit(X_train2,y2)

prob = clf.predict_proba(X_test)
prob2 = clf2.predict_proba(X_test)"
"# creating plot
fig, (ax1, ax2) = plt.subplots(1,2, figsize=(12,5))

# Left plot
ax1.plot(X_test, pd.DataFrame(prob)[1], color='orange', label='Student')
ax1.plot(X_test, pd.DataFrame(prob2)[1], color='lightblue', label='Non-student')
ax1.hlines(127/2817, colors='orange', label='Overall Student',
           xmin=ax1.xaxis.get_data_interval()[0],
           xmax=ax1.xaxis.get_data_interval()[1], linestyles='dashed')
ax1.hlines(206/6850, colors='lightblue', label='Overall Non-Student',
           xmin=ax1.xaxis.get_data_interval()[0],
           xmax=ax1.xaxis.get_data_interval()[1], linestyles='dashed')
ax1.set_ylabel('Default Rate')
ax1.set_xlabel('Credit Card Balance')
ax1.set_yticks([0, 0.2, 0.4, 0.6, 0.8, 1.])
ax1.set_xlim(450,2500)
ax1.legend(loc=2)

# Right plot
sns.boxplot('student', 'balance', data=df, orient='v', ax=ax2,  palette=c_palette);"
"print(classification_report(y, y_pred, target_names=['No', 'Yes']))"
lda.priors_
"print(classification_report(y_test, pred, digits=3))"
"np.unique(pred_p[:,1]>0.5, return_counts=True)"
"np.unique(pred_p[:,1]>0.9, return_counts=True)"
qda.priors_
"print(classification_report(y_test, pred, digits=3))"
"knn = neighbors.KNeighborsClassifier(n_neighbors=1)
pred = knn.fit(X_train, y_train).predict(X_test)
print(confusion_matrix(y_test, pred).T)
print(classification_report(y_test, pred, digits=3))"
"knn = neighbors.KNeighborsClassifier(n_neighbors=3)
pred = knn.fit(X_train, y_train).predict(X_test)
print(confusion_matrix(y_test, pred).T)
print(classification_report(y_test, pred, digits=3))"
"for i in [1,3,5]:
    pred, score, classes = KNN(i)
    cm = confusion_matrix(y_test, pred)
    cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
    plot_confusion_matrix(cm_normalized.T, classes, n_neighbors=i)
    cm_df = pd.DataFrame(cm.T, index=classes, columns=classes)
    cm_df.index.name = 'Predicted'
    cm_df.columns.name = 'True'
    print(cm_df)    
    print(pd.DataFrame(precision_score(y_test, pred, average=None),
                       index=classes, columns=['Precision']))        "
"regr = skl_lm.LogisticRegression()
regr.fit(X_train, y_train)"
"pred = regr.predict(X_test)
cm_df = pd.DataFrame(confusion_matrix(y_test, pred).T, index=regr.classes_,
                     columns=regr.classes_)
cm_df.index.name = 'Predicted'
cm_df.columns.name = 'True'
print(cm_df)
print(classification_report(y_test, pred))"
"pred_p = regr.predict_proba(X_test)
cm_df = pd.DataFrame({'True': y_test, 'Pred': pred_p[:,1] > .25})
cm_df.Pred.replace(to_replace={True:'Yes', False:'No'}, inplace=True)
print(cm_df.groupby(['True', 'Pred']).size().unstack('True').T)
print(classification_report(y_test, cm_df.Pred))"
schedule1.head()
schedule1.tail()
stats1
"# Combine all the scenarios into 1 view
pd.concat([stats1, stats2, stats3], ignore_index=True)"
schedule3.head()
"schedule1.plot(x='Payment_Date', y='Curr_Balance', title=""Pay Off Timeline"");"
"fig, ax = plt.subplots(1, 1)
schedule1.plot(x='Payment_Date', y='Curr_Balance', label=""Scenario 1"", ax=ax)
schedule2.plot(x='Payment_Date', y='Curr_Balance', label=""Scenario 2"", ax=ax)
schedule3.plot(x='Payment_Date', y='Curr_Balance', label=""Scenario 3"", ax=ax)
plt.title(""Pay Off Timelines"");"
"schedule1[""Cum_Interest""] = schedule1[""Interest""].abs().cumsum()
schedule2[""Cum_Interest""] = schedule2[""Interest""].abs().cumsum()
schedule3[""Cum_Interest""] = schedule3[""Interest""].abs().cumsum()

fig, ax = plt.subplots(1, 1)


schedule1.plot(x='Payment_Date', y='Cum_Interest', label=""Scenario 1"", ax=ax)
schedule2.plot(x='Payment_Date', y='Cum_Interest', label=""Scenario 2"", ax=ax, style='+')
schedule3.plot(x='Payment_Date', y='Cum_Interest', label=""Scenario 3"", ax=ax)

ax.legend(loc=""best"");"
"fig, ax = plt.subplots(1, 1)

y1_schedule = schedule1.set_index('Payment_Date').resample(""A"")[""Interest""].sum().abs().reset_index()
y1_schedule[""Year""] = y1_schedule[""Payment_Date""].dt.year
y1_schedule.plot(kind=""bar"", x=""Year"", y=""Interest"", ax=ax, label=""30 Years @ 5%"")

plt.title(""Interest Payments"");"
"fig, ax = plt.subplots(1, 1)
schedule1.plot(x='Month', y='End Balance', label=""Scenario 1"", ax=ax)
schedule2.plot(x='Month', y='End Balance', label=""Scenario 2"", ax=ax)
schedule3.plot(x='Month', y='End Balance', label=""Scenario 3"", ax=ax)
plt.title(""Pay Off Timelines"");"
"figsize(7,5)
fig, ax = plt.subplots(1, 1)
y.plot(kind=""bar"", ax=ax)

plt.legend([label1, label2, label3], loc=1, prop={'size':10})
plt.title(""Interest Payments"");"
"additional_payments = [0, 50, 200, 500]
fig, ax = plt.subplots(1, 1)

for pmt in additional_payments:
    result, _ = amortization_table(100000, .04, 30, addl_principal=pmt, start_date=date(2016,1,1))
    ax.plot(result['Month'], result['End Balance'], label='Addl Payment = ${}'.format(str(pmt)))
plt.title(""Pay Off Timelines"")
plt.ylabel(""Balance"")
ax.legend();"
"# let's take a look at the dataset (the csv I am using has a reduced number of columns) 
pd.options.display.max_columns = 0
df.head()"
df.describe() # display statistitcs from the dataset
"# convert ot a date time
date_as_series = pd.to_datetime(df.datestop.astype(str),format='%m%d%Y')
date_as_series"
"# with the timestamp as an index, you can do all sorts of nifty, accelrated operations
df['2012-6']"
df['2012-4-1':'2012-4-2']
df.frisked['2012-4-1':'2012-4-2'].sum() / float(df.frisked['2012-4-1':'2012-4-2'].count()) *100
"# Property	Description
# year	The year of the datetime
# month	The month of the datetime
# day	The days of the datetime
# hour	The hour of the datetime
# minute	The minutes of the datetime
# second	The seconds of the datetime
# microsecond	The microseconds of the datetime
# nanosecond	The nanoseconds of the datetime
# date	Returns datetime.date
# time	Returns datetime.time
# dayofyear	The ordinal day of year
# weekofyear	The week ordinal of the year
# week	The week ordinal of the year
# dayofweek	The day of the week with Monday=0, Sunday=6
# weekday	The day of the week with Monday=0, Sunday=6
# quarter	Quarter of the date: Jan=Mar = 1, Apr-Jun = 2, etc.
# is_month_start	Logical indicating if first day of month (defined by frequency)
# is_month_end	Logical indicating if last day of month (defined by frequency)
# is_quarter_start	Logical indicating if first day of quarter (defined by frequency)
# is_quarter_end	Logical indicating if last day of quarter (defined by frequency)
# is_year_start	Logical indicating if first day of year (defined by frequency)
# is_year_end	Logical indicating if last day of year (defined by frequency)

df[df.index.dayofweek==2]"
"%matplotlib inline
df['month'] = df.index.month
mcounts = df['month'].value_counts()
mcounts.sort_index().plot(kind='bar')"
"df_frisked = df[df.frisked>0]
mprct = df_frisked['month'].value_counts() / df['month'].value_counts()
mprct.sort_index().plot(kind='bar')"
"import warnings
warnings.filterwarnings('ignore')
%matplotlib inline

import pandas as pd
import seaborn as sns
import numpy as np
import matplotlib.pyplot as plt
import statsmodels.api as sm
import statsmodels.tsa.api as smt
sns.set(style='ticks', context='talk')
"
"sigma_h = 10.0
h = np.random.normal(0, sigma_h, 110)
h[0] = 0.0
a = np.cumsum(h)

df = pd.DataFrame(a[0:100], columns=['a'])
_=df.plot(figsize=(14,6), style='b--')"
"sigma_e = 15.
e = np.random.normal(0, sigma_e, 110)
df['y'] = a[0:100] + e[0:100]
_=df.plot(figsize=(14,6), style=['b--', 'g-',])"
"_=df.y.plot(figsize=(14,6), style=['g-',])"
"pd.DataFrame({'y_sim': ys.flatten(), 'a_sim': ah.flatten()}).plot(figsize=(12,6))"
"df['filtered_state'] = r.filtered_state[0][1:101]
df[['a', 'y', 'filtered_state']].plot(figsize=(14,6), style=['r--', 'g-', 'b.'])"
"p = r.predict(90, 115, dynamic=20)
s_f = pd.Series(p.results.forecasts[0][1:])
s_y = pd.Series(y)
pd.DataFrame({'y':s_y, 'f':s_f}).iloc[-25:].plot(figsize=(12,6))"
"df_indpro = pd.read_csv('./data/pydata_chicago/INDPRO.csv', parse_dates=['DATE'])
df_indpro.set_index('DATE', inplace=True)
df_indpro.head()
_ =df_indpro.plot(figsize=(10,6))"
"from statsmodels.tsa.statespace.structural import UnobservedComponents

indpro_mod = UnobservedComponents(df_indpro.INDPRO,
                                  level=True,
                                  trend=True,
                                  stochastic_level=True,
                                  stochastic_trend=True)
indpro_res = indpro_mod.fit(method='powell', disp=False)
#indpro_res.summary()"
"fig, ax = plt.subplots(figsize=(10,5))

fres = indpro_res.get_forecast('2018-06-01')
df_indpro.ix['2009-01-01':].plot(ax=ax)
fres.predicted_mean.plot()
fres_ci = fres.conf_int()
_=ax.fill_between(fres_ci.index, fres_ci['lower INDPRO'], fres_ci['upper INDPRO'], alpha=0.1)"
"_=df_esales.logUsage.plot(figsize=(12,4))"
"import statsmodels.tsa.api as smt
f=plt.figure(figsize=(12,6))
_=smt.seasonal_decompose(df_esales.Usage).plot()"
"sdiff = df_esales.logUsage.diff(12) # Seasonal diff
i1_sdiff = sdiff.dropna().diff().dropna()
fig = plt.figure(figsize=(12,8))
ax1 = fig.add_subplot(211)
fig = smt.graphics.plot_acf(i1_sdiff, lags=40, ax=ax1)
ax2 = fig.add_subplot(212)
fig = smt.graphics.plot_pacf(i1_sdiff, lags=40, ax=ax2)"
"import statsmodels.api as sm
# Variables
endog = df_esales.logUsage
exog = sm.add_constant(df_esales[['lcoal', 'lgas', 'lpetrol', 'lelec', 'DlINDPRO']])

# Fit the model
ar_mod = smt.statespace.SARIMAX(endog, order=(0,1,0), seasonal_order=(1,1,1,12))
ar_res = ar_mod.fit()
#ar_res.summary()"
"ar_p = ar_res.get_prediction('2016-03-01', '2018-03-01', dynamic=True)

fig, ax = plt.subplots(figsize=(10,6))
df_esales.logUsage.plot(ax=ax)
ar_p.predicted_mean.plot(ax=ax)
ar_p_ci = ar_p.conf_int()
_=ax.fill_between(ar_p_ci.index, ar_p_ci['lower logUsage'], ar_p_ci['upper logUsage'], alpha=0.1)"
"# Fit the model
arx_mod = smt.statespace.SARIMAX(endog[:'2014-03-01'], exog[:'2014-03-01'], order=(0,1,0), seasonal_order=(1,1,1,12))
arx_res = arx_mod.fit()
#arx_res.summary()"
"arx_p = arx_res.get_prediction('2014-03-01', '2016-03-01', exog=exog.ix['2014-05-01':],dynamic=True)

fig, ax = plt.subplots(figsize=(10,6))
df_esales.logUsage.ix[:'2014-02-01'].plot(ax=ax)
df_esales.logUsage.ix['2014-03-01':].plot(ax=ax, style='k.')
arx_p.predicted_mean.plot(ax=ax)
arx_p_ci = arx_p.conf_int()
_=ax.fill_between(arx_p_ci.index, arx_p_ci['lower logUsage'], arx_p_ci['upper logUsage'], alpha=0.1)"
"v_next = []
a_dist = np.random.normal(0, sigma_h, 1000)
a_1 = 0.
for i in range(1000):
    a_next = a_1 + np.random.normal(0, sigma_h)
    v_next.append(a_next + np.random.normal(0, sigma_e))
fig, ax = plt.subplots(figsize=(10,5))
_=sns.distplot(a_dist, color='b', label='alpha', ax=ax)
_=sns.distplot(v_next, color='g', label='vt', ax=ax)"
"from scipy.stats import binom

rv = binom(10, 0.7)
print(sum(rv.pmf(k) for k in (6,7,8)))

rv = binom(100, 0.7)
print(sum(rv.pmf(k) for k in range(60,80)))"
"N = 100
e = np.random.normal(0., 1., N)

def ar1(phi, e, n):
    y = np.zeros(n)
    y[0] = e[0]
    for i in range(1, n):
        y[i] = phi * y[i-1] + e[i]
    return y

df = pd.DataFrame({'(a) phi=0': ar1(0., e, N),
                   '(b) phi=0.5': ar1(0.5, e, N),
                   '(c) phi=0.9': ar1(0.9, e, N)})

_=df.plot(subplots=(3,1), figsize=(12,15))"
"import warnings
warnings.filterwarnings('ignore')
%matplotlib inline

import pandas as pd
import seaborn as sns
import numpy as np

df = pd.DataFrame([[j,0.8 ** j] for j in range(1,25)],
                  columns=('j', 'phi'))

df['phi2'] = [(-0.8) ** j for j in range(1, 25)]
df['phi3'] = [(1.1) ** j for j in range(1, 25)]
df['phi4'] = [(-1.1) ** j for j in range(1, 25)]"
"import matplotlib.pyplot as plt

sns.set(style=""dark"")

# Set up the matplotlib figure
f, axes = plt.subplots(2, 2, figsize=(12, 9), sharex=True)

axes[0][0].set_title(r'$\phi = 0.8$')
g = sns.barplot(x='j', y='phi', data=df, ax=axes[0][0])
axes[0][1].set_title(r'$\phi = -0.8$')
g = sns.barplot(x='j', y='phi2', data=df, ax=axes[0][1])
axes[1][0].set_title(r'$\phi = 1.1$')
g = sns.barplot(x='j', y='phi3', data=df, ax=axes[1][0])
axes[1][1].set_title(r'$\phi = -1.1$')
g = sns.barplot(x='j', y='phi4', data=df, ax=axes[1][1])

_=f.suptitle(""FIGURE 1.1 Dynamic multiplier for first-order difference equation"", y=0.01)
f.tight_layout()"
"f, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 9), sharey=True)

g = sns.barplot(x='t', y='w', data=df, ax=ax1)
g = sns.barplot(x='t', y='y', data=df, ax=ax2)

_=f.suptitle(""FIGURE 1.2 Paths of input variable (wt) and output variable (yt) for dynamic multiplier and \
present-value calculations"", y=0.01)"
"f, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 9), sharey=True)

g = sns.barplot(x='t', y='w', data=df, ax=ax1)
g = sns.barplot(x='t', y='y', data=df, ax=ax2)

_=f.suptitle(""FIGURE 1.2 Paths of input variable (wt) and output variable (yt) assumed for long-run effect"", y=0.01)"
"plt.figure(figsize=(12, 6))
g = sns.barplot(x='t', y='y', data=df)
_=plt.title(r'FIGURE 1.4 (a) $\phi1 = 0.6,$ $\phi2 = 0.2$', x=0.5, y=-0.2)"
"l1, l2"
"r = np.abs(l1)
alpha = np.real(c1i)
beta = np.imag(c1i)
theta = np.arccos(np.real(l1) / r)
r, alpha, beta, theta"
"plt.figure(figsize=(12, 6))
g = sns.barplot(x='t', y='y', data=df)
_=plt.title(r'FIGURE 1.4 (b) $\phi1 = 0.5,$ $\phi2 = -0.8$', x=0.5, y=-0.2)"
"import warnings
warnings.filterwarnings('ignore')
%matplotlib inline

import pandas as pd
import seaborn as sns
import numpy as np
import matplotlib.pyplot as plt
import statsmodels.api as sm
import statsmodels.tsa.api as smt"
"df = pd.DataFrame()
e = np.random.normal(0., 1., 1000)
df['e'] = smt.stattools.acf(e, nlags=20)
df['MA(1)'] = smt.stattools.acf(e[0:-1] + 0.8*e[1:], nlags=20)
df['MA(4)'] = smt.stattools.acf(e[0:-4] - 0.6*e[1:-3] + 0.3*e[2:-2] - 0.5 * e[3:-1] + 0.5 * e[4:] , nlags=20)
_ = df.plot(kind='bar', figsize=(12,6),subplots=(2, 2))"
"y = np.zeros(1000)
y[0] = e[0]
for i in range(1, 1000):
    y[i] = 0.8 * y[i-1] + e[i]

df['AR(1) 1'] = smt.stattools.acf(y, nlags=20)

for i in range(1, 1000):
    y[i] = -0.8 * y[i-1] + e[i]
    
df['AR(1) 2'] = smt.stattools.acf(y, nlags=20)
_ = df[['AR(1) 1', 'AR(1) 2']].plot(kind='bar', figsize=(12,6),subplots=(2, 2))"
"theta = np.arange(-3., 3., 0.1)
rho = [t / (t**2  + 1) for t in theta]

plt.figure(figsize=(20,6))
_=plt.plot(theta, rho)
_=plt.axis([-3., 3., 1.1 * np.amin(rho), 1.1 * np.amax(rho)])
_=plt.axhline(0.5, color='y')
_=plt.axhline(0.0, color='y')
_=plt.axhline(-0.5, color='y')
_=plt.axvline(1.0, ymin=0.50, ymax=0.95)
_=plt.axvline(-1.0, ymin=0.50, ymax=0.05)"
"# Add low freq oscillation
x2 = x + 20000*np.sin(t*2*np.pi*.01)

# Calculate both power spectra with single Fourier Transform (median filter the result with window size of 0.1Hz)
f,psd = f_psd(x,Fs=Fs,method='fftmed', Hzmed = .1)
f2,psd2 = f_psd(x2,Fs=Fs,method='fftmed', Hzmed = .1)

# Calculate both power spectra using Welch's method, Window size = 5 sec
f3,psd3 = f_psd(x,Fs=Fs,method='welch',welch_params={'nperseg':5000})
f4,psd4 = f_psd(x2,Fs=Fs,method='welch',welch_params={'nperseg':5000})

plt.figure(figsize=(12,6))
plt.subplot(1,2,1)
plt.loglog(f,psd,'k',label='raw')
plt.loglog(f2,psd2,'r',label='raw + linear trend')
plt.xlim((.1,200))
plt.title('One FFT',size=20)
plt.xlabel('Freq',size=20)
plt.ylabel('Power',size=20)

plt.subplot(1,2,2)
plt.loglog(f3,psd3,'k',label='raw')
plt.loglog(f4,psd4,'r',label='raw + low freq sine wave')
plt.xlim((.1,200))
plt.title('Welch',size=20)
plt.xlabel('Freq',size=20)
plt.legend(loc='best',fontsize=20)"
"# Add linear trend to data
x2 = x + .1*np.arange(len(x))

# Calculate both power spectra with single Fourier Transform (median filter the result with window size of 0.1Hz)
f,psd = f_psd(x,Fs=Fs,method='fftmed', Hzmed = .1)
f2,psd2 = f_psd(x2,Fs=Fs,method='fftmed', Hzmed = .1)

# Calculate both power spectra using Welch's method, Window size = 5 sec
f3,psd3 = f_psd(x,Fs=Fs,method='welch',welch_params={'nperseg':5000})
f4,psd4 = f_psd(x2,Fs=Fs,method='welch',welch_params={'nperseg':5000})

plt.figure(figsize=(12,6))
plt.subplot(1,2,1)
plt.loglog(f,psd,'k',label='raw')
plt.loglog(f2,psd2,'r',label='raw + linear trend')
plt.xlim((.1,200))
plt.title('One FFT',size=20)
plt.xlabel('Freq',size=20)
plt.ylabel('Power',size=20)

plt.subplot(1,2,2)
plt.loglog(f3,psd3,'k',label='raw')
plt.loglog(f4,psd4,'r',label='raw + linear trend')
plt.xlim((.1,200))
plt.title('Welch',size=20)
plt.xlabel('Freq',size=20)
plt.legend(loc='best',fontsize=20)"
"kernel_fast = np.array([0, .5, 1, .8, .4, .2, .1, 0])
kernel_slow = np.hstack([np.arange(0,1,.15),np.arange(1,0,-.04)])

plt.figure(figsize=(5,6))
plt.subplot(2,1,1)
plt.plot(kernel_fast,'k')
plt.xlim((0,30))
plt.ylabel('Neural response\n(transient)',size=15)
plt.subplot(2,1,2)
plt.plot(kernel_slow,'k')
plt.xlim((0,30))
plt.xlabel('Time (a.u.)',size=20)
plt.ylabel('Neural response\n(sustained)',size=15)"
"plt.plot(neural_pref,'k',label='preferred')
plt.plot(neural_opp,'r',label='opposite')
plt.legend(loc='best',title='Movement direction')
plt.xlabel('Time (a.u.)',size=20)
plt.ylabel('Neural response (a.u.)',size=20)
plt.xlim((0,N))

print('NOTE:')
print('Preferred direction movement: peaks of the transient and sustained responses align')
print('Opposite direction movement: transient peak preceds sustained neural response')"
"plt.plot(neural_pref,'k',label='preferred')
plt.plot(neural_opp,'r',label='opposite')
plt.plot(neural_pref_fast,'b',label='preferred fast')
plt.legend(loc='best',title='Movement direction')
plt.xlabel('Time (a.u.)',size=20)
plt.ylabel('Neural response (a.u.)',size=20)
plt.xlim((0,N))

print('NOTE:\nThe maximum of the neural response is decreased when the preferred motion is very fast')"
"ax = df.plot(
    x=""x"", y=""y"",
    kind=""line"", yerr=""Dy"", title=""Some experimetal data"", 
    linestyle="""", marker=""."",
    capthick=1, ecolor=""gray"", linewidth=1
)"
"ax = df.plot(
    x=""x"", y=""y"",
    kind=""line"", yerr=""Dy"", title=""Some experimetal data"", 
    linestyle="""", marker=""."",
    capthick=1, ecolor=""gray"", linewidth=1
)
ax = df.plot(
    x=""x"", y=""model"",
    kind=""line"", ax=ax, linewidth=1
)"
print(popt)
"a_opt, c_opt = popt
print(""a = "", a_opt)
print(""c = "", c_opt)"
"ax = df.plot(
    x=""x"", y=""y"",
    kind=""line"", yerr=""Dy"", title=""Some experimetal data"", 
    linestyle="""", marker=""."",
    capthick=1, ecolor=""gray"", linewidth=1
)
ax = df.plot(
    x=""x"", y=""model"",
    kind=""line"", ax=ax, linewidth=1
)"
"x = np.linspace(0, 20, 200)
ax = df.plot(
    x=""x"", y=""y"",
    kind=""line"", yerr=""Dy"", title=""Some experimetal data"", 
    linestyle="""", marker=""."",
    capthick=1, ecolor=""gray"", linewidth=1
)
ax.plot(x, f_model(x, a_opt, c_opt), linewidth=1)"
"nval = len(df)
del df[""model""]
Dx = [np.random.normal(0.3, 0.2) for i in range(nval)]
df[""Dx""] = Dx
df.head()"
"ax = df.plot(
    x=""x"", y=""y"",
    kind=""line"", yerr=""Dy"", xerr=""Dx"",
    title=""Some experimetal data"", 
    linestyle="""", marker=""."",
    capthick=1, ecolor=""gray"", linewidth=1
)"
"odr = ODR(data, model, [3, -2])
odr.set_job(fit_type=2)
lsq_output = odr.run()
print(""Iteration 1:"")
print(""------------"")
print(""   stop reason:"", lsq_output.stopreason)
print(""        params:"", lsq_output.beta)
print(""          info:"", lsq_output.info)
print(""       sd_beta:"", lsq_output.sd_beta)
print(""sqrt(diag(cov):"", np.sqrt(np.diag(lsq_output.cov_beta)))

# if convergence is not reached, run again the algorithm
if lsq_output.info != 1:
    print(""\nRestart ODR till convergence is reached"")
    i = 1
    while lsq_output.info != 1 and i < 100:
        print(""restart"", i)
        lsq_output = odr.restart()
        i += 1
    print(""   stop reason:"", lsq_output.stopreason)
    print(""        params:"", lsq_output.beta)
    print(""          info:"", lsq_output.info)
    print(""       sd_beta:"", lsq_output.sd_beta)
    print(""sqrt(diag(cov):"", np.sqrt(np.diag(lsq_output.cov_beta)))"
"a_lsq, c_lsq = lsq_output.beta
print(""        ODR(lsq)    curve_fit"")
print(""------------------------------"")
print(""a = %12.7f %12.7f"" % (a_lsq, a_opt))
print(""c = %12.7f %12.7f"" % (c_lsq, c_opt))"
"odr = ODR(data, model, [3, -2])
odr.set_job(fit_type=0)
odr_output = odr.run()
print(""Iteration 1:"")
print(""------------"")
print(""   stop reason:"", odr_output.stopreason)
print(""        params:"", odr_output.beta)
print(""          info:"", odr_output.info)
print(""       sd_beta:"", odr_output.sd_beta)
print(""sqrt(diag(cov):"", np.sqrt(np.diag(odr_output.cov_beta)))

# if convergence is not reached, run again the algorithm
if odr_output.info != 1:
    print(""\nRestart ODR till convergence is reached"")
    i = 1
    while odr_output.info != 1 and i < 100:
        print(""restart"", i)
        odr_output = odr.restart()
        i += 1
    print(""   stop reason:"", odr_output.stopreason)
    print(""        params:"", odr_output.beta)
    print(""          info:"", odr_output.info)
    print(""       sd_beta:"", odr_output.sd_beta)
    print(""sqrt(diag(cov):"", np.sqrt(np.diag(odr_output.cov_beta)))

# Print the results and compare to least square
a_odr, c_odr = odr_output.beta
print(""\n        ODR(lsq)    curve_fit     True ODR"")
print(""--------------------------------------------"")
print(""a = %12.7f %12.7f %12.7f"" % (a_lsq, a_opt, a_odr))
print(""c = %12.7f %12.7f %12.7f"" % (c_lsq, c_opt, c_odr))"
"x = np.linspace(0, 20, 200)
ax = df.plot(
    x=""x"", y=""y"",
    kind=""line"", yerr=""Dy"", xerr=""Dx"",
    title=""Some experimetal data"", 
    linestyle="""", marker=""."",
    capthick=1, ecolor=""gray"", linewidth=1
)
ax.plot(x, f_model(x, a_lsq, c_lsq), linewidth=1, label=""least square"")
ax.plot(x, f_model(x, a_odr, c_odr), linewidth=1, label=""ODR"")
ax.legend(fontsize=14, frameon=True)"
help(curve_fit)
"# fit to skewed distribution
sopt, scov = curve_fit(skewed, x, yn, p0=(20, 20, 1, 1))
y_fit= skewed(x, *sopt)

# fit to normal distribution
gopt, gcov = curve_fit(normpdf, x, yn, p0=(20, 20))
y_gfit = normpdf(x, *gopt)

# plot
#plt.plot(x, y, ""r-"")
plt.plot(x, yn, ""bo"", label=""data"")
plt.plot(x, y_fit, ""g"", label=""fit skewed"")
plt.plot(x, y_gfit, ""r"", label=""fit normal"")
plt.legend(loc=""upper left"", fontsize=16)

# parameters
print(""Parameters skewed :"")
print(""-------------------"")
print(""mu    :"", sopt[0])
print(""sigma :"", sopt[1])
print(""alpha :"", sopt[2])
print(""a     :"", sopt[3])
print(""\nParameters normal :"")
print(""-------------------"")
print(""mu    :"", gopt[0])
print(""sigma :"", gopt[1])"
ax1.set_ylabel?
"fig, (ax1, ax2) = plt.subplots(ncols=2, sharex=True, sharey=True, figsize=(14, 6))
x = np.linspace(-5, 5, 200)
for alpha in [0, 1, 2, 5, 10]:
    ax1.plot(x, skewed(x, mu=1, sigma=1, alpha=alpha, a=1), label=r""$\alpha$=%d"" % alpha)
ax1.set_title(""Positive skewness"")
ax1.legend(loc=""upper left"", fontsize=16)
for alpha in [0, 1, 2, 5, 10]:
    ax2.plot(x, skewed(x, mu=1, sigma=1, alpha=-alpha, a=1), label=r""$\alpha$=%d"" % -alpha)
ax2.legend(loc=""upper left"", fontsize=16)
ax2.set_title(""Negative skewness"")
ax2.set_yticklabels([])
ax2.set_xlim(-4, 6)
fig.subplots_adjust(wspace=0)"
"N = 10
df = pd.DataFrame({k: np.random.randint(low=1, high=7, size=N) for k in ""ABCD""})
df"
"df_cat = df.apply(cat_column, axis=0)
df_cat"
"df_cat = df.applymap(cat_cell)
df_cat"
df_cat.apply(pd.value_counts)
"df_percent = df_cat.apply(pd.value_counts) / nrows * 100
df_percent"
"df_percent_t = df_percent.transpose()
df_percent_t"
"df_percent_t = df_percent_t[[""defavorise"", ""neutre"", ""favorise""]]
df_percent_t"
"fig = plt.figure()
ax = fig.add_subplot(111)
df_percent_t.plot(kind=""barh"", stacked=True, ax=ax, colormap=""RdYlGn"", alpha=.8)
ax.legend(ncol=3, loc='upper center', bbox_to_anchor=(0.5, 1.15), frameon=False)
ax.set_frame_on(False)
ax.set_xticks([])

# add texts
y = 0
for index, row in df_percent_t.iterrows(): # boucle sur les lignes
    # on calcule les intervalles 
    xbounds = [0]
    for i in range(len(row)): # len(row) est le nombre d'éléments sur la ligne, 3 ici
        xbounds.append(xbounds[i] + row[i])
    print(xbounds)
    # ajout du texte au centre de chaque intervalle
    for i in range(3):
        x = (xbounds[i] + xbounds[i+1]) / 2
        ax.text(x, y, ""%3.0f%%"" % row[i], 
                verticalalignment=""center"",
                horizontalalignment=""center"")
    y += 1
"
"fig = plt.figure()
ax = fig.add_subplot(111)
df_percent_t.plot(kind=""barh"", stacked=True, ax=ax, colormap=""RdYlGn"", alpha=.8, xlim=(0, 101))
ax.legend(ncol=3, loc='upper center', bbox_to_anchor=(0.5, 1.15), frameon=False)"
"odr.set_job(fit_type=0)
lsq_output = odr.run()
print(""         stop reason:"", lsq_output.stopreason)
print(""              params:"", lsq_output.beta)
print(""                info:"", lsq_output.info)
print(""Sum of squares error:"", lsq_output.sum_square)
print(""             sd_beta:"", lsq_output.sd_beta)
print(""      sqrt(diag(cov):"", np.sqrt(np.diag(lsq_output.cov_beta)))

# check convergence and run again the algorithm if it is not reached
if lsq_output.info != 1:
    print(""\nRestart ODR till convergence is reached"")
    i = 1
    while lsq_output.info != 1 and i < 100:
        print(""restart %3d %12.7f"" % (i, lsq_output.sum_square))
        lsq_output = odr.restart()
        i += 1
    print(""         stop reason:"", lsq_output.stopreason)
    print(""              params:"", lsq_output.beta)
    print(""                info:"", lsq_output.info)
    print(""Sum of squares error:"", lsq_output.sum_square)
    print(""             sd_beta:"", lsq_output.sd_beta)
    print(""      sqrt(diag(cov):"", np.sqrt(np.diag(lsq_output.cov_beta)))"
lsq_output.pprint()
"odr = ODR(data, model, [1,0,0])
odr.set_job(fit_type=2)
odr_output = odr.run()
print(""         stop reason:"", odr_output.stopreason)
print(""              params:"", odr_output.beta)
print(""                info:"", odr_output.info)
print(""Sum of squares error:"", odr_output.sum_square)
print(""             sd_beta:"", odr_output.sd_beta)
print(""      sqrt(diag(cov):"", np.sqrt(np.diag(odr_output.cov_beta)))

# check convergence and run again the algorithm if it is not reached
if odr_output.info != 1:
    print(""\nRestart ODR till convergence is reached"")
    i = 1
    while odr_output.info != 1 and i < 100:
        print(""restart"", i)
        odr_output = odr.restart()
        i += 1
    print(""         stop reason:"", odr_output.stopreason)
    print(""              params:"", odr_output.beta)
    print(""                info:"", odr_output.info)
    print(""Sum of squares error:"", odr_output.sum_square)
    print(""             sd_beta:"", odr_output.sd_beta)
    print(""      sqrt(diag(cov):"", np.sqrt(np.diag(odr_output.cov_beta)))"
"xn = np.linspace(-3, 2, 50)

plt.errorbar(x, y, marker=""o"", linestyle="""", label=""data"", xerr=sigma_x, yerr=sigma_y, elinewidth=1, capthick=1)
plt.plot(xn, func(lsq_output.beta, xn), label='leastsq')
plt.plot(xn, func(odr_output.beta, xn), label='odr')
plt.legend()"
"# create a new plot with a title and axis labels
p = bp.figure(x_axis_label='x', y_axis_label='y')

# add a line renderer with legend and line thickness
p.circle(x, y, size=10, color=""red"")

bp.show(p)"
samples['500'].loc[163].hist();
"fig, ax = plt.subplots()
heatmap = ax.pcolormesh(accepted)

fig = plt.gcf()
fig.set_size_inches(6, 21)

plt.ylim(0,len(accepted.index))
ax.xaxis.tick_top()
ax.set_yticks(np.arange(len(accepted.index)) + 0.5, minor=False)
ax.set_yticklabels(accepted.index, minor=False)
ax.set_xticks(np.arange(len(accepted.columns)) + 0.5, minor=False)
ax.set_xticklabels(accepted.columns, minor=False)
plt.ylabel('Means')
plt.xlabel('Sample size')
ax.grid(True)"
df.head()
"%%timeit
pd.Series([
    x[1]['foo'] * x[1]['bar'] for x in df.iterrows()
])"
"%%timeit
df.apply(lambda x: x['foo'] * x['bar'], axis=1)"
"%%timeit
df.foo.mul(df.bar)"
"X = np.linspace(0,10,50)
X[:3] # sample of X"
"regr.fit(X.reshape(-1,1),Y)"
"plt.plot(
    X,Y,'*', # Data
    X, Y_pred,'-o' # Regression
)
plt.legend(['Data','Regression'])
plt.title('Input vs. Regression');"
(Y_pred[10] - Y_pred[9])/(X[10] - X[9])
regr.coef_
Y_pred[0]
regr.intercept_
"plt.plot(
    rads,np.abs(np.add(-1,coeff)), # we know that the slope we use equals 1
    rads,np.abs(inter)
)
plt.title('Error in Coeffieicents and intercepts vs. Noise radius')
plt.legend(['Coefficient error', 'Intercept error']);"
"(np.dot(X,Y)/(len(Y)) - X.mean()*Y.mean())/\
(np.dot(X,X)/len(X) - np.power(X.mean(),2))"
"Y.mean() - X.mean()* \
(np.dot(X,Y)/(len(Y)) - X.mean()*Y.mean())/\
(np.dot(X,X)/len(X) - np.power(X.mean(),2))"
regr.intercept_
"# ゼロで初期化
A = np.zeros((3,4))
print(A)"
"# 乱数で初期化(0.0から1.0の一様乱数)
A = np.random.rand(2,3)
print(A)"
"# 乱数で初期化(標準正規分布 (平均0, 標準偏差1))
A = np.random.randn(2,3)
print(A)"
"# せっかくなので標準正規分布を描いてみよう
%matplotlib inline
import matplotlib
import matplotlib.pyplot as plt

x = np.random.randn(10000)
y = plt.hist(x, bins=50) # binsは区切る個数

plt.show()"
"la, v = np.linalg.eig(A)
print(la)
print(v)
print(2/np.sqrt(5), 1/np.sqrt(5))
print(-1/np.sqrt(5), 2/np.sqrt(5))
print(v[0])
print(v[:,0])"
"np.dot(invA, A)"
"# generates a synthetic data
sample_rate = 2000
sensors_distance = [1, 2]

data = gen_truck_raw_data(
    sample_rate=sample_rate, speed=20, vehicle_layout='O--O------O-',
    sensors_distance=sensors_distance, p_signal_noise=100.0
)

data.plot()
plt.show()"
data.head()
"date_time = datetime.now()
site_id = '001'
lane_id = '01'
collection_type = 'day'  # stored per day

f_id = 'wim_{}_{}_{}_{}'.format(
    collection_type, site_id, lane_id, 
    date_time.strftime('%Y%m%d')
)

f = h5py.File('/tmp/{}.h5'.format(f_id), 'w')
print(f_id)"
"dset_id = 'run_{}_{}_{}'.format(
    site_id, lane_id, date_time.strftime('%Y%M%d_%H%M%S')
)
print(dset_id)"
"print('/tmp/{}.h5'.format(f_id))

f = h5py.File('/tmp/{}.h5'.format(f_id), 'r')"
"for dset_id in f.keys():
    dset = f[dset_id]
    
    paddle = len(max(dset.attrs, key=lambda v: len(v)))
    
    print('')
    print('='*80)
    print(dset_id)
    print('='*80)
    
    for k in dset.attrs:
        print('{}:'.format(k).ljust(paddle, ' '), dset.attrs[k], sep='\t')
        
    pd.DataFrame(dset[dset.dtype.names[1:]], index=dset['index']).plot()
    plt.show()
    
    # f.__delitem__(dset_id)"
"print('Minimum Tolerance Solver Test', end=' ... ')

data.min_tolerance = 0

%time solver_min_tolerance(data=data)

np.testing.assert_allclose(
    data.min_tolerance.values, 
    np.array([
        13.0293783341772,
        17.2176428723888,
        17.0913086307026,
        20.2663837816549
    ]), atol=1e-5
)
print('SUCCESS')"
"# show graphical visualization about the solver function to achieve the 
# mininum tolerance

# data sheet value
data_y = [
    13.0293783341772,
    17.2176428723888,
    17.0913086307026,
    20.2663837816549
]

_delta = 1e-5

lims = [
    (data_y[0]-_delta, data_y[0]+_delta),
    (data_y[1]-_delta, data_y[1]+_delta),
    (data_y[2]-_delta, data_y[2]+_delta),
    (data_y[3]-_delta, data_y[3]+_delta),
]

for k, s in data.T.items():
    _number = s['number']
    _mean = s['mean']
    _std = s['std']
    _min_confidence = s['min_confidence']
    
    _factor = stats.t.isf(0.05/2, _number-1)/np.sqrt(_number)
    _dof = _number-1

    func = lambda _min_tolerance: _min_confidence-100*(
        1-
        stats.t.sf(
            (_min_tolerance/_std-_mean/_std)-_factor, _dof)-
        stats.t.sf(
            (_min_tolerance/_std+_mean/_std)-_factor, _dof)
    )
    
    xlim = lims.pop(0)
    x = np.linspace(xlim[0], xlim[1], 1000)
    
    ax = plt.figure().gca()   
    
    data_func = pd.DataFrame({k: [func(xi) for xi in x]}, index=x)
    data_func.plot(ax=ax)
    
    ax.plot(data_y.pop(0), 0, 'o', label='excel', color='black')
    ax.plot(fsolve(func, [1]), 0, 'o', label='fsolve', color='green')
    
    ax.legend()
    
    locs, labels = plt.xticks()
    plt.setp(labels, rotation=45)
    
    plt.grid(True)
    plt.show()"
"def gen_truck_raw_data(
    sample_rate: int, speed: float, vehicle_layout: str, 
    sensors_distance: list=[],
    p_signal_noise: float=10.0
) -> pd.DataFrame:
    """"""
    if sensors_distance is None return just one signal wave
    
    """"""
    axles_distance, vehicle_lenght = (
        translate_vehicle_layout_to_distance(vehicle_layout)
    )
    
    sensors_distance.insert(0, 0)
    axles_distance.insert(0, 0)
    
    total_seconds = (sum(sensors_distance)+vehicle_lenght)/speed
    total_points = int(sample_rate*total_seconds)+1
    total_seconds = total_points/sample_rate  # correction
    
    footprint = 50/100
    
    x = np.linspace(0, total_seconds, total_points)
    
    data = {}
    shift = 0.5
    Δd_cum = 0
    
    for i, Δd in enumerate(sensors_distance):
        Δd_cum += Δd
        y = np.zeros(total_points)
        axd_cum = 0
        for axd in axles_distance:
            axd_cum += axd
            j = int(((axd_cum+shift+Δd_cum)*sample_rate)//speed)
            #print(axd_cum, j, total_points)
            width = int(600//speed)
            y[j:j+width] = gen_slope(width, c=2.5)
        
        y += np.random.random(total_points)/p_signal_noise  # noise
        y += np.random.randint(0, 100)/70  # changes baseline
        
        data[i] = y
        
    return pd.DataFrame(data, index=x)

vehicle_layout = '-O-O------O--'

data = gen_truck_raw_data(
    sample_rate=2000, 
    speed=20, 
    vehicle_layout=vehicle_layout, 
    sensors_distance=[2, 2],
    p_signal_noise=100.0
)

data.plot()
plt.grid(True)
plt.show()"
"import warnings
warnings.filterwarnings('ignore')
from IPython.display import display, HTML
import pandas as pd
import numpy as np
import io

data = pd.DataFrame({
    'A1_v': np.random.random(10), 
    'A2_v': np.random.random(10),
    'D1_v': np.random.random(10)    
})
data.index /= 5000
data.index.name = 'TIME_ss'

buffer = io.StringIO()

data.to_csv(buffer, sep=';')
buffer.seek(0)

print(buffer.read())
print('20151231')"
"rh = np.arange(0.5,1.7,0.2)
rh"
"for bt in [0.5, 0.75, 0.9, 1]:
    plt.plot(rh, bt**rh)
    plt.text(1.3, bt**1.3, r'$\beta ={}$'.format(bt))
plt.title(r'ratio $\frac{c_1}{c_0}$',fontsize=14)
plt.show()"
"c = np.arange(0,5,0.1)
for bt in [0.5,0.75, 0.95]:
    for rh in [0.3, 0.5, 0.7, 1.3, 2]:
        u =  (1/(1-rh)) * c**(1-rh)
        plt.plot(c, u)
        plt.text(5, u[-1], r'$\rho = {:3.2f}$'.format(rh), fontsize=14)
    #plt.annotate('{:3.1f}'.format(rh),xy=(50,u(50)))
plt.title(r'consumption $c_0^{RP}$')
plt.xlabel(r'$\beta$')
plt.show()"
"rh = 1/2
u = (c**(1-rh))/(1-rh)
plt.plot(c,u)"
"i = 0
err_store = []
iter_store = []
fig,axes = plt.subplots(5,4,figsize=(15,15))
for ax in axes.flatten():
    qs,num_iters,error = build_dist(N[i])
    err_store.append(error)
    iter_store.append(num_iters)
    ax.hist(qs,bins=10,histtype='stepfilled',alpha=0.25)
    ax.set_xlim([0.01,10])
    ax.set_ylim([1,100])
    ax.set_xscale('log')
    ax.set_yscale('log')
    ax.set_title(r'$N$ = %d, err = %.4f'%(num_iters,error))
    i+=1
plt.show()"
"fig = plt.figure(figsize=(8,8))
ax = fig.gca()
ax2 = ax.twinx()
ax.plot(N,err_store,'k')
ax2.plot(N,iter_store,'r')"
"i = 0
fig,axes = plt.subplots(5,4,figsize=(15,15))
plt.subplots_adjust(hspace=0.5)
for ax in axes.flatten():
    n,bins,_ = ax.hist(dists[i],bins=10,histtype='stepfilled',alpha=0.25)
    bin_centers = np.log10(np.diff(bins)/2.0+bins[0:-1])
    noise = np.where(n <= 100)
    if len(noise[0]) > 0:
        n = n[0:noise[0][0]]
        bin_centers = bin_centers[0:noise[0][0]]
    pars,covar = curve_fit(power_law_curve,bin_centers,np.log10(n))#,sigma=np.sqrt(np.log10(n)))
    pl_fit = power_law_curve(bin_centers,*pars)
    ax.plot(10**bin_centers,10**pl_fit,'--r',linewidth=2.0)
    ax.set_title(r'$N$ = %d, $\alpha = $%f'%(N[i],pars[1]),fontsize=12)
    ax.set_yscale('log')
    ax.set_xscale('log')
    ax.set_xlim([1e-2,10])
    ax.set_ylim([10,10000])
    i+=1
plt.show()"
"from matplotlib import pyplot as plt
%matplotlib inline
plt.scatter(boston.data[:, 5], boston.target, color='r')"
"x = boston.data[:, 5]
x = np.array([[v, 1] for v in x])
y = boston.target
(slope,bias), total_error, _, _ = np.linalg.lstsq(x, y)

# This is root mean squared error (RMSE)
rmse = np.sqrt(total_error[0]/len(x))
rmse"
"ax = plt.subplot(111)
fs = 18
ax.arrow(.6,.6,0,-.4,head_width=0.03, head_length=0.1, fc='k', ec='k')
ax.set_xlabel(r'$x$',fontsize=fs)
ax.set_ylabel(r'$y$',fontsize=fs)
ax.text(.7,.6,r'$\vec{g}=-g\hat{y}$',fontsize=fs+2)"
"#First define a vector for time
texact = np.linspace(0,2,1000)
#Now define a constant for the initial velocity
v0 = 10 #in m/s
#Next, define the gravitational acceleration
g = -9.8 #in m/s
#Construct the velocity vector
vexact = g*texact + v0
#Plot for velocity with constant acceleration
fig = plt.figure(figsize=(12,10))
ax = fig.gca()
ax.plot(texact,vexact)
ax.set_xlabel(r'$t$',fontsize=fs)
ax.set_ylabel(r'$v$',fontsize=fs)
ax.set_title(r'Velocity with Constant Acceleration',fontsize=fs)"
"#Define our formula for the exact position in time
yexact = y0 + v0*texact + g/2*texact**2
fig, axes = plt.subplots(1,2,figsize=(12,12))
axes[0].plot(t,vy,'.')
axes[0].plot(texact,vexact,'r--')
axes[0].set_xlabel(r'$t$',fontsize=fs)
axes[0].set_ylabel(r'$v_y$',fontsize=fs)
axes[0].set_title('Velocity with Constant Acceleration',fontsize = fs)
axes[1].plot(t,y,'.',label='Numerical')
axes[1].plot(texact,yexact,'r--',label='Exact')
axes[1].set_xlabel(r'$t$',fontsize=fs)
axes[1].set_ylabel(r'$y$',fontsize=fs)
axes[1].set_title(r'Position with Constant Acceleration',fontsize=fs)
axes[1].legend(loc=1)"
"#Now plot the results
#Define our formula for the exact position in time
texact = np.linspace(ta,tb,1000)
yexact = y0 + v0*np.sin(theta)*texact + g/2*texact**2
xexact = v0*np.cos(theta)*texact
vxexact = v0*np.cos(theta)*np.ones([len(texact),1])
fig, axes = plt.subplots(3,1,figsize=(12,12))
plt.subplots_adjust(hspace=0.5)
axes[0].plot(t,vx,'.')
axes[0].plot(texact,vxexact,'r--')
axes[0].set_xlabel(r'$t$',fontsize=fs)
axes[0].set_ylabel(r'$v_x$',fontsize=fs)
axes[0].set_title(r'Velocity with Constant Acceleration, $x$',fontsize = fs)
axes[0].set_xlim([0,texact[-1]])
axes[1].plot(t,x,'.',label='Numerical')
axes[1].plot(texact,xexact,'r--',label='Exact')
axes[1].set_xlabel(r'$t$',fontsize=fs)
axes[1].set_ylabel(r'$x$',fontsize=fs)
axes[1].set_title(r'Position with Constant Acceleration, $x$',fontsize=fs)
axes[1].set_xlim([0,texact[-1]])
axes[1].legend(loc=4)
axes[2].plot(x,y,'.')
axes[2].plot(30*np.cos(35*np.pi/180)*texact,yexact,'--r')
axes[2].plot(xexact,np.zeros([len(xexact),1]),'k--')
axes[2].arrow(45,9,0,-5,head_width=0.5, head_length=1, fc='k', ec='k')
axes[2].text(50,9,r'$\vec{g}=-g\hat{y}$',fontsize=fs+2)
axes[2].set_xlabel(r'$x$',fontsize=fs)
axes[2].set_ylabel(r'$y$',fontsize=fs)
axes[2].set_title('Position',fontsize = fs)
axes[2].set_xlim([0,xexact[-1]])"
"#Plot the results
#Do the formatting first
fig = plt.figure(figsize=(12,12))
ax1 = plt.subplot2grid((3,2),(0,0))
ax2 = plt.subplot2grid((3,2),(0,1))
ax3 = plt.subplot2grid((3,2),(1,0))
ax4 = plt.subplot2grid((3,2),(1,1))
ax5 = plt.subplot2grid((3,2),(2,0),colspan=2)
fig.tight_layout(h_pad=2.5,w_pad=2.5)
#Plot x,y,vx,vy
ax1.plot(t,x)
ax1.set_ylabel(r'$x$',fontsize=fs)
ax2.plot(t,vx)
ax2.set_ylabel(r'$v_x$',fontsize=fs)
ax3.plot(t,y)
ax3.set_xlabel(r'$t$',fontsize=fs)
ax3.set_ylabel(r'$y$',fontsize=fs)
ax4.plot(t,vy)
ax4.set_xlabel(r'$t$',fontsize=fs)
ax4.set_ylabel(r'$v_y$',fontsize=fs)
ax5.plot(x,y)
ax5.set_xlabel(r'$x$',fontsize=fs)
ax5.set_ylabel(r'$y$',fontsize=fs)
ax5.set_xlim([x[0],x[-1]])"
"TOOLS=""save,hover,crosshair,pan,wheel_zoom,box_zoom,reset,tap,box_select,poly_select,lasso_select""

colors2 = [""#%02x%02x%02x"" % (int(r), int(g), 150) for r, g in zip(50+2*x, 30+2*y)]
p1 = figure(width=300, height=300, tools=TOOLS)
p1.scatter(x,y, radius=radii, fill_color=colors2, fill_alpha=0.6, line_color=None)

colors2 = [""#%02x%02x%02x"" % (150, int(g), int(b)) for g, b in zip(50+2*x, 30+2*y)]
p2 = figure(width=300, height=300, tools=TOOLS)
p2.scatter(x,y, radius=radii, fill_color=colors2, fill_alpha=0.6, line_color=None)"
"plt.figure(figsize=(w,h))
x_para = np.linspace(-3000,3000)
y_para = x_para/1e6
plt.plot(x_para,y_para,linewidth=2)
plt.title('Ideal Paramagnet',size=30)
plt.ylabel('M(emu)',size=25)
plt.xlabel('H(Oe)',size=25)
plt.grid()
plt.show()"
"plt.figure(figsize=(w,h))
x_dia = np.linspace(-3000,3000)
y_dia = -x_para/1e5
plt.plot(x_dia,y_dia,linewidth=2)
plt.title('Ideal Diamagnet',size=30)
plt.ylabel('M(emu)',size=25)
plt.xlabel('H(Oe)',size=25)
plt.grid()
plt.show()"
"plt.figure(figsize=(w,h))
sat = 12
coer = 6
rem = 4
plt.plot([sat,-coer],[rem,rem],c='r')
plt.plot([-coer,-coer],[rem,-rem],c='r')
plt.plot([-sat,coer],[-rem,-rem],c='r')
plt.plot([coer,coer],[-rem,rem],c='r')

plt.plot([0,0],[-20,20],c='k',linewidth=2)
plt.plot([-20,20],[0,0],c='k',linewidth=2)

plt.xlim(-sat-2,sat+2)
plt.ylim(-rem-1,rem+1)

plt.text(coer,0.2,r'H$_{coer}$',size=25)
plt.text(-coer,0.2,r'-H$_{coer}$',size=25)
plt.text(0.1,rem+0.2,r'M$_{rem}$',size=25)
plt.text(0.1,-rem+0.3,r'M$_{rem}$',size=25)

#plt.plot(x_dia,y_dia,linewidth=2)
plt.title('Ideal Ferromagnet',size=30)
plt.ylabel('M(emu)',size=25)
plt.xlabel('H(Oe)',size=25)
plt.grid()
plt.show()"
"graph1,Fr1,F1 = grafica_Forc(data1)
plt.show(graph1)"
"fig = plt.figure(figsize=(w-1,h-1))
plt.scatter(Fr1[:,0],Fr1[:,1],s=10,c='r',label=r'H$_\beta$')
plt.scatter(F1[:,0],F1[:,1],s=10,c='b',label=r'H$_\alpha$')
plt.title('Floppy Disk 50',size=30)
plt.ylabel('M(emu)',size=25)
plt.xlabel('H(Oe)',size=25)
plt.grid()
plt.legend(loc=0)
plt.show()"
"graph2,Fr2,F2 = grafica_Forc(data2)
plt.show(graph2)"
"fig = plt.figure(figsize=(w-1,h-1))
plt.scatter(Fr2[:,0],Fr2[:,1],s=10,c='r',label=r'H$_\beta$')
plt.scatter(F2[:,0],F2[:,1],s=10,c='b',label=r'H$_\alpha$')
plt.title('Floppy Disk 200',size=30)
plt.ylabel('M(emu)',size=25)
plt.xlabel('H(Oe)',size=25)
plt.grid()
plt.legend(loc=0)
plt.show()"
"graph3,Fr3,F3 = grafica_Forc(data3)
plt.show(graph3)"
"fig = plt.figure(figsize=(w-1,h-1))
plt.scatter(Fr3[:,0],Fr3[:,1],s=10,c='r',label=r'H$_\beta$')
plt.scatter(F3[:,0],F3[:,1],s=10,c='b',label=r'H$_\alpha$')
plt.title('Floppy Disk 400',size=30)
plt.ylabel('M(emu)',size=25)
plt.xlabel('H(Oe)',size=25)
plt.grid()
plt.legend(loc=0)
plt.show()"
"G = nx.Graph()
G.add_nodes_from(range(6))
G.add_edges_from([(1,2),(1,3),(1,4),(2,3),(2,5),(5,4)])
nx.draw(G, with_labels=True)"
"open_triangles=get_open_triangles(G,5)
print(""There are {} open triangles:"".format(len(open_triangles)))
print(open_triangles)
draw(open_triangles)"
"iris = load_iris()
iris"
"plt.hist(data)
plt.title('plt.hist(xs)', fontsize=30, fontname='Courier New', y=1.05)
plt.show()"
"plt.boxplot(data)
plt.title('plt.boxplot(xs)', fontsize=30, fontname='Courier New', y=1.05)
plt.show()"
"plt.violinplot(data)
plt.title('plt.violinplot(xs)', fontsize=30, fontname='Courier New', y=1.05)
plt.show()"
"plt.scatter(xs, ys)
plt.title('plt.scatter(xs, ys)', fontsize=30, fontname='Courier New', y=1.05)
plt.show()"
"plt.bar(xs, ys)
plt.title('plt.bar(xs, ys)', fontsize=30, fontname='Courier New', y=1.05)
plt.show()"
"plt.plot(xs, ys)
plt.title('plt.plot(xs, ys)', fontsize=30, fontname='Courier New', y=1.05)
plt.show()"
"fig, ax = plt.subplots(1, figsize=(5, 5))
data = df[df['dataset']=='I']
ax.scatter(data['x'], data['y'], s=80, edgecolor='none', facecolor='maroon', alpha=0.8)
ax.set_yticks([0, 5, 10, 15])
ax.set_xlim([0, 20])
ax.set_ylim([0, 15])
ax.axhline(linewidth=10, c='black')
ax.axvline(linewidth=10, c='black')
for tick in ax.xaxis.get_major_ticks():
    tick.label.set_fontsize(14)
for tick in ax.yaxis.get_major_ticks():
    tick.label.set_fontsize(14)
plt.show()"
"fig, ax = plt.subplots(1, figsize=(5, 5))
data = df[df['dataset']=='II']
ax.scatter(data['x'], data['y'], s=80, edgecolor='none', facecolor='#006F6F', alpha=0.8)
ax.set_yticks([0, 5, 10, 15])
ax.set_xlim([0, 20])
ax.set_ylim([0, 15])
ax.axhline(linewidth=10, c='black')
ax.axvline(linewidth=10, c='black')
for tick in ax.xaxis.get_major_ticks():
    tick.label.set_fontsize(14)
for tick in ax.yaxis.get_major_ticks():
    tick.label.set_fontsize(14)
plt.show()"
"fig, ax = plt.subplots(1, figsize=(5, 5))
data = df[df['dataset']=='III']
ax.scatter(data['x'], data['y'], s=80, edgecolor='none', facecolor='indigo', alpha=0.8)
ax.set_yticks([0, 5, 10, 15])
ax.set_xlim([0, 20])
ax.set_ylim([0, 15])
ax.axhline(linewidth=10, c='black')
ax.axvline(linewidth=10, c='black')
for tick in ax.xaxis.get_major_ticks():
    tick.label.set_fontsize(14)
for tick in ax.yaxis.get_major_ticks():
    tick.label.set_fontsize(14)
plt.show()"
"fig, ax = plt.subplots(1, figsize=(5, 5))
data = df[df['dataset']=='IV']
ax.scatter(data['x'], data['y'], s=80, edgecolor='none', facecolor='darkgreen', alpha=0.8)
ax.set_yticks([0, 5, 10, 15])
ax.set_xlim([0, 20])
ax.set_ylim([0, 15])
ax.axhline(linewidth=10, c='black')
ax.axvline(linewidth=10, c='black')
for tick in ax.xaxis.get_major_ticks():
    tick.label.set_fontsize(14)
for tick in ax.yaxis.get_major_ticks():
    tick.label.set_fontsize(14)
plt.show()"
"fig, ax = plt.subplots(1)

# Hack to get colorbar (as there is no data)
sm = cm.ScalarMappable(cmap=cm.viridis)
sm._A = []

ax.set_xticks([1, 2, 3])

ax.set_xlim(0, 4)
ax.set_ylim(0, 1)

ax.set_xticklabels([1, 'Two', 200], fontsize=12)

ax.set_ylabel('Y-Label', fontsize=16)

ax.set_title('My Title', fontsize=20)

ax.text(2, 0.7, 'An annotation', fontsize=20)

ax.scatter([1, 1, 2], [0.5, 0.2, 0.1])
fig.colorbar(sm)

plt.show()"
"fig, axarr = plt.subplots(2, 2)

axarr[0, 0].scatter([1.5, 2, 3, 3.5, 3.8, 4.5], [3, 1, 4, 5.5, 2, 1])
axarr[0, 1].hist([random.lognormvariate(0.1, 0.5) for _ in range(250)])
axarr[1, 0].plot([1, 2, 3, 4, 5, 6, 7, 8, 9, 11], [7, 5, 3, 4, 5, 2, 3, 1, 2, 2])
axarr[1, 1].violinplot([random.expovariate(0.2) for _ in range(30)])

plt.show()"
"fig, ax = plt.subplots(1)
ax.plot(dates, gold, c='darkgoldenrod', zorder=1, label='Gold Medal')
ax.plot(dates, silver, c='slategray', zorder=1, label='Silver Medal')
ax.plot(dates, bronze, c='sienna', zorder=1, label='Bronze Medal')

ax.scatter(usa_x, usa_y,
           facecolor='black', marker='*', s=100,
           zorder=2, label='USA Athletes')

plt.legend()

ax.set_xticks(dates)
ax.set_xticklabels(dates, rotation='vertical')

ax.set_xlabel(""Year"")
ax.set_ylabel(""Time"")
ax.set_title(""Women's 200m Olympic Medalists"", fontsize=18)

plt.tight_layout()
plt.show()"
"fig, ax = plt.subplots(1)

hm = ax.pcolor(z, cmap='inferno', zorder=0)

ticks = np.linspace(-2, 2, 9)
ax.set_xticks([i*5 + 0.5 for i in range(9)])
ax.set_yticks([i*5 + 0.5 for i in range(9)])
ax.set_xticklabels(ticks)
ax.set_yticklabels(ticks)

ax.grid(which='major', color='darkslategrey',
        linestyle=':', linewidth=1, axis='both')

cbar = fig.colorbar(hm)
cbar.set_label(r""$f(x, y)$"")

ax.set_title(r""$-\left(x^2+3y^2\right)e^{-x^2-y^2}$"",
             fontsize=18, y=1.02)
ax.set_xlabel(r""$x$"", fontsize=16)
ax.set_ylabel(r""$y$"", fontsize=16)

plt.tight_layout()
plt.show()"
"fig, ax = plt.subplots(1)

ax.bar([1, 4, 7], male_survived,
       width=0.9, color='darkorange', label='Male')
ax.bar([2, 5, 8], male_died,
       width=0.9, color='darkorange')
ax.bar([1, 4, 7], female_survived,
       width=0.9, color='seagreen', bottom=male_survived,
      label='Female')
ax.bar([2, 5, 8], female_died,
       width=0.9, color='seagreen', bottom=male_died)

ax.set_xlim(0, 9)
plt.xticks([1, 2, 4, 5, 7, 8],
           ['Survived', 'Died']*3, rotation='vertical')
ax.set_xticks([3, 6], minor=True)
ax.xaxis.grid(False, which='major')
ax.xaxis.grid(True, which='minor')
ax2 = ax.twiny()

plt.sca(ax2)
plt.xticks([1.5, 4.5, 7.5],
           [""1st Class"", ""2nd Class"", ""3rd Class""], fontsize=12)
ax2.set_xlim(0, 9)
ax2.xaxis.grid(False, which='both')

ax.legend(loc=0, fontsize=14)
ax.set_ylabel(""Frequency"")
ax.set_title(""Breakdown of Titanic Survivors by Class and Gender"",
              fontsize=16, y=1.15)
plt.tight_layout()

plt.show()"
"fig, ax = plt.subplots(1)

# Hack to get colorbar (as we dont have a mappable, we are applying colour directly to the boxplots)
sm = plt.cm.ScalarMappable(cmap=cm.viridis)
sm._A = []


maxy = max([max(data) for data in heights])
miny = min([min(data) for data in heights])


for i, data in enumerate(heights):
    plot_colour = cm.viridis((np.mean(data)-miny)/(maxy-miny))
    meanpointprops = dict(marker='*',markeredgecolor=plot_colour,
                          markerfacecolor=plot_colour, markersize=15)
    bp = ax.boxplot(data, positions=[i+0.5], widths=0.8, meanprops=meanpointprops,
                    showmeans=True)
    vp = ax.violinplot(data, positions=[i+0.5], widths=0.8,
                       showmeans=False, showextrema=False, showmedians=False)
    plt.setp(bp['boxes'], color=plot_colour)
    plt.setp(bp['whiskers'], color=plot_colour)
    plt.setp(bp['medians'], color=plot_colour)
    plt.setp(bp['fliers'], color=plot_colour)
    plt.setp(bp['caps'], color=plot_colour)
    plt.setp(vp['bodies'], color=plot_colour)


ax.set_xticks([i+0.5 for i in range(4)])
ax.set_xticklabels(voices, fontsize=12)

ax.set_ylabel(""Height in Inches"")
ax.set_title(""Heights of Singers in the New York Choral Society"",
             fontsize=18)
ax.set_xlim(0, 4)

cbar = fig.colorbar(sm, alpha=0.3)
cbarzero = ((maxy - miny)/2)+miny
cbar.set_ticks([0, 0.5, 1])
cbar.ax.set_yticklabels([miny, cbarzero, maxy])
cbar.set_label(""Average Height"")

plt.tight_layout()
plt.show()"
"import warnings
warnings.filterwarnings('ignore')
'''
author: Alvason Zhenhua Li
date:   04/16/2015

Home-made machinery for sorting a list from min-max
'''
import numpy as np


%matplotlib inline

import numpy as np
import matplotlib.pyplot as plt
###############
# time-watching and progress-bar
class TimeWatch(object):
    def __init__(cell):
        import time  
        cell.start_time = time.time()
    
    def progressBar(cell, starting , current_step, stopping):
        progressing = float(current_step - starting) / (stopping - starting) 
        from IPython.core.display import clear_output
        clear_output(wait = True) 
        import time 
        current_time = time.time()
        print('[{:6.6f} second {:} {:}% {:}]'.format(current_time - cell.start_time
                                              , int(10 * progressing) * '--'
                                              , int(100 * progressing)
                                              , int(10 - 10 * progressing) * '++'))
    def runTime(cell):
        import time 
        current_time = time.time()
        total_time = current_time - cell.start_time
        print('[running time = {:6.6f} second]'.format(total_time))
        return total_time
###############
import datetime
previous_running_time = datetime.datetime.now()
print ('Previous running time is {:}'.format(previous_running_time))"
"var(""m1 m2 J1 J2 L1 L2 l1 l2 t g"")"
"x1 = l1*cos(q1)
y1 = l1*sin(q1)
v1 = x1.diff(""t"")**2 + y1.diff(""t"")**2
v1.trigsimp()"
"x2 = L1*cos(q1) + l2*cos(q1 + q2)
y2 = L1*sin(q1) + l2*sin(q1 + q2)
v2 = x2.diff(""t"")**2 + y2.diff(""t"")**2
v2.trigsimp()"
"K1 = Rational(1, 2)*m1*v1 + Rational(1, 2)*J1*ω1**2
K1"
"K2 = Rational(1, 2)*m1*v2 + Rational(1, 2)*J2*ω2**2
K2"
"U1 = m1*g*y1
U1"
"U2 = m2*g*y2
U2"
"K = K1 + K2
K"
"U = U1 + U2
U"
"L = (K - U).expand().simplify()
L"
(L.diff(q1.diff(t))).simplify().expand().collect(q1.diff(t).diff(t)).collect(q2.diff(t).diff(t))
(L.diff(q1.diff(t)).diff(t)).simplify().expand().collect(q1.diff(t).diff(t)).collect(q2.diff(t).diff(t))
(L.diff(q1)).simplify().expand().collect(q1.diff(t).diff(t)).collect(q2.diff(t).diff(t))
(L.diff(q2.diff(t))).simplify().expand().collect(q1.diff(t).diff(t)).collect(q2.diff(t).diff(t))
(L.diff(q2.diff(t)).diff(t)).simplify().expand().collect(q1.diff(t).diff(t)).collect(q2.diff(t).diff(t))
(L.diff(q2)).simplify().expand().collect(q1.diff(t).diff(t)).collect(q2.diff(t).diff(t))
τ1
τ2
"x=np.linspace(-5,10,100)
f = lambda x : 0.5*x**2-3
f1 = lambda x : x
def ftayl(a):
    def fa(x):
        return f(a)+(x-a)*f1(a)
    return fa

fig=plt.figure(0,figsize=(8,8))
ax=fig.add_subplot(111)
plt.title(""Overview of newton method"")
plt.xlabel('X axis')
plt.ylabel('Y axis')
ax.grid(True, which='both')
ax.axhline(y=0, color='k')
ax.axvline(x=0, color='k')
plt.xlim(0,12)
plt.ylim(-5,50)

#Plot original function
plt.plot(x,f(x))

#Initial estimate
xi = 8

for i in range(4):
    #Plot estimate
    plt.scatter(xi,0)
    plt.plot([xi,xi],[0,f(xi)],'k')
    ax.annotate('$x_'+str(i)+'$',(xi,0))
    #plot linearization in xi
    plt.plot(x,ftayl(xi)(x),'g')
    #Compute next estimate
    xi = xi-f(xi)/f1(xi)"
"import warnings
warnings.filterwarnings('ignore')
import sys
sys.path #shows how imports ale looked"
"print(id(t))
print(hash(t))
print(id(li))
# mutable keeps id but can change id
# print(hash(li)) #unhashable"
"import warnings
warnings.filterwarnings('ignore')
%matplotlib inline
import numpy as np
import matplotlib.pyplot as plt

dist1 = np.random.normal(loc=5, scale=1, size=10000)
dist2 = np.random.normal(loc=7, scale=0.5, size=10000)

d = dist1
h, e = np.histogram(a=d, bins=100,range=(0,10))
chist = np.cumsum(h)

def calc_hist(d):
    h, e = np.histogram(a=d, bins=100,range=(0,10), normed=True)
    chist = np.cumsum(h)
    return e, h, chist

e1, h1, c1 = calc_hist(dist1)
e2, h2, c2 = calc_hist(dist2)

fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))

ax1.plot(e1[:-1], h1, label='d1')
ax1.plot(e2[:-1], h2, label='d2')

ax2.plot(e1[:-1], c1)
ax2.plot(e2[:-1], c2)


ax1.set_ylabel('probability density function')
ax2.set_ylabel('cumulative density function')


ax1.text(s='$\mu_1=5; \sigma_1=1$'+'\n'+'$\mu_2=7; \sigma_2=0.5$', x=1, y=0.7, fontsize=15)
ax1.legend()"
"institution_id = [n for n in np.arange(10)]
question_id = [n for n in np.arange(1, 16, 1)]
data = np.array([x for x in product(institution_id, question_id)])
df = pd.DataFrame(data, columns=['institution_id', 'question_id'])
df['response'] = np.random.randint(0, 2, size=df.shape[0])
df.head()"
df.describe()
"df_pivot = df.pivot(index='institution_id', columns='question_id', values='response')
df_pivot"
"x = np.arange(df_pivot.columns.min()-1, df_pivot.columns.max() + 1, 1)
y = np.arange(df_pivot.index.min()-1, df_pivot.index.max() + 1, 1)

X, Y = np.meshgrid(x, y)
z = df_pivot.values

fig, ax = plt.subplots()

cbar = ax.pcolormesh(X, Y, z, cmap='Greys')

fig.colorbar(cbar, )

ax.set_xlabel('question id')
ax.set_ylabel('institution id')

# ax.set_ylim(-1, 10)

ax.set_xticks(x + 0.5)
ax.set_xticklabels(x+1)
ax.grid(True)

ax.set_xlim(0, 15)
"
"from numpy import *
import matplotlib.pyplot as plt
x = arange(0.,2 * pi,0.001)
y = sin(x)
y2 = abs(sin(x))
plt.plot(x,y)
plt.plot(x,y2)
plt.show()
sqrt(mean(square(y))) == sqrt(mean(square(y2))) # True
sqrt(mean(square(y2))) "
"fig = plt.figure()
ax = fig.add_subplot(111)
ax.plot(range(3))
#fig.savefig('temp.png')
fig.show()"
"
import numpy as np
def f(z,t):
    return np.exp(-z)*np.sin(t-z)

z = np.linspace(0,5,100)
t = np.arange(0,40000,4000)

for tval in t:
    plt.plot(z, f(z, tval))
plt.show()
t
len(z)"
"# visualization of the density
pyplot.figure(figsize=(16,8))
pyplot.subplot(131)
pyplot.grid(True)
pyplot.title('Density (kg/m^3)')
pyplot.plot(x,y0[:,0]);

# visualization of density*velocity
pyplot.subplot(132)
pyplot.grid(True)
pyplot.title('Propellant mass (kg)')
pyplot.plot(x,y0[:,1]);

# visualization of densoi
pyplot.subplot(133)
pyplot.grid(True)
pyplot.title('Total mass (kg)')
pyplot.plot(x,y0[:,2]);"
"# visualization of the density
pyplot.figure(figsize=(16,8))
pyplot.subplot(131)
pyplot.grid(True)
pyplot.title('rho')
pyplot.plot(x,y[:,0]);

# visualization of density*velocity
pyplot.subplot(132)
pyplot.grid(True)
pyplot.title('rho*u')
pyplot.plot(x,y[:,1]);

# visualization of densoi
pyplot.subplot(133)
pyplot.grid(True)
pyplot.title('rho*e_t')
pyplot.plot(x,y[:,2]);"
"# Model parameters
Vmax = 80
L = 11
rho_max = 250

# Numerical parameters
nx = 51
dt = 0.001
t_final = 6/60

# Driven numerical parameters
nt = int(t_final/dt)
dx = L/(nx-1)

# Initial condition
x = numpy.linspace(0,L,nx)
rho0 = numpy.ones(nx)*10
rho0[10:20] = 50

rho_array = traffic_model_iteration(rho0,nx,nt,dt,dx,Vmax,rho_max)"
"# Model parameters
Vmax = 136
L = 11
rho_max = 250

# Numerical parameters
nx = 51
dt = 0.001
t_final = 6/60

# Driven numerical parameters
nt = int(t_final/dt)
dx = L/(nx-1)

# Initial condition
x = numpy.linspace(0,L,nx)
rho0 = numpy.ones(nx)*20
rho0[10:20] = 50

rho_array = traffic_model_iteration(rho0,nx,nt,dt,dx,Vmax,rho_max)"
"plt.plot(n_e, h)
plt.semilogx()
plt.grid()
plt.xlabel(r""$n_e / m^{-3}$"")
plt.ylabel(r""$h / km$"")"
"plt.plot(n_e[:N // 3], h[:N // 3])
plt.semilogx()
plt.grid()
plt.xlabel(r""$n_e / m^{-3}$"")
plt.ylabel(r""$h / km$"")"
"fig = plt.figure(figsize=(12.80, 7.20))
fig.subplots_adjust(left=0, right=1, bottom=0, top=1)
ax = fig.add_subplot(
    111,
    aspect=""equal"",
    autoscale_on=False,
    projection=""3d"",
)
ax.grid()

axlim = 50

ax.set_xlim3d(-axlim, axlim)
ax.set_ylim3d(-axlim, axlim)
ax.set_zlim3d(-axlim, axlim)
ax.set_xlabel(""$x$"")
ax.set_ylabel(""$y$"")
ax.set_zlabel(""$z$"")"
"for csv_filename in glob(""*.csv""):
    plt.plot(
        *np.loadtxt(csv_filename, unpack=True),
        ""."" if csv_filename.startswith(""cont"") else ""x"",
        label=csv_filename
    )

for step_length in 2, 3, 4:
    plt.plot(steps, steps * step_length ** 2, label=""Fit {}"".format(step_length))

plt.grid()
plt.legend(loc=""best"")"
"%matplotlib inline
from tests.test_infotaxis import show_hit_rate_map_for_example_parameters

show_hit_rate_map_for_example_parameters()"
"%matplotlib inline
from tests.test_infotaxis import show_update_log_p_src_gives_correct_qualitative_behavior_for_examples

show_update_log_p_src_gives_correct_qualitative_behavior_for_examples()"
"%matplotlib inline
from tests.test_infotaxis import show_infotaxis_demo

show_infotaxis_demo()"
"# Random matrix with standard Gaussian entries
#   NOTE: argument is NOT a tuple
Q = np.random.randn(4, 4)

print(Q)
print(Q[:, 1]) # Second column (everything is 0-indexed)
print(Q[2, 3]) # (3, 4) entry (as a real number)"
"# Random column vector of length 4
v = np.random.randn(4, 1)

# v.T: v tranpose
# @: matrix multiplication
z = v.T @ Q @ v

# The result is a 1-by-1 matrix
print(z)

# Extract the result as a real number
print(z[0, 0])"
"# Can use custom namespace name different from the module name
# This imports random but uses 'rn' namespace
import random as rn
for i in range(5):
    # print a random number in [0, 1)
    print(rn.random())"
"import warnings
warnings.filterwarnings('ignore')
# matplotlib package has the plotting functionality of MATLAB
import matplotlib.pyplot as plt

# In a Jupyter notebook, include the command
%matplotlib inline
# to display graphs in the notebook.

# plt.plot generates a line on the graph from a list of
# x-coordinates and a list of y-coordinates.
x = range(101)
y1 = [xi**2 for xi in x]
# Use labels to name lines in the graph.
plt.plot(x, y1, label=""x^2"")

# Call plt.plot multiple times to plot multiple lines.
y2 = [xi**(1.5) for xi in x]
# Labels can be latex code.
plt.plot(x, y2, label=r""$x^{1.5}$"")

# plt.xlabel and plt.ylabel assign names to the axes.
plt.xlabel(r""$x$"", fontsize=16)
# plt.legend adds a legend to the graph with the line labels.
plt.legend(loc='upper left')

# plt.show displays the graph.
plt.show()"
"import warnings
warnings.filterwarnings('ignore')
import numpy
%matplotlib inline
import matplotlib.pyplot as plt
import scipy.stats

def make_continuous_data(mean=[45,100],var=[10,10],cor=0.6,N=100):
    """"""
    generate a synthetic data set with two variables
    """"""
    cor=numpy.array([[1.,cor],[cor,1.]])
    var=numpy.array([[var[0],0],[0,var[1]]])
    cov=var.dot(cor).dot(var)
    return numpy.random.multivariate_normal(mean,cov,N)

n=25
d=make_continuous_data(N=n)
plt.scatter(d[:,0],d[:,1])
plt.xlabel('age')
plt.ylabel('processing speed')
print ('r=',numpy.corrcoef(d.T)[0,1])"
"y=d[:,1]
X=numpy.vstack((d[:,0]-numpy.mean(d[:,0]),numpy.ones(d.shape[0]))).T
sigma2=1  # this is the variance - just set to 1 for this example

priorvals=10.**numpy.arange(-8,8)

bhat_bayes=numpy.zeros((len(priorvals),2))

bhat_glm=numpy.linalg.inv(X.T.dot(X)).dot(X.T.dot(y))
resid=y - X.dot(bhat_glm)
df=(X.shape[0] - X.shape[1])
mse=resid.dot(resid)
sigma2hat=(mse)/float(df)

print ('beta_hat (GLM):',bhat_glm)
for i in range(len(priorvals)):
    prior_variance=priorvals[i]
    v=numpy.identity(2)*prior_variance 


    bhat_bayes[i,:]=numpy.linalg.inv(X.T.dot(X) + (sigma2hat/prior_variance)*numpy.identity(2)).dot(X.T.dot(y))

plt.figure(figsize=(12,5))
plt.subplot(121)
plt.plot(range(len(priorvals)),bhat_bayes[:,0])
plt.xticks(range(len(priorvals)),priorvals, rotation='vertical')
plt.xlabel('prior variance')
plt.ylabel('parameter estimate')
plt.plot([0,len(priorvals)],[bhat_glm[0],bhat_glm[0]],color='green')
plt.legend(['Bayesian estimate','OLS estimate'],loc=4)
plt.subplot(122)
plt.plot(range(len(priorvals)),bhat_bayes[:,1])
plt.xticks(range(len(priorvals)),priorvals, rotation='vertical')
plt.xlabel('prior variance')
plt.ylabel('parameter estimate')
plt.plot([0,len(priorvals)],[bhat_glm[1],bhat_glm[1]],color='green')
plt.legend(['Bayesian estimate','OLS estimate'],loc=4)"
"%matplotlib inline
from IPython.core.pylabtools import figsize
import numpy as np
import numpy
from matplotlib import pyplot as plt
figsize(11, 9)

import scipy.stats as stats

dist = stats.beta
n_trials = [0, 1, 2, 3, 4, 5, 8, 15, 50, 500]
data = stats.bernoulli.rvs(0.5, size=n_trials[-1])
x = np.linspace(0, 1, 100)

def hpd(x,y,pct):
    """"""
    return indices for highest posterior density from array
    """"""
    idx=numpy.argsort(y)[::-1]
    sorted_data=y[idx]

    hits=idx[numpy.where(numpy.cumsum(sorted_data)<=pct)[0]]
    return [x[numpy.min(hits)],x[numpy.max(hits)]],hits

print ('95% highest posterior density interval')
# For thealready prepared, I'm using Binomial's conj. prior.
for k, N in enumerate(n_trials):
    sx = plt.subplot(len(n_trials) / 2, 2, k + 1)
    plt.xlabel(""$p$, probability of heads"") \
        if k in [0, len(n_trials) - 1] else None
    plt.setp(sx.get_yticklabels(), visible=False)
    heads = data[:N].sum()
    y = dist.pdf(x, 1 + heads, 1 + N - heads)
    # compute the 95% highest posterior density
    hpdint,hpdhits=hpd(x,y,95)
    hpdhits=numpy.sort(hpdhits)
    print (k,'tosses',hpdint)
    plt.plot(x, y, label=""observe %d tosses,\n %d heads"" % (N, heads))
    plt.fill_between(x[hpdhits], 0, y[hpdhits], color=""#348ABD"", alpha=0.4)
    plt.vlines(0.5, 0, 4, color=""k"", linestyles=""--"", lw=1)

    leg = plt.legend()
    leg.get_frame().set_alpha(0.4)
    plt.autoscale(tight=True)


plt.suptitle(""Bayesian updating of posterior probabilities"",
             y=1.02,
             fontsize=14)

plt.tight_layout()"
addresses
addresses
requests_json.keys()
requests_json['features'][0].keys()
"largest_so_far = quakes[0]

for quake in quakes:
    if quake['properties']['mag'] > largest_so_far['properties']['mag']:
        largest_so_far = quake
        
largest_so_far"
IPython.core.display.Image(map_png.content)
plt.plot([  quake['properties']['mag'] for quake in quakes  ]) 
"for room_name in house:
    print(room_name)"
house
house.values()
quakes.text[0:100]
data.keys()
data['features'][0].keys()
data['features'][0]['properties'].keys()
data['features'][0]['geometry']
sunspots
"%matplotlib inline
from matplotlib import pyplot as plt
plt.plot(sunspots[:,0], sunspots[:,3]) # Numpy syntax to access all 
                                       #rows, specified column."
"%matplotlib inline
import requests
import numpy as np
from io import BytesIO
from matplotlib import pyplot as plt

spots =  requests.get('http://www.sidc.be/silso/INFO/snmtotcsv.php').text
sunspots = np.genfromtxt(BytesIO(spots.encode()), delimiter=';')
plt.plot(sunspots[:,0], sunspots[:,3])"
sunspots
sunspots['year']
sunspots
sunspots['year']
"plt.plot(sunspots['year'],sunspots['mean'])"
"import datetime
now = datetime.datetime.now()

founded = {""James"": 1976, ""UCL"": 1826, ""Cambridge"": 1209}

current_year = now.year

for x in founded:
    print(x, ""is"", current_year - founded[x], ""years old."")"
founded
"for name, year in founded.items():
    print(name, ""is"", current_year - year, ""years old."")"
"for name in founded:
    print(name, ""is"", current_year - founded[name], ""years old."")"
"map(str, range(10))"
"from numpy.random import random

N = 1000
in_circle, out_circle = [], []
radius = 0.5

def add_sample(in_circle, out_circle, radius):
    point = random() - 0.5, random() - 0.5
    if point[0] * point[0] + point[1] * point[1] <= radius * radius:
        in_circle.append(point)
    else:
        out_circle.append(point)
        
for i in range(N):
    add_sample(in_circle, out_circle, radius)

assert N == len(in_circle) + len(out_circle)
print(""Pi is PI? "", 4 * float(len(in_circle)) / N)"
"%matplotlib inline
from matplotlib import pyplot as plt
from numpy import array

in_circle = array(in_circle)
out_circle = array(out_circle)

plt.scatter(in_circle[:, 0], in_circle[:, 1], linewidth=0, alpha=0.5)
plt.scatter(out_circle[:, 0], out_circle[:, 1], alpha=0.5, color='red')
plt.axis('equal')
plt.show()"
"from numpy import random, sum
N = 1000
radius = 0.5
all_points = random.random((N, 2)) - 0.5
norms = sum(all_points * all_points, axis=1)
n_in_circle = len(all_points[norms < radius * radius])

print(""Pi is "", 4 * float(n_in_circle) / len(all_points))"
myroom
house.simulate(3)
house.simulate(3)
"pl.plot(np.linspace(0.,2.,241), wave(30.0,np.linspace(0.,2.,241)),'r.-')
pl.xlim(0,0.07)"
"pl.figure(figsize=(15,5))
f1 = random.randint(30)
f2 = random.rand()*10
print (f1,f2)
f2 = 1.0/0.250501
f1=1.0/0.25
pl.plot(np.arange(0,3,1.0/120),wave(f1,np.arange(0,3,1.0/120)),'-')
pl.plot(np.arange(0,3,1.0/120),wave(f1,np.arange(0,3,1.0/120)),'.')
pl.plot(np.arange(0,3,1.0/f2),wave(f1,np.arange(0,3,1.0/f2)),'r.',ms=30)
pl.xlim(0,3)"
"#print (1.0/120)
#print (0.008333333333333333*np.arange(100.0))
#print ((0.008333333333333333*np.arange(100.0))%0.225)
#print ("""")
#print (np.arange(0,20,2.0))

ax =     pl.figure(figsize=(20,5)).add_subplot(111)
ax.plot(np.arange(0,3,1.0/3000),wave(f1,np.arange(0,3,1.0/3000)),'g-')
folding (wave(f1,np.arange(0,3,1.0/f2)),np.arange(0,3,1.0/f2), f1, ax)
ax.plot(np.arange(0,3,1.0/f2),wave(f1,np.arange(0,3,1.0/f2)),'k.')
ax.set_xlim(0,0.25)"
"#now plot the component capacitances and the total as well
fig, ax = plt.subplots(nrows=1, ncols=1)

#plot the data,multiply by 1e6 to get µF
ax.plot(E, 1e6*cap_helmholtz()*np.ones(len(E)), 'b-', label='Helmholtz')
ax.plot(E, 1e6*cap_gouychapman(E), 'g-', label='Gouy-Chapman')
ax.plot(E, 1e6*cap_stern(E), 'r-', label='Stern')

#set axis labels
ax.set_xlabel('$E-E_{pz}$ [V]')
ax.set_ylabel('C [$\mu F \cdot cm^{-2}$]')

#set axis limits
ax.set_ylim(0,100)
ax.set_xlim(-0.2,0.2)

#figure legend
ax.legend(loc='best', ncol=1, frameon=False, fontsize=10)

#savefig
#plt.savefig('double-layer-cap_vs_potential.png', dpi=200)

plt.show()"
"#now plot the component capacitances and the total as well
fig, ax = plt.subplots(nrows=1, ncols=1)

cmap = plt.cm.Greens(range(50,255,20))

#plot the data,multiply by 1e6 to get µF
for i,conc in enumerate(c_electrolyte):
    ax.plot(E, 1e6*cap_stern(E, c=conc), ls='-', color=cmap[i], label=str(conc/1e3)+' M')
    
ax.plot(E, 1e6*cap_helmholtz()*np.ones(len(E)), 'k--', label='Helmholtz')

#set axis labels
ax.set_xlabel('$E-E_{pz}$ [V]')
ax.set_ylabel('C [$\mu F \cdot cm^{-2}]$')

#figure legend
ax.legend(loc='best', ncol=3, frameon=False, fontsize=10)

#set axis limits
ax.set_ylim(0,32)
ax.set_xlim(-0.2,0.2)

#save figure
#plt.savefig('double-layer-cap_vs_conc.png', dpi=200)

plt.show()"
"random.shuffle(input)
input"
"runningTimeBestCase.plot(x = 'N', y = 'Running Time', figsize=(16, 8))"
"runningTimeWorstCase.plot(x = 'N', y = 'Running Time', figsize=(16, 8))"
"Vs = np.linspace(V_0, V_2, num=128)
Vs = V_0 + (V_2 - V_0)*np.linspace(0,1,num=500)**10

Vs = Vs[1:]
rs = l(Vs)
vs = f(Vs)
rhos = g(Vs)
ps = h(Vs)

plt.plot(rs, vs, label=""v"")
plt.plot(rs, rhos, label=""rho"")
plt.plot(rs, ps, label=""P"")
plt.legend(loc=""best"")
plt.xlabel(""r [non-dim]"")
plt.ylabel(""[non-dim]"")
plt.xscale(""log"")
# plt.yscale(""log"")"
"t_fs = np.array([3.74e6,
                 3.67e6,
                 3.50e6,
                 3.04e6,
                 2.13e6,
                 1.15e6,
                 1.86e7,
                 5.49e6,
                 1.59e6,
                 4.47e5,
                 1.26e5,
                 3.98e4,
                 1.24e4])

n_0s = np.array([1.33e-1,
                 1.33e-1,
                 1.33e-1,
                 1.33e-1,
                 1.33e-1,
                 1.33e-1,
                 1.33e-3,
                 1.33e-2,
                 1.33e-1,
                 1.33e+0,
                 1.33e+1,
                 1.33e+2,
                 1.33e+3])

log_Zs = np.array([-3.0,
                   -2.0,
                   -1.5,
                   -1.0,
                   -0.5,
                   +0.5,
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.0])
Z_solar = 0.2
Zs = Z_solar * 10**log_Zs

runs = np.array([t_fs, n_0s, Zs])
runs = runs.T
df = pd.DataFrame(runs, columns=[""t_f"", ""n_0"", ""Z""])

log_t_f_model = models.Polynomial2D(1)
fitter = fitting.LinearLSQFitter()

log_t_f_fit = fitter(log_t_f_model, np.log10(df.n_0 / 1), np.log10(df.Z / Z_solar), np.log10(df.t_f))
print(log_t_f_fit)
print(""fiducial t_f: {0:e}"".format(10**log_t_f_fit.c0_0))"
"plt.loglog(df.n_0, df.t_f, marker=""."", linestyle="""", label=""Results"")
plt.loglog(df.n_0, 10**log_t_f_fit(np.log10(df.n_0/1), np.log10(df.Z/Z_solar)), marker=""."", linestyle="""", label=""Fit"")


plt.ylabel(""t_f"")
plt.legend(loc=""best"")"
"plt.loglog(df.Z, df.t_f, marker=""."", linestyle="""", label=""Results"")
plt.loglog(df.Z, 10**log_t_f_fit(np.log10(df.n_0/1), np.log10(df.Z/Z_solar)), marker=""."", linestyle="""", label=""Fit"")

plt.ylabel(""t_f"")
plt.legend(loc=""best"")"
"plt.xkcd()
plt.bar([0,1],[1e5,1e9])
plt.xlim(-0.5,2.5)
plt.yscale('log')
plt.xticks([0.5,1.5],['OpenWorm','HBP'])
plt.ylabel('Budget ($)')
plt.ylim(1e3,1e10)
plt.figure()
plt.bar([0,1],[5,5])
plt.xlim(-0.5,2.5)
plt.xticks([0.5,1.5],['OpenWorm','HBP'])
plt.ylabel('Papers')
plt.ylim(0,7);"
"plt.suptitle('Chernobyl\'s accident - Major radioactive remains', fontsize=22)
plt.gcf().set_size_inches(10, 7.5)
plt.plot([30., 30.], [0., 100.], 'k--')
plt.plot(t, N_I131, color='red', linewidth=3, label=r'$^{131}I$')
plt.plot(t, N_Cs137, color='blue', linewidth=3, label=r'$^{137}Cs$')
plt.plot(t, N_Sr90, color='green', linewidth=3, label=r'$^{90}Sr$')
plt.plot(t, N_Pu241, color='black', linewidth=3, label=r'$^{241}Pu$')
plt.xlabel('Years', fontsize=16)
plt.ylabel('Remaining isotope percetage', fontsize=16)
plt.legend(loc=1, fontsize=16)"
"plt.suptitle('Chernobyl\'s accident - Major radioactive remains', fontsize=22)
plt.gcf().set_size_inches(10, 7.5)
plt.plot([30., 30.], [0., 100.], 'k--')
plt.plot(t, N_I131, color='red', linewidth=3, label=r'$^{131}I$')
plt.plot(t, N_Cs137, color='blue', linewidth=3, label=r'$^{137}Cs$')
plt.plot(t, N_Sr90, color='green', linewidth=3, label=r'$^{90}Sr$')
plt.plot(t, N_Pu241, color='black', linewidth=3, label=r'$^{241}Pu$')
plt.plot(t, N_Am241, color='gray', linewidth=3, label=r'$^{241}Am$')
plt.xlabel('Years', fontsize=16)
plt.ylabel('Remaining isotope percetage', fontsize=16)
plt.legend(loc=4, fontsize=16)"
"t = np.linspace(0., 4000., 250)
plt.gcf().set_size_inches(10, 7.5)
N_Am241 = subproduct_remain(t, T_Am241, T_Pu241)
plt.suptitle('$^{241}Am$ radioactive remains', fontsize=22)
plt.plot(t, N_Am241, color='gray', linewidth=3)
plt.xlabel('Years', fontsize=16)
plt.ylabel('Remaining isotope percetage', fontsize=16)"
"num_folds = 10
N_list = [1000, 10000, 100000, 1000000]
F_list = [5, 50]
min_lambda = 0
max_lambda = 3
num_lambdas = 10
Lambda_list = np.arange(min_lambda, max_lambda, (max_lambda-min_lambda)/num_lambdas)

print(""Naive way:"")
print(""~~~~~~"")
# number of columns, or features
for F in F_list:
    # number of rows, or observations
    for N in N_list:
        print(""F ="", F, "", N ="", N)
        print(""------"")
        (X,Y) = generate_test_data(N,F)
        (optimal_lambda, optimal_beta) = naive_ridge_cv(X, Y, num_folds, Lambda_list)
        print(""------"")
        print(""Optimal lambda = "", np.round(optimal_lambda, 3)) 
        print(""======"")
        
print(""Fast way:"")
print(""~~~~~~"")
# number of columns, or features
for F in F_list:
    # number of rows, or observations
    for N in N_list:
        print(""F ="", F, "", N ="", N)
        print(""------"")
        (X,Y) = generate_test_data(N,F)
        (optimal_lambda, optimal_beta) = fast_ridge_cv(X, Y, num_folds, Lambda_list)
        print(""------"")
        print(""Optimal lambda = "", np.round(optimal_lambda, 3))
        print(""======"")"
"plt.ylabel('XTX computation time (sec)')
plt.xlabel('N')
plt.plot(N_list, naive_XTX_time_F_5, label='F=5, Naive', color='red')
plt.plot(N_list, fast_XTX_time_F_5, label='F=5, Fast', color='blue')
plt.plot(N_list, naive_XTX_time_F_50, label='F=50, Naive', color='gray')
plt.plot(N_list, fast_XTX_time_F_50, label='F=50, Fast', color='black')
plt.legend(bbox_to_anchor=(0., 1.02, 1., .102), loc=3,
           ncol=2, mode=""expand"", borderaxespad=0.)"
"plt.xscale('log')
plt.yscale('log')
plt.ylabel('XTX computation time (sec) log scale')
plt.xlabel('N')
plt.plot(N_list, naive_XTX_time_F_5, label='F=5, Naive', color='red')
plt.plot(N_list, fast_XTX_time_F_5, label='F=5, Fast', color='blue')
plt.plot(N_list, naive_XTX_time_F_50, label='F=50, Naive', color='gray')
plt.plot(N_list, fast_XTX_time_F_50, label='F=50, Fast', color='black')
plt.legend(bbox_to_anchor=(0., 1.02, 1., .102), loc=3,
           ncol=2, mode=""expand"", borderaxespad=0.)"
"plt.ylabel('Total computation time (sec)')
plt.xlabel('N')
plt.plot(N_list, naive_total_time_F_5, label='F=5, Naive', color='red')
plt.plot(N_list, fast_total_time_F_5, label='F=5, Fast', color='blue')
plt.plot(N_list, naive_total_time_F_50, label='F=50, Naive', color='gray')
plt.plot(N_list, fast_total_time_F_50, label='F=50, Fast', color='black')
plt.legend(bbox_to_anchor=(0., 1.02, 1., .102), loc=3,
           ncol=2, mode=""expand"", borderaxespad=0.)"
"plt.xscale('log')
plt.yscale('log')
plt.ylabel('XTX computation time (sec) log scale')
plt.xlabel('N')
plt.plot(N_list, naive_total_time_F_5, label='F=5, Naive', color='red')
plt.plot(N_list, fast_total_time_F_5, label='F=5, Fast', color='blue')
plt.plot(N_list, naive_total_time_F_50, label='F=50, Naive', color='gray')
plt.plot(N_list, fast_total_time_F_50, label='F=50, Fast', color='black')
plt.legend(bbox_to_anchor=(0., 1.02, 1., .102), loc=3,
           ncol=2, mode=""expand"", borderaxespad=0.)"
"import warnings
warnings.filterwarnings('ignore')
import numpy as np
from IPython import display
from IPython.display import Image
# import pylab as pl
import time
from mpl_toolkits.mplot3d import Axes3D


sigmoid = lambda x: 1/(1+np.exp(-x))
dsigmoid = lambda x: sigmoid(x) * (1-sigmoid(x))
F = 2

%pylab notebook
Image(""./BackpropDemo.png"")
"
"#Simple Forward Prop
np.random.seed(0)

x = np.array([.5, -1])
w = np.array([0.25, 1])
y = 0
print('x: {}'.format(x))
print('w: {}'.format(w))
output_pre = x.T.dot(w)
print('output_pre: {}'.format(output_pre))
output = sigmoid(output_pre)
print('output: {}'.format(output))
loss = 0.5 * (output - y)**2
print('loss: {}'.format(loss))
"
"#Backprop training on a single row
eta = 0.25
wp = w

w0 = []
w1 = []
losses = []
fig = plt.figure()
ax = fig.add_subplot(111, projection='3d')
for i in range(150):
    print('Iteration: {}'.format(i))
    output_pre = x.T.dot(wp)
    print('output_pre: {}'.format(output_pre))
    output = sigmoid(output_pre)
    print('output: {}'.format(output))
    loss = 0.5 * (output - y)**2
    print('loss: {}'.format(loss))
    dloss_dout = output - y
    print('dloss_dout: {}'.format(dloss_dout))
    dout_dpre = dsigmoid(output)
    print('dout_dpre: {}'.format(dout_dpre))

    dpre_dw = x
    print('dpre_dw: {}'.format(dpre_dw))
    # dpre_dx = w
    dloss_dw = dloss_dout * dout_dpre * dpre_dw
    print('dloss_dw: {}'.format(dloss_dw))
    w0.append(wp[0])
    w1.append(wp[1])
    losses.append(loss)
    
    wp = wp - eta * dloss_dw

ax.set_xlabel('w0')
ax.set_ylabel('w1')
ax.set_zlabel('loss')

ax.scatter(w0, w1, losses)
    "
"#Backprop training on 2 examples, one row at a time, ie SGD without the random
eta = 0.25
xall = np.array([[.5, -1], [0, 1]])
yall = np.array([1,0])
wp = wx = np.array([.5, -1])
num_epoch = 50

w0 = []
w1 = []
losses = []
fig = plt.figure()
ax = fig.add_subplot(111, projection='3d')
for i in range(num_epoch):
    for j in range(2):
        xp = xall[j]
        yp = yall[j]
        output_pre = xp.T.dot(wp)
        output = sigmoid(output_pre)
        loss = 0.5 * (output - yp)**2
        dloss_dout = output - yp
        dout_dpre = dsigmoid(output)
    
        dpre_dw = xp
        dloss_dw = dloss_dout * dout_dpre * dpre_dw
        w0.append(wp[0])
        w1.append(wp[1])
        losses.append(loss)
    
    wp = wp - eta * dloss_dw
#     if len(w0) > 1:
#         ax.plot(w0, w1, losses)
#     pl.plot(losses)
#     display.clear_output(wait=True)
#     display.display(pl.gcf())
    time.sleep(0.05)
# ax.plot(w0, w1, losses, c=[.25, .75]*num_epoch)
ax.set_xlabel('w0')
ax.set_ylabel('w1')
ax.set_zlabel('loss')

ax.plot(w0, w1, losses)
"
"#Backprop training on 2 examples, both rows at a time, ie Basic Batch Gradient Descent
eta = 0.25
N=10
xall = np.random.randn(N, F)#np.array([[.5, -1], [0, 1]])
# xall = np.array([[.5, -1], [0.5, -1]])
yall = np.where(np.sum(xall,1) > 0, 0, 1) #np.array([1,0])
wp = wx = np.array([.5, -1])
num_epoch = 50

w0 = []
w1 = []
losses = []
fig = plt.figure()
ax = fig.add_subplot(111, projection='3d')
ax.set_xlabel('w0')
ax.set_ylabel('w1')
ax.set_zlabel('loss')
for i in range(num_epoch):
    xp = xall
    yp = yall
    output_pre = xp.dot(wp)
    print('op: {}'.format(output_pre))
    output = sigmoid(output_pre)
    print('output: {}'.format(output))
    loss = 0.5 * np.sum((output - yp)**2)
    print('loss: {}'.format(loss))
    dloss_dout = output - yp
    print('dloss_dout: {}'.format(dloss_dout))
    dout_dpre = dsigmoid(output) * np.eye(N)
    print('dout_dpre: {}'.format(dout_dpre))
    
    dpre_dw = xp
    
    dloss_dw = dloss_dout.dot(dout_dpre).dot(dpre_dw)
    print('dloss_dw: {}'.format(dloss_dw))
    w0.append(wp[0])
    w1.append(wp[1])
    output_all = sigmoid(xall.dot(wp))
    total_loss = np.sum(0.5 * (output_all - yall)**2)
    losses.append(total_loss)
    
    wp = wp - eta * dloss_dw
# ax.plot(w0, w1, losses, c=[.25, .75]*num_epoch)
ax.plot(w0, w1, losses,'x')
"
"import warnings
warnings.filterwarnings('ignore')
from IPython.display import Image
Image(""./BackpropDemo.png"")"
"import matplotlib.pyplot as plt
plt.figure(figsize=(15,10))
line1, = plt.plot(losses_01, 'r-', label='$\eta=0.1$')
line2, = plt.plot(losses_05, 'b-', label='$\eta=0.5$')
line3, = plt.plot(losses_1, 'y-', label='$\eta=1.0$')
line4, = plt.plot(losses_not_converge, 'k--', label='$\eta=1.055$')
plt.ylabel('Losses')
plt.xlabel('Iterations')
plt.legend(handles=[line1, line2, line3, line4])
plt.show()"
"# plot R^2 as a function of max tree depth
fig, axarr = plt.subplots(1,2)
fig.set_figheight(5)
fig.set_figwidth(15)

axarr[0].plot(depth_list_1, r2_fit_list_1, 'b-', label='In-Sample')
axarr[0].plot(depth_list_1, r2_predict_list_1, 'r-', label='Out-of-Sample')
axarr[0].set_title('Noise level = 10')
axarr[0].set_xlabel('Max tree depth')
axarr[0].set_ylabel('$R^2$')
legend = axarr[0].legend(loc='lower center', shadow=False)

axarr[1].plot(depth_list_2, r2_fit_list_2, 'b-', label='In-Sample')
axarr[1].plot(depth_list_2, r2_predict_list_2, 'r-', label='Out-of-Sample')
axarr[1].set_title('Noise level = 100')
axarr[1].set_xlabel('Max tree depth')
axarr[1].set_ylabel('$R^2$')
legend = axarr[1].legend(loc='lower left', shadow=False)

plt.tight_layout()
plt.show()"
"# plot R^2 as a function of max tree depth
fig, axarr = plt.subplots(1,2)
fig.set_figheight(5)
fig.set_figwidth(15)

axarr[0].plot(depth_list_1, r2_fit_list_1, 'b-', label='In-Sample')
axarr[0].plot(depth_list_1, r2_predict_list_1, 'r-', label='Out-of-Sample')
axarr[0].plot(depth_list_3, r2_fit_list_3, 'b--', label='In-Sample CV')
axarr[0].plot(depth_list_3, r2_predict_list_3, 'r--', label='Out-of-Sample CV')
axarr[0].set_title('Noise level = 10')
axarr[0].set_xlabel('Max tree depth')
axarr[0].set_ylabel('$R^2$')
legend = axarr[0].legend(loc='lower center', shadow=False)

axarr[1].plot(depth_list_2, r2_fit_list_2, 'b-', label='In-Sample')
axarr[1].plot(depth_list_2, r2_predict_list_2, 'r-', label='Out-of-Sample')
axarr[1].plot(depth_list_4, r2_fit_list_4, 'b--', label='In-Sample CV')
axarr[1].plot(depth_list_4, r2_predict_list_4, 'r--', label='Out-of-Sample CV')
axarr[1].set_title('Noise level = 100')
axarr[1].set_xlabel('Max tree depth')
axarr[1].set_ylabel('$R^2$')
legend = axarr[1].legend(loc='lower left', shadow=False)

plt.tight_layout()
plt.show()"
"a = np.log(1); b = np.log(10); b1 = np.log(100)
print(a,b,b1)"
"psimplex = np.array([[1, 0, 0], [0, 1, 0], [0, 0, 1]])

fig = pl.figure(0, figsize=(10, 10))
ax = mpl3.Axes3D(fig)
smplx = mpl3.art3d.Poly3DCollection([psimplex])
ax.add_collection3d(smplx)

for x in np.random.randn(10, 3):
    x /= sum(x)
    y = proj(x)
    print(x, y)
    pl.plot((x[0], y[0]), (x[1], y[1]), (x[2], y[2]), color='r')"
"#Code in lambda body follows same scope look up rules as a def

def knights():
    title=""Sir""
    action =(lambda x: title+ ' '+ x)
    return action

act =knights()
msg= act('robin')
print(msg)

print(act)
"
"#global scope
x=99
def func3(y): #y and z are assigned inside function; local scop
    z=x+y #x has global scope
    return z

print(func3(1))"
"import warnings
warnings.filterwarnings('ignore')
#String to list and back
S = ""shrubbery""
L = list(S)
print(""List of alphabets of Shrubbery:"", L)"
"import random
print(random.random())"
"print(random.choice([1,3,5,9])) #Choose a random number from a set"
"#Functions can be created and returned to use elsewhere
def make(label): #Make a function but dont call it
    def echo(message):
        print(label+"":"" + message)
    return echo

F=make(""Spam"") #label in enclosing scope is retained
print(F)
F(""Ham"") #call the function that 'make {function name}' returned

F(""Eggs"")"
"#Function introspection
#because functions are objects, they can also process functions with normal
# object tools.
def func(a):
    b='spam'
    return b*a

print(func(4))
#The call expression is just one operation defined on function objects.
#can also inspect their attirbutes generically
print(func.__name__)
dir(func)"
"#introspection tools allow us to explore implementation details. Functions
# have attached code objects, which provide details  on aspects such as
# function's local variables and arguments

print(func.__code__)
print(dir(func.__code__))
print()
print(func.__code__.co_varnames)
print(func.__code__.co_argcount)
print(func.__code__.co_stacksize)
"
"def func(a):
    b='spam'
    return b*a

func.handles=""Button Press""
print(func.handles)
print()
print(dir(func)) #note handles is added as an attribute
"
"#Python's own implementation-related data stored on functions follow naming
#conventions that prevent them from clashing with more arbitrary names
#that ppl can assign. In Python 3.0+ all function interna names have a 
#leading and training double score
def f():
    pass
dir(f)"
"#Function annotations:
#Possible to attach annotation information- arbitrary user defined data
#about the function arguments and results.

def func(a,b,c):
    return a+b+c
func(1,2,3)
print(func.__annotations__)

def func(a: 'spam',b:(1,10),c:float)->int:
    return a+b+c
func(1,2,3)
print(func.__annotations__)"
"#like range, map, zipa dn filter built-ins also became iterable in 3.x
#in 3.X to conserve space rather than producing a result list all at once
#in memory. However, unlike range, though they are their own iterators
# after stepping through their results once, they are exhausted i.e you
# you cannot have multiple iterators on their results that maintain differen
# positions in those results

M= map(abs,(-1,0,1))
print(M)
#print(list(M))
print(next(M))
print(next(M))
print(next(M))
#print(next(M)) #Throws stop iteration error

for x in M: print(x) #Map iterator is empty , hence no results

M= map(abs,(-1,0,1))
for x in M: print(x)
print(list(M)) #Again M is empty

M= map(abs,(-1,0,1))
print(list(M))   "
"#similar thing for zip
Z=zip((1,2,3),(10,20,30))
print(Z)
print(list(Z))
for pair in Z: print(pair) #no results because iterator is exhausted after 1
                            #pass
print()
Z=zip((1,2,3),(10,20,30))
for pair in Z: print(pair)"
"#Dictionary view Iterables
D= dict(a=1,b=2,c=3)
print(D)
K=D.keys()
print(K)
#print(next(K)) #Throws error saying dictionary keys are not iterators

I=iter(K)
print(next(I))
print(next(I))

for k in D.keys():
    print(list(k), end =' ')

"
"K= D.keys()
print(list(K)) #list of keys

V= D.values()
print(V) #list of values

print(list(V))
#print(V[0]) #throws error; needs list for index access
print(list(V)[0])

E= D.items()
print(E) #list of dictionary items

for (k,v) in D.items(): print(k,v, end = "" "")
"
"#Dictionaries are iterables themselves with an iterator that 
#returs successive keys. 
print(D)
I= iter(D)
print(next(I))
print(next(I))

for key in D: print(key, end="" "") #does not need keys() to iterate"
"#Keys no longer returs a list; the traditional coding pattern for scanning
# a dictionary by sorted work. Instead convert keys view with a list call
# or use sorted call on either a keys view or the dictionary itself.
D= dict(a=1,b=5,c=8,d=2)
print(D)

for k in sorted(D.keys()): print(k, D[k], end=' ')
print()    
for k in sorted(D): print(k, D[k],end=' ')"
"#Cycles, paths and stack limits:
#Larger recursive applications require more infrastructure and need to avoid
#cycles or repeat paths and expand stack space when using recursive calls
#instead of explicity queues or stacks.

#To do better, recurse calls could keep and pass a set, dicitionary or list
# of states visited so far. "
"D = {""food"":""Spam"",""quantity"":4, ""color"":""pink""}
print(D.keys()) #the indices are keys
print(D.values()) #values corresponding to indices"
"# Another way to declare dictionaries:
D = {}
D[""name""] = ""Bob""
D[""job""] = ""Dev""
D[""age""] = 40
D"
"# Another way to declare dictionaries:
a = dict(Country=""USA"", Population=""600M"", GDP=""1B"")
a"
"#Dictionaries can be nested as well
rec = {""name"":{""first"":""Bob"",""last"":""Dole""}, 
       ""jobs"":[""dev"",""mgr""],
       ""age"": 45}
rec"
"print(rec.keys())
print(rec.values())"
"D = {'a':1,'b':2,""c"":3}
print(D)
print(D.keys())
print(D.values())"
"import warnings
warnings.filterwarnings('ignore')
#List functions
a = [1,2,3,4]
a.append(99) #append adds 1 element; 
                #f you try adding [4,5] result = [1,2,3,[4,5]]
a.extend([4,5])# a becomes[1,2,3,4,5]
a.insert(0,999)
b = a.copy() #copies element

a.sort()
a.reverse() #or a.sort(reverse= True)


a.remove(4)
a.pop(2)
a.clear() #removes all element from list
    # same as del a[:]
         "
"#String function
a = ""Hello World""
a.upper()
a.lower()
a.count(""l"")

a.find(""Wo"")
a.index(""r"")

a.replace(""He"",""Se"") #assign to a different string; a doesnt change

a.isdigit()
a.isalpha()
a.isnumeric()
a.isspace()
a.istitle()

"
"import warnings
warnings.filterwarnings('ignore')
import numpy
numpy.__version__"
"np.array([1,2,3,4], dtype='float32')"
"np.ones((3,5), dtype=float)"
"np.full((3,5),3.14)"
"np.linspace(0, 1, 5)"
"np.random.random((3,3))"
"np.random.normal(0, 1, (3,3))"
"np.random.randint(0, 10, (3,3))"
np.eye(3)
np.empty(3)
"import matplotlib.pyplot as plt
x = [0, 1, 2]
y = [0, 1, 4]
fig = plt.figure()
axes = fig.add_subplot(111)
axes.plot(x, y)
plt.show()"
df.head(5)
"df.tail(2)
"
plot_corr(df)
df.head(5)
"from sklearn.preprocessing import Imputer


# Impute with mean all 0 readings
fill_0 = Imputer(missing_values=0, strategy=""mean"", axis=0)

x_train = fill_0.fit_transform(x_train)
x_test = fill_0.fit_transform(x_test)"
"from sklearn.naive_bayes import GaussianNB 

# create gaussian naive bayes model object and train it with data
nb_model = GaussianNB()

#train with training set
nb_model.fit(x_train, y_train.ravel())"
"from sklearn.ensemble import RandomForestClassifier 

rf_model = RandomForestClassifier(random_state=42)

rf_model.fit(x_train, y_train.ravel())"
"from sklearn.linear_model import LogisticRegression

lr_model = LogisticRegression(C=0.7,random_state=42)

lr_model.fit(x_train, y_train.ravel())"
"print(""----- Confusion matrix of Naive Bays"")
# upper left = TP, upper right = FP
# lower left = FN, upper right = TN
print(""{0}"".format(metrics.confusion_matrix(y_test, nb_predict_test, labels=[1,0])))
print("""")
print(""----- Confusion matrix of Random Forrest"")
# upper left = TP, upper right = FP
# lower left = FN, upper right = TN
print(""{0}"".format(metrics.confusion_matrix(y_test, rf_predict_test, labels=[1,0])))
print("""")
print(""----- Confusion matrix of Logistic Regression"")
# upper left = TP, upper right = FP
# lower left = FN, upper right = TN
print(""{0}"".format(metrics.confusion_matrix(y_test, lr_predict_test, labels=[1,0])))

print("""")
print(""----- Classification report of Naive Bayes"")
# recall = how well are we predicting diabetes when result is diabetes = TPR
# precision = how often the patient has diabetes when the model said they would
print(metrics.classification_report(y_test, nb_predict_test, labels=[1,0]))

print("""")
print(""----- Classification report of Random Forrest"")
# recall = how well are we predicting diabetes when result is diabetes = TPR
# precision = how often the patient has diabetes when the model said they would
print(metrics.classification_report(y_test, rf_predict_test, labels=[1,0]))

print("""")
print(""----- Classification report of Logistic Regression"")
# recall = how well are we predicting diabetes when result is diabetes = TPR
# precision = how often the patient has diabetes when the model said they would
print(metrics.classification_report(y_test, lr_predict_test, labels=[1,0]))"
"C_start = 0.1
C_end = 5
C_inc = 0.1

C_values, recall_scores = [], []

C_val = C_start
best_recall_score = 0
while (C_val < C_end):
    C_values.append(C_val)
    lr_model_loop=LogisticRegression(C=C_val,random_state=42) 
    lr_model_loop.fit(x_train, y_train.ravel())
    lr_predict_loop_test = lr_model_loop.predict(x_test)
    recall_score = metrics.recall_score(y_test, lr_predict_loop_test)
    recall_scores.append(recall_score)
    if (recall_score > best_recall_score):
        best_recall_score = recall_score
        best_lr_predict_test = lr_predict_loop_test
        
    C_val = C_val + C_inc
    
best_score_C_val = C_values[recall_scores.index(best_recall_score)]
print(""1st max value of {0:.3f} occured at C={1:.3f}"".format(best_recall_score, best_score_C_val))

%matplotlib inline
plt.plot(C_values, recall_scores,""-"")
plt.xlabel(""C value"")
plt.ylabel(""recall score"")"
"# Using hyperparemeter of LR called class_weight = ""balanced""

C_start = 0.1
C_end = 5
C_inc = 0.1

C_values, recall_scores = [], []

C_val = C_start
best_recall_score = 0
while (C_val < C_end):
    C_values.append(C_val)
    lr_model_loop=LogisticRegression(C=C_val,class_weight=""balanced"",random_state=42) 
    lr_model_loop.fit(x_train, y_train.ravel())
    lr_predict_loop_test = lr_model_loop.predict(x_test)
    recall_score = metrics.recall_score(y_test, lr_predict_loop_test)
    recall_scores.append(recall_score)
    if (recall_score > best_recall_score):
        best_recall_score = recall_score
        best_lr_predict_test = lr_predict_loop_test
        
    C_val = C_val + C_inc
    
best_score_C_val = C_values[recall_scores.index(best_recall_score)]
print(""1st max value of {0:.3f} occured at C={1:.3f}"".format(best_recall_score, best_score_C_val))

%matplotlib inline
plt.plot(C_values, recall_scores,""-"")
plt.xlabel(""C value"")
plt.ylabel(""recall score"")"
"lr_model_improved=LogisticRegression(C=best_score_C_val,class_weight=""balanced"",random_state=42) 
lr_model_improved.fit(x_train, y_train.ravel())
lr_improved_predict_test = lr_model_improved.predict(x_test)

print(""Accuracy of Logistic Regression: {0:.4f}"".format(metrics.accuracy_score(y_test, lr_improved_predict_test)))
print("""")
print(""----- Confusion matrix of Logistic Regression Improved"")
# upper left = TP, upper right = FP
# lower left = FN, upper right = TN
print(""{0}"".format(metrics.confusion_matrix(y_test, lr_improved_predict_test, labels=[1,0])))

print("""")
print(""----- Classification report of Logistic Regression Improved"")
# recall = how well are we predicting diabetes when result is diabetes = TPR
# precision = how often the patient has diabetes when the model said they would
print(metrics.classification_report(y_test, lr_improved_predict_test, labels=[1,0]))"
"import warnings
warnings.filterwarnings('ignore')
%matplotlib inline
import numpy as np
from matplotlib import pyplot as plt
import scipy.misc
face = scipy.misc.face()
image = face[:,:,1]
# image size, square side length, number of squares
ncols, nrows = 768, 1024
sq_size, nsq = 10, 20

plt.figure(),plt.imshow(image,cmap='gray')

# Take the 2-dimensional DFT and centre the frequencies
ftimage = np.fft.fft2(np.fft.fftshift(image))
ftimage = np.fft.fftshift(ftimage)
plt.figure(),plt.imshow(np.abs(ftimage),cmap='gray',vmin=0, vmax=10000)

# Build and apply a Gaussian filter.
sigmax, sigmay = 50, 50
cx,cy = nrows/2, ncols/2
x = np.linspace(0, nrows, nrows)
y = np.linspace(0, ncols, ncols)
X, Y = np.meshgrid(x, y)
gmask = np.exp(-(((X-cx)/sigmax)**2 + ((Y-cy)/sigmay)**2))

ftimagep = ftimage * gmask
plt.figure(),plt.imshow(gmask,cmap='gray')
plt.figure(),plt.imshow(np.abs(ftimagep),cmap='gray',vmin=0, vmax=10000)


# Finally, take the inverse transform and show the blurred image
imagep = np.fft.fftshift(np.fft.ifft2(np.fft.fftshift(ftimagep)))
plt.figure(),plt.imshow(np.abs(imagep),cmap='gray')
"
"ftimage_restore = ftimagep / gmask

plt.figure(),plt.imshow(gmask,cmap='gray')
plt.figure(),plt.imshow(np.abs(ftimage_restore),cmap='gray',vmin=0, vmax=10000)


# Finally, take the inverse transform and show the blurred image
imagep = np.fft.fftshift(np.fft.ifft2(np.fft.fftshift(ftimage_restore)))
plt.figure(),plt.imshow(np.abs(imagep),cmap='gray')"
!dir
"df = pd.DataFrame({'key':['A','B','C','A','B','C','A','B','C'],
                   'data': [0, 5, 10, 5, 10, 15, 10, 15, 20]})
df"
df['Survived'].sum() / len(df['Survived'])
"df25 = df[df['Age'] <= 25]
df25['Survived'].sum() / len(df25['Survived'])"
df.groupby('Pclass')['Survived'].aggregate(survival_ratio).plot(kind='bar')
"pd.Series(pd.date_range(start=""2016-01-01"", periods=10, freq='3H'))"
data.index.day
data.index.dayofyear
data.index.year
data.plot()
data.resample('A').mean().plot() # 10D
data.resample('M').std().plot() # 'A'
"fig, ax = plt.subplots()
data.loc['2011':'2012', 'L06_347'].resample('M').mean().plot(ax=ax)
data.loc['2011':'2012', 'L06_347'].resample('M').median().plot(ax=ax)
ax.legend([""mean"", ""median""])"
"data.loc['2011':'2012', 'L06_347'].resample('M').agg(['mean', 'median']).plot()"
"daily.resample('M').agg(['min', 'max']).plot() # monthly minimum and maximum values of these daily averages"
"fig, ax = plt.subplots()
data['2013'].mean().plot(kind='barh', ax=ax)"
data.groupby(data.index.month).mean().plot()
df['Age'].hist()
df.groupby('Pclass')['Survived'].aggregate(lambda x: x.sum() / len(x)).plot(kind='bar')
df['Survived'].sum() / df['Survived'].count()
"df25 = df[df['Age'] <= 25]
df25['Survived'].sum() / len(df25['Survived'])"
s.values
s[0]
"pop_dict = {'Germany': 81.3, 
            'Belgium': 11.3, 
            'France': 64.3, 
            'United Kingdom': 64.9, 
            'Netherlands': 16.9}
population = pd.Series(pop_dict)
population"
population['France']
population * 1000
"data = {'country': ['Belgium', 'France', 'Germany', 'Netherlands', 'United Kingdom'],
        'population': [11.3, 64.3, 81.3, 16.9, 64.9],
        'area': [30510, 671308, 357050, 41526, 244820],
        'capital': ['Brussels', 'Paris', 'Berlin', 'Amsterdam', 'London']}
countries = pd.DataFrame(data)
countries"
countries.columns
countries.dtypes
countries.info()
countries.values
"countries = countries.set_index('country')
countries"
population / 100
countries.median()
population / population['Belgium'].mean()
"countries['density'] = countries['population']*1000000 / countries['area']
countries"
"countries.sort_values('density', ascending=False)"
countries.describe()
countries.plot()
countries['population'].plot(kind='bar')
"import warnings
warnings.filterwarnings('ignore')
#P165 of ""Doing Math with Python"": Interactive version of drawing the Barnsley Fern

%matplotlib inline

from ipywidgets import interact
import ipywidgets as widgets

import random
import matplotlib.pyplot as plt

def transformation_1(p):
    x = p[0]
    y = p[1]
    x1 = 0.85*x + 0.04*y
    y1 = -0.04*x + 0.85*y + 1.6
    return x1, y1

def transformation_2(p):
    x = p[0]
    y = p[1]
    x1 = 0.2*x - 0.26*y
    y1 = 0.23*x + 0.22*y + 1.6
    return x1, y1

def transformation_3(p):
    x = p[0]
    y = p[1]
    x1 = -0.15*x + 0.28*y
    y1 = 0.26*x  + 0.24*y + 0.44
    return x1, y1

def transformation_4(p):
    x = p[0]
    y = p[1]
    x1 = 0
    y1 = 0.16*y
    return x1, y1

def get_index(probability):
    r = random.random()
    c_probability = 0
    sum_probability = []
    for p in probability:
        c_probability += p
        sum_probability.append(c_probability)
    for item, sp in enumerate(sum_probability):
        if r <= sp:
            return item
    return len(probability)-1

def transform(p):
    # list of transformation functions
    transformations = [transformation_1, transformation_2,
                           transformation_3, transformation_4]
    probability = [0.85, 0.07, 0.07, 0.01]
    # pick a random transformation function and call it
    tindex = get_index(probability)
    t = transformations[tindex]
    x, y = t(p)
    return x, y

def draw_fern(n):
    # We start with (0, 0)
    x = [0]
    y = [0]
    x1, y1 = 0, 0
    for i in range(n):
       x1, y1 = transform((x1, y1))
       x.append(x1)
       y.append(y1)
    
    # Plot the points
    plt.plot(x, y, 'o')
    plt.title('Fern with {0} points'.format(n))
    plt.show()
 
# Allow interaction via the interact() function and an Integer slider widget
i = interact(draw_fern, n=widgets.IntSlider(min=0, max=10000,step=1,value=10))"
"import warnings
warnings.filterwarnings('ignore')
%matplotlib inline

from ipywidgets import interact
import ipywidgets as widgets

# Interactive version of drawing the Sierpinski triangle
# Doing Math with Python (Chapter 6)

'''
sierpinski.py

Draw Sierpinski Triangle
'''
import random
import matplotlib.pyplot as plt

def transformation_1(p):
    x = p[0]
    y = p[1]
    x1 = 0.5*x
    y1 = 0.5*y
    return x1, y1

def transformation_2(p):
    x = p[0]
    y = p[1]
    x1 = 0.5*x + 0.5
    y1 = 0.5*y + 0.5
    return x1, y1

def transformation_3(p):
    x = p[0]
    y = p[1]
    x1 = 0.5*x + 1
    y1 = 0.5*y
    return x1, y1

def get_index(probability):
    r = random.random()
    c_probability = 0
    sum_probability = []
    for p in probability:
        c_probability += p
        sum_probability.append(c_probability)
    for item, sp in enumerate(sum_probability):
        if r <= sp:
            return item
    return len(probability)-1

def transform(p):
    # list of transformation functions
    transformations = [transformation_1, transformation_2, transformation_3]
    probability = [1/3, 1/3, 1/3]
    # pick a random transformation function and call it
    tindex = get_index(probability)
    t = transformations[tindex]
    x, y = t(p)
    return x, y

def draw_sierpinski(n):
    # We start with (0, 0)
    x = [0]
    y = [0]

    x1, y1 = 0, 0
    for i in range(n):
       x1, y1 = transform((x1, y1))
       x.append(x1)
       y.append(y1)  
    
    plt.plot(x, y, 'o')
    plt.title('Sierpinski with {0} points'.format(n))
    plt.show()

i = interact(draw_sierpinski, n=widgets.IntSlider(min=100, max=10000, step=1, value=10))"
"import warnings
warnings.filterwarnings('ignore')
# Interactive version of the ""Mandelbrot Set"" - solution to challenge in 
# ""Doing Math with Python"", chapter 6
%matplotlib inline
'''
mandelbrot.py

Draw a Mandelbrot set

Using ""Escape time algorithm"" from:
http://en.wikipedia.org/wiki/Mandelbrot_set#Computer_drawings

Thanks to http://www.vallis.org/salon/summary-10.html for some important
ideas for implementation.

'''

from ipywidgets import interact
import ipywidgets as widgets


import matplotlib.pyplot as plt
import matplotlib.cm as cm

# Subset of the complex plane we are considering
x0, x1 = -2.5, 1
y0, y1 = -1, 1

def initialize_image(x_p, y_p):
    image = []
    for i in range(y_p):
        x_colors = []
        for j in range(x_p):
            x_colors.append(0)
        image.append(x_colors)
    return image

def mandelbrot_set(n, max_iterations):
    image = initialize_image(n, n)
    
    # Generate a set of equally spaced points in the region
    # above
    dx = (x1-x0)/(n-1)
    dy = (y1-y0)/(n-1)
    x_coords = [x0 + i*dx for i in range(n)]
    y_coords = [y0 + i*dy for i in range(n)]

    for i, x in enumerate(x_coords):
        for k, y in enumerate(y_coords):
            z1 = complex(0, 0)
            iteration = 0
            c = complex(x, y)
            while (abs(z1) < 2  and iteration < max_iterations):
                z1 = z1**2 + c
                iteration += 1
            image[k][i] = iteration
    return image

def draw_mandelbrot(n, max_iterations):
    image = mandelbrot_set(n, max_iterations)
    plt.imshow(image, origin='lower', extent=(x0, x1, y0,y1),
               cmap=cm.Greys_r, interpolation='nearest')
    plt.show()
    

# Allow interaction via the interact() function and an Integer slider widget
i = interact(draw_mandelbrot, 
             n=widgets.IntSlider(min=100, max=600,step=1,value=10), 
             max_iterations=widgets.IntSlider(min=100, max=10000,step=1,value=10),
             # This keyword argument adds a button so that the drawing happens
             # only when the button is clicked
             __manual=True
             )"
"
import numpy as np
import scipy.stats as st
import matplotlib.pyplot as plt

num_replications = 5000
outcomes = np.empty(num_replications)
N = 1000
k = 5     # Degrees of freedom
chi = st.chi2(k)

for i in range(num_replications):
    xvec = chi.rvs(N)
    outcomes[i] = np.sqrt(N / (2 * k)) * (xvec.mean() - k)

xmin, xmax = -4, 4
grid = np.linspace(xmin, xmax, 200)

fig, ax, = plt.subplots(figsize=(10, 8))
ax.hist(outcomes, bins=50, normed=True, alpha=0.4)
ax.plot(grid, st.norm.pdf(grid), 'k-', lw=2, alpha=0.7)
plt.show()

"
"import seaborn as sns
import numpy as np
from scipy.stats import beta, gaussian_kde
from mpl_toolkits.mplot3d import Axes3D
from matplotlib.collections import PolyCollection
import matplotlib.pyplot as plt

beta_dist = beta(2, 2)


def gen_x_draws(k):
    """"""
    Returns a flat array containing k independent draws from the
    distribution of X, the underlying random variable.  This distribution is
    itself a convex combination of three beta distributions.
    """"""
    bdraws = beta_dist.rvs((3, k))
    # == Transform rows, so each represents a different distribution == #
    bdraws[0, :] -= 0.5
    bdraws[1, :] += 0.5
    bdraws[2, :] -= 1.0
    # == Set X[i] = bdraws[j, i], where j is a random draw from {0, 1, 2} == #
    js = np.random.random_integers(0, 2, size=k)
    X = bdraws[js, np.arange(k)]
    # == Rescale, so that the random variable is zero mean == #
    m, sigma = X.mean(), X.std()
    return (X - m) / sigma

nmax = 5
reps = 100000
ns = list(range(1, nmax + 1))
nsy = ['$Q_{}$'.format(i) for i in ns]

# == Form a matrix Z such that each column is reps independent draws of X == #
Z = np.empty((reps, nmax))
for i in range(nmax):
    Z[:, i] = gen_x_draws(reps)
# == Take cumulative sum across columns
S = Z.cumsum(axis=1)
# == Multiply j-th column by sqrt j == #
Y = (1 / np.sqrt(ns)) * S

# == Plot == #

fig = plt.figure(figsize=(14, 10))
ax = fig.gca(projection='3d')
ax.view_init(elev=20., azim=220)

a, b = -3, 3
gs = 100
xs = np.linspace(a, b, gs)

# == Build verts == #
greys = np.linspace(0.3, 0.7, nmax)
verts = []
for n in ns:
    density = gaussian_kde(Y[:, n-1])
    ys = density(xs)
    verts.append(list(zip(xs, ys)))

poly = PolyCollection(verts)
#poly = PolyCollection(verts, facecolors=[str(g) for g in greys])
poly.set_alpha(0.85)
ax.add_collection3d(poly, zs=ns, zdir='x')

# ax.text(np.mean(rhos), a-1.4, -0.02, r'$\beta$', fontsize=16)
# ax.text(np.max(rhos)+0.016, (a+b)/2, -0.02, r'$\log(y)$', fontsize=16)
ax.set_xlim3d(1, nmax)
ax.set_xticks(ns)
ax.set_xticklabels(nsy)
ax.set_yticks([0])
ax.set_ylim3d(a, b)
ax.set_zlim3d(0, 0.4)
ax.set_zticks((0.2, 0.4))
plt.show()
"
"
import numpy as np
import matplotlib.pyplot as plt
from matplotlib import cm


N = 250
alpha = 0.8
num_reps = 25000
beta = 10
ols = np.empty(num_reps)
ivls = np.empty(num_reps)

for j in range(num_reps):
    z = np.random.randn(N)
    u = np.random.randn(N)
    x = alpha * z + (1 - alpha) * u
    y = x * beta + u
    ols[j]  = np.sum(x * y) / np.sum(x**2)
    ivls[j]  = np.sum(z * y) / np.sum(z * x)

fig, ax = plt.subplots(figsize=(10, 6.5))
plot_args = dict(bins=32, histtype='stepfilled', alpha=0.3, normed=True)
ax.set_ylim(0, 6)
ax.hist(ols, color='g', label='OLS', **plot_args)
ax.hist(ivls, color='b', label='IVLS', **plot_args)
ax.plot([beta, beta], [0, 6], 'k-', label=r'$\beta$')
ax.legend(loc='upper left', frameon=False)

"
"
import numpy as np
from random import uniform
import matplotlib.pyplot as plt
from scipy.stats import norm
from sklearn.neighbors import KernelDensity

f, g = norm(), norm(3, 0.6)
a = 0.4

def mixnorm_pdf(s):
    return a * f.pdf(s) + (1 - a) * g.pdf(s)

def mixnorm_rvs(N):
    y = np.empty(N)
    for n in range(N):
        h = f if uniform(0, 1) < a else g
        y[n] = float(h.rvs(size=1))
    return y

xmin, xmax = -3, 6
grid_len = 160
grid = np.linspace(xmin, xmax, grid_len)
N = 40
obs = mixnorm_rvs(N)

fig, axes = plt.subplots(2, 2, figsize=(12, 10))

axes = axes.flatten()
bws = 0.1, 0.5, 1.0, 1.5

for bw, ax in zip(bws, axes):

    ax.set_ylim(0, 0.6)
    ax.set_yticks((0, 0.6))
    ax.set_xticks([])
    ax.fill_between(grid, 0*grid, mixnorm_pdf(grid), alpha=0.2)

    ax.plot(obs, [0.01] * N, 'go', alpha=0.6)

    kde = KernelDensity(bandwidth=bw).fit(obs.reshape(N, 1))
    log_dens = kde.score_samples(grid.reshape(grid_len, 1))
    lb = ""bandwidth = {}"".format(bw)
    ax.plot(grid, np.exp(log_dens), 'k-', lw=2, alpha=0.6, label=lb)
    ax.legend(loc='upper left', frameon=False)

plt.show()




"
"
import numpy as np
from numpy import dot
from numpy.linalg import solve
import matplotlib.pyplot as plt

sigma = 0.5   # Parameterizes measurement error for c
lmda = 1      # Regularization parameter
numreps = 10  # Number of times to solve system

# Construct an arbitrary N x K matrix A 
N, K = 40, 20
A = np.empty((N, K))
A[:, 0] = 1
for i in range(1, K):
    A[:, i] = A[:, i-1] + 0.1 * np.random.randn(N)

I = np.identity(K)             # K x K identity
Ap = A.T                       # A transpose
bstar = np.zeros((K, 1)) + 10  # True solution
c = dot(A, bstar)              # Corresponding c
index = range(1, K+1)          # For plotting

fig, ax = plt.subplots(figsize=(12, 8))

bbox = (0., 1.02, 1., .102)
legend_args = {'bbox_to_anchor': bbox, 'loc': 3, 'mode': 'expand'}

# Plot the solutions
for j in range(numreps):
    # Observe c with error
    c0 = c.flatten() + sigma * np.random.randn(N)
    # Compute the regularized solution
    b1 = solve(dot(Ap, A) + lmda * I, dot(Ap, c0))
    if j == numreps - 1:
        ax.plot(index, b1, 'b-', lw=2, alpha=0.4, label='ridge estimates')
    else:
        ax.plot(index, b1, 'b-', lw=2, alpha=0.4)
    # Compute the standard least squares solution
    b2 = solve(dot(Ap, A), dot(Ap, c0))
    if j == numreps - 1:
        ax.plot(index, b2, 'g-', lw=2, alpha=0.5, label='OLS estimates')
    else:
        ax.plot(index, b2, 'g-', lw=2, alpha=0.5)


ax.plot(index, bstar, 'k-', lw=3, alpha=1, label='true value')
ax.legend(ncol=3, **legend_args)
ax.set_xlim(1, K)
plt.show()
"
"# %load ../../norm_den_seq.py
import numpy as np
import func_plot_style
from scipy.stats import norm 

mu0 = 0
sigma0 = 1
a = 0.8
b = 1
c = 1

plt, fig, ax = func_plot_style.subplots()

xmin, xmax = -4.0, 8.0
x = np.linspace(xmin, xmax, 100)

mu = mu0
sigma2 = sigma0**2

for t in range(8):
    f = lambda x: norm.pdf(x, mu, np.sqrt(sigma2))
    lb = r'$t={}$'.format(t)
    ax.plot(x, f(x), '-', lw=2.6, label=lb)
    mu = b + a * mu
    sigma2 = a * a * sigma2 + c**2

ax.legend(loc='upper left')
plt.show()

"
"
from matplotlib import cm
import matplotlib.pyplot as plt
from scipy.stats import norm
import numpy as np


alpha0 = 0.5
alpha1 = 0.8

def arch_sk(x, y):
    ""ARCH stochastic kernel""
    v = np.sqrt(alpha0 + alpha1 * x**2)
    return norm.pdf(y / v) / v

left, right, bottom, top = -1.4, 1.4, -1.4, 1.4

x_grid = np.linspace(left, right, 60)
y_grid = np.linspace(bottom, top, 60)
X, Y = np.meshgrid(x_grid, y_grid)

Z = arch_sk(X, Y)

fig, ax = plt.subplots(figsize=(12, 10))

ax.contourf(X, Y, Z, 10, alpha=0.5, cmap=cm.jet)
cs = ax.contour(X, Y, Z, 10, colors='k', lw=0.5, alpha=0.5, antialias=True)
plt.clabel(cs, inline=1, fontsize=12)

ax.plot([bottom, top], [bottom, top], 'k-', lw=2, alpha=0.6)

ax.set_xlabel(""$s$"", fontsize=16)
ax.set_ylabel(""$s'$"", fontsize=16)

plt.show()
"
"from matplotlib import cm
import matplotlib.pyplot as plt
from scipy.stats import norm
import numpy as np

alpha0 = 0.5
alpha1 = 0.8

def arch_sk(x, y):
    v = np.sqrt(alpha0 + alpha1 * x**2)
    return norm.pdf(y / v) / v

left, right, bottom, top = -1.4, 1.4, -1.4, 1.4

x_grid = np.linspace(left, right, 60)
y_grid = np.linspace(bottom, top, 60)
X, Y = np.meshgrid(x_grid, y_grid)

Z = arch_sk(X, Y)

fig, ax = plt.subplots(figsize=(10, 8))

#ax.set_ylim(bottom, top)
#ax.set_xlim(left, right)

#ax.set_xticks((-0.5, 0, 0.5))
#ax.set_yticks((-0.5, 0, 0.5))

ax.contourf(X, Y, Z, 10, alpha=0.5, cmap=cm.jet)
cs = ax.contour(X, Y, Z, 10, colors='k', lw=0.5, alpha=0.5, antialias=True)
plt.clabel(cs, inline=1, fontsize=12)

ax.plot([bottom, top], [bottom, top], 'k-', lw=2, alpha=0.6)

ax.set_xlabel(""$s$"", fontsize=16)
ax.set_ylabel(""$s'$"", fontsize=16)

plt.show()
"
"
from matplotlib.mlab import bivariate_normal
from matplotlib import cm
import matplotlib.pyplot as plt
import numpy as np
from scipy.stats import norm


def make_grid(left=-2, right=2, bottom=-2, top=2):
    x_grid = np.linspace(left, right, 100)
    y_grid = np.linspace(bottom, top, 100)
    X, Y = np.meshgrid(x_grid, y_grid)
    return X, Y


def gumbel_C(u, v, theta):
    g = lambda u: (-np.log(u))**theta
    z = (g(u) + g(v))**(1.0/theta)
    return np.exp(-z)


def clayton_C(X, Y, theta):
    ""Clayton copula""
    g = lambda t: t**(-theta)
    z = np.maximum(g(u) + g(v) - 1, 0)
    return z**(-1.0/theta)

def clayton_c(u, v, theta):
    ""Clayton density""
    a = (theta + 1) * (u * v)**(-theta-1)
    b = (u**(-theta) + v**(-theta) -1)**(-(2*theta+1.0)/theta)
    return a * b

def clayton_normal_mix(x, y, theta):
    u = norm.cdf(x)
    v = norm.cdf(y)
    return clayton_c(u, v, theta) * norm.pdf(x) * norm.pdf(y)


fig, axes = plt.subplots(2, 2, figsize=(10, 10))
fig.subplots_adjust(hspace=.3)
axes = axes.flatten()

# Common operations
for ax in axes:
    ax.set_xticks(np.linspace(-2, 2, 5))
    ax.set_yticks(np.linspace(-1, 2, 4))


# Plot 1
ax = axes[0]

s_x, s_y = 1, 1
s_xy = 0.0

X, Y = make_grid()
Z = bivariate_normal(X, Y, s_x, s_y, 0, 0, s_xy)

ax.contourf(X, Y, Z, 10, alpha=0.8, cmap=cm.jet)
ax.contour(X, Y, Z, 10, colors='k', lw=1, alpha=0.4, antialias=True)

ax.set_title(r'Gaussian, $\rho=0.0$', fontsize=14)


# Plot 2
ax = axes[1]
s_x, s_y = 1, 1
s_xy = 0.5

X, Y = make_grid()
Z = bivariate_normal(X, Y, s_x, s_y, 0, 0, s_xy)

ax.contourf(X, Y, Z, 10, alpha=0.8, cmap=cm.jet)
ax.contour(X, Y, Z, 10, colors='k', lw=1, alpha=0.4, antialias=True)

ax.set_title(r'Gaussian, $\rho=0.5$', fontsize=14)



# Plot 3
ax = axes[2]

X, Y = make_grid()
Z = clayton_normal_mix(X, Y, 2)

ax.contourf(X, Y, Z, 10, alpha=0.8, cmap=cm.jet)
ax.contour(X, Y, Z, 10, colors='k', lw=1, alpha=0.4, antialias=True)

ax.set_title(r'Clayton, $\theta=2$', fontsize=14)



# Plot 4
ax = axes[3]

X, Y = make_grid()
Z = clayton_normal_mix(X, Y, 6.)

ax.contourf(X, Y, Z, 10, alpha=0.8, cmap=cm.jet)
ax.contour(X, Y, Z, 10, colors='k', lw=1, alpha=0.4, antialias=True)

ax.set_title(r'Clayton, $\theta=6$', fontsize=14)
plt.show()
"
"
from __future__ import division
import numpy as np
import matplotlib.pyplot as plt 

K = 100 # length of mu vector (number of agents)
N = 5  # number of observations per agent
sigma = 1

mu_vec = 1 * np.random.rand(K)  # k-th element is mu_k

Y = sigma * np.random.randn(N, K)  # k-th col is shocks for individual k

# Form the sample analog estimator
hat_mu_sap = np.empty(K)
for k in range(K):
    hat_mu_sap[k] = np.mean(mu_vec[k] + Y[:,k])

# Form the Stein estimator
t1 = (sigma**2) * (K - 2) / N
t2 = sum(hat_mu_sap * hat_mu_sap)
t3 = 1 - t1/t2
hat_mu_stein = t3 * hat_mu_sap 

# Errors
stein_error = hat_mu_stein - mu_vec
sap_error = hat_mu_sap - mu_vec

ss_sap = np.round(sum(sap_error * sap_error), decimals=3)
ss_stein = np.round(sum(stein_error * stein_error), decimals=3)

fig, axes =plt.subplots(nrows=2, ncols=1, figsize=(14, 14))

ymin, ymax = -1.2, 1.2

for ax in axes:
    ax.set_ylim(ymin, ymax)
    ax.set_xlim(0, K)
    ax.set_xticks((1, K))

ax = axes[0]
ax.set_title('sample mean error')
ax.bar(range(1, K+1), sap_error, alpha=0.6)
ax.text(5, 1.10, 'SSE = {}'.format(ss_sap), fontsize=14)
ax.legend()

ax = axes[1]
ax.set_title('stein error')
ax.bar(range(1, K+1), stein_error, color='g', alpha=0.6)
ax.text(5, 1.10, 'SSE = {}'.format(ss_stein), fontsize=14)


plt.show()


"
"
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
from scipy.stats import norm

import seaborn as sns
sns.set(style=""ticks"")

sns.set_context(""talk"")

N = 20
num_reps = 100
v = 1.0

true_mean = np.exp(v/2)  # Lognormal case

outcomes_mean = np.empty(num_reps)
outcomes_midrange = np.empty(num_reps)
outcomes_mle = np.empty(num_reps)

for i in range(num_reps):
    y = norm.rvs(size=N)
    x = np.exp(y)
    mu_hat = y.mean()
    sigma2_hat = y.var()
    outcomes_mle[i] = np.exp(mu_hat + sigma2_hat/2.0)
    outcomes_mean[i] = x.mean()
    outcomes_midrange[i] = 0.5 * (x.max() - x.min())


d = {'sample mean' : outcomes_mean,
        'mle' : outcomes_mle,
        'midrange' : outcomes_midrange}

df = pd.DataFrame(d)

fig, ax = plt.subplots()

sns.boxplot(data=df, orient='h', palette=""PRGn"")
sns.despine(offset=10, trim=True)

plt.show()
"
"
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np

# == Data from Compustat == #
filename = ""../data/USsales_2013.csv""
df = pd.read_csv(filename) 
sales = df['sales']

def bootstrap(xo, stat, M):
    N = len(xo)
    gamma_b = np.empty(M)
    for m in range(M):
        x_b = np.random.choice(xo, size=N)
        gamma_b[m] = stat(x_b)
    return gamma_b


xb = bootstrap(sales, np.median, 5000)

se = np.std(xb)
print(se)

fig, ax = plt.subplots(figsize=(10, 8))
ax.hist(xb, bins=40, normed=True, alpha=0.5)

plt.show()
"
"
import matplotlib.pyplot as plt
import numpy as np
from scipy.stats import norm

v = 1.0
true_mean = np.exp(v/2)  # Lognormal case

N = 20
num_reps = 10000
xbar_outcomes = np.empty(num_reps)

for i in range(num_reps):
    x = norm.rvs(size=N)
    x = np.exp(x)
    xbar_outcomes[i] = x.mean()


fig, ax = plt.subplots(figsize=(10, 8))

ax.hist(xbar_outcomes, bins=88, alpha=0.32, normed=True, label='sample mean')
ax.vlines([true_mean], [0], [1.0], lw=2, label='true mean')

ax.set_xlim(0.5, 4)
ax.set_ylim(0, 1)

ax.legend()

plt.show()
"
"
import matplotlib.pyplot as plt
import numpy as np
from scipy.special import kolmogi
from scipy.stats import t

samp_size = 1000
grid_size = 200
df = 10

xgrid = np.linspace(-3, 3, grid_size)

def ecdf(s, X):
    return(np.mean(X <= s))  

X = t.rvs(df, size=samp_size) 
Y = np.empty(grid_size)

for i in range(grid_size):
    Y[i] = ecdf(xgrid[i], X)

Y_upper = Y + 1.36 / np.sqrt(samp_size)
Y_lower = Y - 1.36 / np.sqrt(samp_size)

fig, ax = plt.subplots(figsize=(12, 8))

ax.fill_between(xgrid, Y_lower, Y_upper, color=""blue"", alpha=0.4)

ax.plot(xgrid, t.cdf(xgrid, df), 'k-', lw=2, alpha=0.8, label='true $F$')

ax.set_ylim(0, 1)
ax.set_title(""sample size = {}"".format(samp_size))

ax.legend(loc='upper left', frameon=False)

plt.show()
"
"
import numpy as np
import func_plot_style
from scipy.stats import beta as b
from random import uniform

alpha = 3
beta = 5

theta_0 = 0.7
N = 100


plt, fig, ax = func_plot_style.subplots(figsize=(12, 8))

xmin, xmax = 0.0, 1.0
t = np.linspace(xmin, xmax, 160)

ax.set_xlim(xmin, xmax)

ax.plot(t, b.pdf(t, alpha, beta), '-', lw=2, label='prior')

x = 0
for n in range(N):
    v = int(uniform(0, 1) < theta_0)
    x += v
    alpha_n = alpha + x
    beta_n = n - x + beta
    if not n % 20 and n > 0:
        lb = r'$N={}$'.format(n)
        ax.plot(t, b.pdf(t, alpha_n, beta_n), '-', lw=2, label=lb)
    #ax.plot(t, b.pdf(t, alpha_n, beta_n), 'g-', lw=2, alpha=0.6)

ax.set_xticks((0, theta_0, 1))
ax.set_xticklabels((0, r'$\theta_0$', 1),fontsize=14)

ax.legend(loc='upper left',fontsize=14)

plt.show()


"
"
import numpy as np
from scipy.stats import beta
from matplotlib import pyplot as plt
import seaborn as sns
from matplotlib import rc


sns.set(context='talk')
sns.set_style(""ticks"", {""xtick.major.size"": 4, ""ytick.major.size"": 4})


sample_size = 15 

class ECDF:

    def __init__(self, observations):
        ""Parameters: observations is a NumPy array.""
        self.observations = observations

    def __call__(self, x): 
        return (self.observations <= x).mean()


q = beta(5, 5)
F = q.cdf
grid = np.linspace(0, 1, num=200)
e = ECDF(None)

fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(14, 10))

axes = axes.flatten()

for i in range(4):
    ax = axes[i]
    e.observations = q.rvs(sample_size)
    with sns.color_palette(""muted""):
        ax.plot(grid, F(grid), label=r""$F$"")
        ax.plot(grid, [e(x) for x in grid], label=r""$\hat F_N$"")
        ax.legend(loc=2)

plt.show()
"
"pars = (.52, 1)
x = np.linspace(-10, 10, 50).astype(float)
y, e = line(x[::10], pars)
pl.plot(x[::10], y, 'ro')
"
"
pl.plot(x[::10], y, 'ro')

pl.xlabel('IV')
pl.ylabel('DV')

pl.plot(x, line(x, pars, err=False)[0])"
"for i in range(100):
    pars = (5*(np.random.rand()-0.50), 80*(np.random.rand()-0.5))
    pl.plot(x, line(x, pars, err=False)[0])
pl.xlabel('IV')
pl.ylabel('DV')

pl.plot(x[::10], y, 'ro')
"
"pl.plot(x[::10], y, 'ro')

pl.xlabel('IV')
pl.ylabel('DV')
pars = (2,1)
pl.plot(x, line(x, pars, err=False)[0])"
"reg0 = sm.ols(formula='y ~ X', data = a).fit()
reg0.summary()"
"pl.plot(a.X, a.Y, 'ro')
pl.plot(aall.X, reg0.predict(aall))"
"a['X2'] = a.X**2
aall['X2'] = aall.X**2
reg1 = sm.ols(formula='y ~ X + X2', data = a).fit()
pl.plot(a.X, a.Y, 'ro')
pl.plot(aall.X, reg1.predict(aall))
reg1.summary()"
"a['X3'] = a.X**3
aall['X3'] = aall.X**3
reg2 = sm.ols(formula='y~X + X2 + X3', data = a).fit()
pl.plot(a.X, a.Y, 'ro')
pl.plot(aall.X, reg2.predict(aall))
reg2.summary()"
"a['X4'] = a.X**4
aall['X4'] = aall.X**4
reg3 = sm.ols(formula='y~ X + X2  + X3 + X4', data = a).fit()
pl.plot(a.X, a.Y, 'ro')
pl.plot(aall.X, reg3.predict(aall))
reg3.summary()"
"
pl.errorbar(x[::10], y, yerr=e, fmt='ro')
"
"pl.errorbar(x[::10], y, yerr=e*3, fmt='ro')
pl.plot(aall.X, reg0.predict(aall))
pl.xlabel(""IV"")
pl.ylabel(""DV"")
pl.ylim(-10,15)
chisq(reg0.predict(a), y, e*3, reg0.df_model)
"
"pl.errorbar(x[::10], y, yerr=e, fmt='ro')
pl.plot(aall.X, reg1.predict(aall))
chisq(reg1.predict(a), y, e, reg1.df_model
     )
"
"pl.errorbar(x[::10], y, yerr=e, fmt='ro')
pl.plot(aall.X, reg3.predict(aall))
chisq(reg3.predict(a), y, e, reg3.df_model)
"
"##second degree polynomial: arguments are i
#independent variable, dependent variable, degree of the polynomial

x = np.arange(0, 3, .02)
y1, y2 = 2 * x[::3]**2, 2 * x[::3]**2.05

c = np.exp(x)
d = c[::-1]

pl.plot(x[::3], y1, 'o', color='DarkGreen', label='one line ', ms=10)
pl.plot(x[::3], y2, 'o', color='Red', label='two lines ')
pl.plot(x, c, 'k--', label='Model')
pl.plot(x, d, 'k-', label='wrong model')
pl.plot(x, c + d, 'k-', label='models sum')

#pl.plot(xnew, np.poly1d(mrnew)(xnew), 'y', alpha=1)
legend = pl.legend(loc='upper right', shadow=True, 
                   fontsize='x-large')

pl.xlabel(""some IV (in fed's units)"")
#pl.ylabel(""some DVs (in fed's units)"")"
"##second degree polynomial: arguments are i
#independent variable, dependent variable, degree of the polynomial

ax = pl.figure(figsize=(10,10)).add_subplot(111)

pl.rcParams['font.size']=20
pl.plot(x[::3], y1, 'o', color='DarkGreen', label='one line ', 
        lw = 2, ms=10, markerfacecolor='none')
pl.plot(x[::3], y2, 'o', color='DarkOrange', label='two lines ', 
        alpha=0.5, ms=10)
pl.plot(x, c, 'k--', label='Model')
pl.plot(x, d, 'k-', label='wrong model')
pl.plot(x, c + d, 'k-.', label='models sum')



legend = pl.legend(loc='upper middle', shadow=False)

pl.xlabel(""some IV (in fed's units)"")
pl.ylabel(""some DVs (in fed's units)"")"
"##second degree polynomial: arguments are i
#independent variable, dependent variable, degree of the polynomial

ax = pl.figure(figsize=(10,10)).add_subplot(111)

pl.rcParams['font.size']=20
pl.plot(x[::3], y1, 'o', color='DarkGreen', label='one line ', 
        lw = 2, ms=10, markerfacecolor='none')
pl.plot(x[::3], y2, 'o', color='DarkOrange', label='two lines ', 
        alpha=0.5, ms=10)
pl.plot(x, c, 'k--', label='Model')
pl.plot(x, d, 'k-', label='wrong model')
pl.plot(x, c + d, 'k-.', label='models sum')


from matplotlib.ticker import MultipleLocator, FormatStrFormatter
pl.rcParams['xtick.major.size'] = 40
pl.rcParams['xtick.major.width'] = 1
pl.rcParams['xtick.minor.size'] = 20
pl.rcParams['xtick.minor.width'] = 1
pl.rcParams['ytick.major.size'] = 40
pl.rcParams['ytick.major.width'] = 1
pl.rcParams['ytick.minor.size'] = 20
pl.rcParams['ytick.minor.width'] = 1
pl.grid(""True"")
xminorLocator = MultipleLocator(0.1)
ax.xaxis.set_minor_locator(xminorLocator)
yminorLocator = MultipleLocator(1)
ax.yaxis.set_minor_locator(yminorLocator)


legend = pl.legend(loc='upper middle', shadow=False)

pl.xlabel(""some IV (in fed's units)"")
pl.ylabel(""some DVs (in fed's units)"")"
"gal = pd.read_csv(""MyResult_2017221.csv"")
gal

"
"pl.rcParams.update(pl.rcParamsDefault)
pl.rcParams['font.size']=20
pl.figure(figsize=(10,10))

pl.plot(x, y, '.')
pl.xlabel(""u - g"", fontsize=20)
pl.ylabel(""r - i"", fontsize=20)"
"pl.figure(figsize=(10,10))

pl.plot(x, y, '.', alpha=0.5)

pl.xlabel(""u - g"", fontsize=20)
pl.ylabel(""r - i"", fontsize=20)
"
"H, xbins, ybins = np.histogram2d(x, y, bins=(30,30))
Hsort = np.sort(H.flatten())
Hsort"
"extent = [xbins[0], xbins[-1], ybins[0], ybins[-1]]
ax = pl.figure(figsize=(10,10)).add_subplot(111)
levels = np.linspace(H.max()/15, H.max(), 10)
i_min = np.argmin(levels)
outline = ax.contour(H.T, levels[i_min:i_min + 1],
                         linewidths=0, extent=extent)

#contours = ax.contourf(H.T, levels, extent=extent, cmap='bone')
X = np.hstack([x[:, None], y[:, None]])

if len(outline.allsegs[0]) > 0:
    outer_poly = outline.allsegs[0][0]
    try:
        # this works in newer matplotlib versions
        from matplotlib.path import Path
        points_inside = Path(outer_poly).contains_points(X)
    except:
        # this works in older matplotlib versions
        import matplotlib.nxutils as nx
        points_inside = nx.points_inside_poly(X, outer_poly)

    Xplot = X[~points_inside]
else:
    Xplot = X

points = ax.plot(Xplot[:, 0], Xplot[:, 1], '.', alpha=0.5, c='b')
outline = ax.contour(H.T, levels[i_min:i_min + 1],
                         linewidths=0, extent=extent)
axc = contourf(H.T, extent=extent, levels=levels, linewidths=3, cmap='winter')

"
"xnew = x[(x>-5) * (y>-5) * (x<5) * (y<5)]
ynew = y[(x>-5) * (y>-5) * (x<5) * (y<5)]
H, xbins, ybins = np.histogram2d(xnew, ynew, bins=(30,30))
extent = [xbins[0], xbins[-1], ybins[0], ybins[-1]]

i_min = np.argmin(levels)

ax = pl.figure(figsize=(10,10)).add_subplot(111)
outline = ax.contour(H.T, levels[i_min:i_min + 1],
                         linewidths=0, extent=extent)

#contours = ax.contourf(H.T, levels, extent=extent, cmap='bone')
levels = np.linspace(H.max()/15, H.max(), 10)

X = np.hstack([xnew[:, None], ynew[:, None]])

if len(outline.allsegs[0]) > 0:
    outer_poly = outline.allsegs[0][0]
    try:
        # this works in newer matplotlib versions
        from matplotlib.path import Path
        points_inside = Path(outer_poly).contains_points(X)
    except:
        # this works in older matplotlib versions
        import matplotlib.nxutils as nx
        points_inside = nx.points_inside_poly(X, outer_poly)

    Xplot = X[~points_inside]
else:
    Xplot = X

points = ax.plot(Xplot[:, 0], Xplot[:, 1], '.', alpha=0.5, c='b')
outline = ax.contour(H.T, levels[i_min:i_min + 1],
                         linewidths=0, extent=extent)
axc = contourf(H.T, extent=extent, levels=levels, linewidths=3, cmap='winter')

"
"import warnings
warnings.filterwarnings('ignore')
from IPython.display import Image
Image(filename='img/43_2.png') "
Image(filename='img/44.png')
Image(filename='img/45.png')
Image(filename='img/46.png')
Image(filename='img/47.png')
Image(filename='img/48.png')
"import warnings
warnings.filterwarnings('ignore')
from IPython.display import Image
Image(filename='img/1.png') "
Image(filename='img/2.png') 
Image(filename='img/3.png') 
Image(filename='img/4.png') 
Image(filename='img/5.png') 
Image(filename='img/6.png') 
Image(filename='img/7.png') 
Image(filename='img/8.png') 
Image(filename='img/9.png') 
Image(filename='img/10.png')
Image(filename='img/11.png')
Image(filename='img/12.png')
Image(filename='img/13.png')
Image(filename='img/14.png')
Image(filename='img/15.png')
Image(filename='img/16.png')
Image(filename='img/17.png')
Image(filename='img/18.png')
Image(filename='img/19.png')
Image(filename='img/20.png')
Image(filename='img/21.png')
Image(filename='img/22.png')
Image(filename='img/23.png')
Image(filename='img/24.png')
Image(filename='img/25.png')
Image(filename='img/26.png')
Image(filename='img/27.png')
Image(filename='img/28.png')
Image(filename='img/29.png')
Image(filename='img/30.png')
Image(filename='img/31.png')
Image(filename='img/32.png')
Image(filename='img/33.png')
Image(filename='img/34.png')
Image(filename='img/35.png')
Image(filename='img/36.png')
Image(filename='img/37.png')
Image(filename='img/38.png')
Image(filename='img/39.png')
Image(filename='img/40.png')
Image(filename='img/41.png')
Image(filename='img/42.png')
Image(filename='img/44.png')
"n_pts = 100
m = 2

bias = np.ones(n_pts)
Xa = np.array([bias,
               np.random.normal(10, 2, n_pts),
               np.random.normal(12, 2, n_pts)])
Xb = np.array([bias,
               np.random.normal(5, 2, n_pts),
               np.random.normal(6, 2, n_pts)])

initial_theta = np.matrix([np.zeros(m + 1)]).T
X = np.append(Xa, Xb, axis=1).T
y = np.matrix(np.append(np.zeros(n_pts), np.ones(n_pts))).T

fig, ax = plt.subplots(figsize=(4,4))
ax.scatter(X[:n_pts,1], X[:n_pts,2], color='lightcoral',
          label='$y = 0$')
ax.scatter(X[n_pts:,1], X[n_pts:,2], color='lightblue',
          label='$y = 1$')
ax.set_title('Sample Dataset')
ax.set_xlabel('$x_1$')
ax.set_ylabel('$x_2$')
ax.legend(loc=4);"
"fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(8,4))

x1 = np.array([X[:,1].min()-1, X[:,1].max()+1])
x2 = - theta.item(0) / theta.item(2) + x1 * (- theta.item(1) / theta.item(2))

# Plot decision boundary
ax[0].plot(x1, x2, color='k', ls='--', lw=2)

ax[0].scatter(X[:int(n_pts),1], X[:int(n_pts),2], color='lightcoral', label='$y = 0$')
ax[0].scatter(X[int(n_pts):,1], X[int(n_pts):,2], color='lightblue', label='$y = 1$')
ax[0].set_title('$x_1$ vs. $x_2$')
ax[0].set_xlabel('$x_1$')
ax[0].set_ylabel('$x_2$')
ax[0].legend(loc=4)

ax[1].plot(costs, color='r')
ax[1].set_ylim(0,ax[1].get_ylim()[1])
ax[1].set_title(r'$J(\theta)$ vs. Iteration')
ax[1].set_xlabel('Iteration')
ax[1].set_ylabel(r'$J(\theta)$')

fig.tight_layout()"
"# Generate dataset
X, y = make_blobs(centers=3, n_samples=500, random_state=1)

# Visualize
fig, ax = plt.subplots(figsize=(4,4))
ax.scatter(X[:,0], X[:,1], alpha=0.5)
ax.set_xlabel('$x_1$')
ax.set_ylabel('$x_2$');"
"group_colors = ['skyblue', 'coral', 'lightgreen']
colors = [group_colors[j] for j in classes]

fig, ax = plt.subplots(figsize=(4,4))
ax.scatter(X[:,0], X[:,1], color=colors, alpha=0.5)
ax.scatter(centroids[:,0], centroids[:,1], color=['blue', 'darkred', 'green'], marker='o', lw=2)
ax.set_xlabel('$x_0$')
ax.set_ylabel('$x_1$');"
"fig, ax = plt.subplots()
grid, ax = plot_decision_boundary(output, ax)"
"output = train(model)
fig, ax = plt.subplots()
grid, ax = plot_decision_boundary(output, ax)"
"X, y = datasets.make_moons(n_samples=1000, noise=0.1, random_state=0)
colors = ['darkred' if label == 1 else 'lightblue' for label in y]
plt.scatter(X[:,0], X[:,1], color=colors)
y.shape, X.shape"
"Trained_thetas, costs = train_network(X, y, Thetas, alpha=0.5, max_iter=1000)

fig, ax = plt.subplots()
ax.plot(costs)
ax.set_ylim(0, ax.get_ylim()[1])
ax.set_ylabel('Cost')
ax.set_xlabel('Iteration #');"
"plot_decision_boundary(X, y, Trained_thetas);"
"Trained_thetas, costs = train_network(X[:500], y[:500], Thetas, alpha=0.5, max_iter=5000)

fig, ax = plt.subplots()
ax.plot(costs)
ax.set_ylim(0, ax.get_ylim()[1])
ax.set_ylabel('Cost')
ax.set_xlabel('Iteration #');"
"plot_decision_boundary(X, y, Trained_thetas);"
"Image(filename=""images/pydata_logo.png"", width=600)"
"IFrame(src='https://docs.python.org/3.6/whatsnew/3.6.html', width=1000, height=600)"
"IFrame(src='https://www.pycon.it/en', width=1000, height=600)"
"IFrame(src='https://github.com/samshadwell/TrumpScript', width=1000, height=600)"
"IFrame(src='http://pydata.org/london2017/', width=1000, height=600)"
"Image('images/jupytercon.png', width=1000, height=600)"
"IFrame('http://jamesbvaughan.com/python-twilio-scraping/', width=1024, height=600)"
"IFrame(src=""http://docs.pachyderm.io/en/latest/getting_started/beginner_tutorial.html"", width=1024, height=600)"
"Image('images/review-committee.png', width=1024, height=600)"
"Image(filename=""images/pydata_logo.png"", width=600)"
"Image(""images/fullfact.png"", width=1024, height=600)"
"Image(""images/datakind.png"", width=1024, height=600)"
"Image(""images/genekogan.png"", width=1024, height=600)"
"%matplotlib inline
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np

plt.figure(figsize=(10, 8))

sns.set_style(""whitegrid"")

speaker_data = [
    {
        'year': 2014,
        'percent_female': 0.075,
    },
    {
        'year': 2015,
        'percent_female': 0.1961,
    },
    {
        'year': 2016,
        'percent_female': 0.1538,
    },
    {
        'year': 2017,
        'percent_female': 0.1983,
    }

]

df = pd.DataFrame(data=speaker_data)

# annotation
ax = sns.barplot(x=""year"", y=""percent_female"", data=df)
ax.set_ylim(0,0.25)
ax.set_title(""Percent of female speakers at the PyData Conference"")

# plot Strata female attendance rate 
plt.plot(np.linspace(-1,4,100), [0.21]*100, 'b')"
"IFrame(src='https://sircmpwn.github.io/2017/01/13/The-problem-with-Python-3.html', width=1000, height=600)"
"IFrame(src=""http://mypy-lang.org/"", width=1024, height=600)"
"Image('images/ahl.png', width=1024, height=600)"
"Image(filename=""images/pydata_logo.png"", width=600)"
"Image(filename=""images/pydata_logo.png"", width=600)"
"IFrame(src=""https://www.bloomberg.com/company/london/?utm_source=bloomberg-menu&utm_medium=bcom"", **default_size)"
Image('images/ahl.png')
"IFrame(src=""https://www.pivigo.com/"", **default_size)"
Image('images/jetbrains.png')
"IFrame(src=""https://cambridgespark.com/"", **default_size)"
Image('images/endava.png')
Image('images/oracle.png')
"IFrame('https://www.meetup.com/pro/pydata/', **default_size)"
"IFrame('http://www.scikit-yb.org/en/latest/', **default_size)"
"Image(filename=""images/pydata_logo.png"", width=600)"
"IFrame(src=""https://petl.readthedocs.io/en/latest/"", **default_size)"
Image('images/schedule_pydata_london_2017.png')
Image('images/ahl.png')
"IFrame(src=""https://www.pivigo.com/"", **default_size)"
Image('images/jetbrains.png')
"IFrame(src=""https://cambridgespark.com/"", **default_size)"
Image('images/oracle.png')
Image('images/diversity_form.png')
"IFrame('http://matplotlib.org/style_changes.html', width=1000, height=600)"
"IFrame('https://losc.ligo.org/s/events/GW150914/GW150914_tutorial.html', width=800, height=600)"
"IFrame(src='https://www.theguardian.com/science/2016/feb/11/gravitational-waves-discovery-hailed-as-breakthrough-of-the-century', width=800, height=600)"
"Image(filename=""images/pydata_logo.png"", width=600)"
Image('images/ligo_figure1.png')
Image(filename='images/pydatalondon_cfp.png')
"Image('images/CDF of when members joined each month 2016-03-01.png', width=800, height=600)"
"IFrame('https://pypi.python.org/pypi/numexpr', width=800, height=600)"
"IFrame(src='http://pythonpodcast.com/', width=1000, height=600)"
"Image(filename=""images/pydata_logo.png"", width=600)"
"IFrame(src='http://blog.jupyter.org/2016/07/08/ipython-5-0-released/', height=600, width=1000)"
"IFrame(src='https://phosphorjs.github.io/examples.html', height=600, width=1024)"
"IFrame(src='http://blog.jupyter.org/2016/07/14/jupyter-lab-alpha/', height=600, width=1000)"
Image('images/LIGO_keynote.png')
"IFrame(src='http://ep2016.europython.eu/en/events/pydata/', height=600, width=1024)"
"IFrame(src=""http://2016.pyconuk.org/"", height=600, width=1024)"
Image('images/pycon_ireland.png')
Image('images/CDF of when members joined 2016-08-02.png')
"IFrame('http://blog.yhat.com/posts/words2map.html', width=1000, height=600)"
"Image(filename=""images/pydata_logo.png"", width=600)"
"IFrame(src='http://ep2016.europython.eu/en/events/pydata/', height=600, width=1024)"
Image('images/work_python.png')
"IFrame(src='http://www.sloan.org/major-program-areas/', height=600, width=1000)"
"IFrame(src='http://www.numfocus.org/blog/numfocus-receives-grant-for-project-sustainability', height=600, width=1000)"
"IFrame('http://www.pytables.org/usersguide/tutorials.html', width=1000, height=600)"
img
"import matplotlib.pyplot as plt
%matplotlib inline

plt.imshow(img, cmap='gray')
plt.show()"
img
i
"import unittest

class TestCastle(unittest.TestCase):
    
    def test_candy(self):
        outer = """"
        with Halloween() as context:
            outer = context.candy
        self.assertEqual(outer, [], ""Not OK!"")
        
    def test_trick(self):
        outer = """"
        def func():
            with Halloween() as context:
                raise Trick
        self.assertRaises(Trick, func)
        
a = TestCastle()  # the test suite
suite = unittest.TestLoader().loadTestsFromModule(a) # fancy boilerplate
unittest.TextTestRunner().run(suite)  # run the test suite"
"import unittest

class TestComposer(unittest.TestCase):
    
    def test_composing(self):
        
        def Plus2(x):
            return x + 2
        
        @Composer
        def Times2(x):
            return x * 2
        
        H = Times2 @ Plus2
        self.assertEqual(H(10), 24)

    def test_composing2(self):
        
        def Plus2(x):
            return x + 2
        
        @Composer
        def Times2(x):
            return x * 2
        
        H = Plus2 @ Times2
        self.assertEqual(H(10), 22)
        
    def test_composing3(self):
        
        def Plus2(x):
            return x + 2
        
        @Composer
        def Times2(x):
            return x * 2
        
        H = Plus2 @ Times2
        self.assertEqual(H(10), 22)
        
a = TestComposer()  # the test suite
suite = unittest.TestLoader().loadTestsFromModule(a) # fancy boilerplate
unittest.TextTestRunner().run(suite)  # run the test suite"
"from random import choice

def add_tricks(cls):
    tricks = [""play dead"", ""roll over"", ""sit up""]
    def do_trick(self):
        return choice(tricks)
    cls.do_trick = do_trick
    return cls
    
@add_tricks
class Animal:
    
    def __init__(self, nm):
        self.name = nm

class Mammal(Animal):
    pass

obj = Animal(""Rover"")
print(obj.name, ""does this trick:"", obj.do_trick())

new_obj = Mammal(""Trixy"")
print(new_obj.name, ""does this trick:"", obj.do_trick())"
"import unittest
import sys

class TestComposer(unittest.TestCase):

    def test_simple(self):
        x = 5
        self.assertEqual((f*g*g*f*g*f)(x), f(g(g(f(g(f(x)))))), ""Not same!"")
    
    def test_function(self):
        def addA(s): # not decorated
            return s + ""A""
        @Composable
        def addM(s): 
            return s + ""M""  
            
        addAM = addM * addA  # Composable times regular function, OK?
        self.assertEqual(addAM(""I ""), ""I AM"", ""appends A then M"")
        addMA = addA * addM  # regular function, times Composable OK?
        self.assertEqual(addMA(""HI ""), ""HI MA"", ""appends M then A"")
        
    def test_inputs(self):
        @Composable           
        def f(x):
            ""second powering""
            return x ** 2
        
        self.assertRaises(TypeError, f.__pow__, 2.0)  # float not OK!
        self.assertRaises(TypeError, f.__pow__, g)    # another function? No!
        self.assertRaises(ValueError, f.__pow__, -1)  # negative number? No!
        
    def test_powering(self):
        @Composable           
        def f(x):
            ""second powering""
            return x ** 2
        @Composable
        def g(x):
            ""adding 2""
            return x + 2
        
        self.assertEqual((f*f)(10), 10000, ""2nd power of 2nd power"")
        self.assertEqual(pow(f, 3)(4), f(f(f(4))), ""Powering broken"")        
        h = (f**3) * (g**2)
        self.assertEqual(h(-11), f(f(f(g(g(-11))))), ""Powering broken"")
        self.assertEqual((f**0)(100), 100, ""Identity function"")
        
the_tests = TestComposer()        
suite = unittest.TestLoader().loadTestsFromModule(the_tests)
output = unittest.TextTestRunner(stream=sys.stdout).run(suite)
if output.wasSuccessful():
    print(""All tests passed!"")"
volume_sphere(5)
"volume_cylinder(12,3)"
"six_pack_volume = volume_cylinder(2.5, 5) * 6
print(six_pack_volume)"
"import warnings
warnings.filterwarnings('ignore')
import random
 
for i in range(10):
    x = random.randrange(20)
    print(x)"
"import random
 
for i in range(10):
    x = random.randrange(20)
    print(""{}"".format(x) )"
"import random
 
for i in range(10):
    x = random.randrange(20)
    print(""{:2}"".format(x) )"
"import random

for i in range(10):
    x = random.randrange(100000)
    print(""{:6,}"".format(x) )"
"def get_csv(name):
    path = 'Data/' + name + '.csv'
    return pd.read_csv(path).set_index('State').astype(float)

ObamaMcCain = get_csv('ObamaMcCain')
Population = get_csv('Population2008')['7/1/2008']
Trucks = get_csv('Trucks2008')

print( 'Votes\n' + '-' * 50 )
print( ObamaMcCain.head() )
print( '\nTrucks\n' + '-' * 50 )
print( Trucks.head() )
print( '\nPeople\n' + '-' * 50 )
print( Population.head() )"
"VotePct = ObamaMcCain.apply(lambda x: x / ObamaMcCain['Total'])
VotePct = VotePct[['Obama','McCain','AllOthers']]
VotePct.sort_values('McCain').plot(color=['b','r','g'],kind='bar',stacked='True',ylim=(0,1))"
"Truckness = Trucks['Pickup'] / Population
RedPct = VotePct['McCain'] / (VotePct['McCain'] + VotePct['Obama'])

TruckVotes = pd.concat([Truckness,RedPct,Population],axis=1,join='outer')
TruckVotes.columns = ['Truckness','RedPct','Population']
TruckVotes.sort_values('Truckness',inplace=True)
fBadRow = pd.isnull(TruckVotes).any(axis=1)
print( TruckVotes.head() )
print( ""\n%s of %s rows are missing data"" % (fBadRow.sum(),len(TruckVotes)) )

def redbluedots(df,xname,yname):
    x = df[xname]
    y = df[yname]
    c = df['RedPct']
    dot_size = df['Population'] / 2e4
    plt.scatter(x,y,c=c,s=dot_size,cmap=redwhiteblue,vmin=0.25,vmax=0.75)
    plt.xlabel(xname)
    plt.ylabel(yname)
    
redbluedots(TruckVotes,'Truckness','RedPct')"
"def linear_fit(df,xname,yname):
    x = df[xname]
    y = df[yname]
    slope, intercept, r, p, stderr = stats.linregress(x,y)
    predicted_y = slope * x + intercept
    print( ""slope:\t\t"", round(slope,4) )
    print( ""intercept:\t"", round(intercept,4) )
    print( ""r-squared:\t"", round(r*r,3) )
    print( ""standard error:\t"", round(stderr,2) )
    return predicted_y
TruckVotes['LinearFit'] = linear_fit(TruckVotes,'Truckness','RedPct')

redbluedots(TruckVotes,'Truckness','RedPct')
plt.plot(TruckVotes['Truckness'],TruckVotes['LinearFit'],'ks-',linewidth=2)"
"def show_error(predicted,actual):

    fTrueRed = (predicted > 0.5) & (actual > 0.5)
    fTrueBlue = (predicted < 0.5) & (actual < 0.5)
    fCorrect = fTrueRed | fTrueBlue
    fClose = (predicted - actual).abs() < 0.1
    
    err = actual - predicted
    rms = np.sqrt( (err*err).mean() )
    mad = err.abs().median()
    worst = err.abs().max()
    bias = err.mean()
    
    print( ""Correct:\t%s of %s samples"" % (fCorrect.sum(),len(actual)) )
    print( ""Within 10%%:\t%s of %s samples"" % (fClose.sum(),len(actual)) )
    print( ""RMS error:\t"", round(rms,3) )
    print( ""Typical error:\t"", round(mad,3) )
    print( ""Worst error:\t"", round(worst,3) )
    print( ""Bias:\t\t"", round(bias,3) )
    bar_colors = redwhiteblue(0.5+np.r_[err])
    err.plot(kind='bar',color=bar_colors)
    plt.ylabel('Error')
    plt.ylim((-0.3,0.3))
    
show_error(TruckVotes['LinearFit'],TruckVotes['RedPct'])"
"def logit(s): return np.log(s) - np.log(1-s)
TruckVotes['LogTruckness'] = np.log(TruckVotes['Truckness'])
TruckVotes['Redness'] = logit(TruckVotes['RedPct'])

predicted_redness = linear_fit(TruckVotes,'LogTruckness','Redness')

redbluedots(TruckVotes,'LogTruckness','Redness')
plt.plot(TruckVotes['LogTruckness'],predicted_redness,'ks-',linewidth=2)"
"def logistic(s): return 1.0 / (1.0 + np.exp(-s))
TruckVotes['FancyFit'] = logistic(predicted_redness)
redbluedots(TruckVotes,'Truckness','RedPct')
plt.plot(TruckVotes['Truckness'],TruckVotes['FancyFit'],'ks-',linewidth=2)"
"show_error(TruckVotes['FancyFit'],TruckVotes['RedPct'])"
"# Get 2012 data
ObamaRomney = get_csv('ObamaRomney')
Population2012 = get_csv('Population2012')['2012']
Trucks2012 = get_csv('Trucks2012')

# Calculate Truckness and RedPct
VotePct2012 = ObamaRomney.apply(lambda x: x / ObamaRomney['Total'])
VotePct2012 = VotePct2012[['Obama','Romney','AllOthers']]
RedPct2012 = VotePct2012['Romney'] / (VotePct2012['Romney'] + VotePct2012['Obama'])
Truckness2012 = Trucks2012['Pickup'] / Population2012

# Align, sort, and check for bad data
TruckVotes2012 = pd.concat([Truckness2012,RedPct2012,Population2012],axis=1,join='outer')
TruckVotes2012.columns = ['Truckness','RedPct','Population']
TruckVotes2012.sort_values('Truckness',inplace=True)
fBadRow = pd.isnull(TruckVotes2012).any(axis=1)
print( TruckVotes2012.head() )
print( ""%s of %s rows are missing data"" % (fBadRow.sum(),len(TruckVotes)) )

# Make a predictor function calibrated to 2008 results.
def predict_redness(truckness):
    x = np.log(truckness)
    y = 0.6516*x + 1.1727
    return logistic(y)

# Compare predicted results with actual 2012 results
TruckVotes2012['FancyFit'] = predict_redness(TruckVotes2012['Truckness'])
redbluedots(TruckVotes2012,'Truckness','RedPct')
plt.plot(TruckVotes2012['Truckness'],TruckVotes2012['FancyFit'],'ks-',linewidth=2)"
"show_error(TruckVotes2012['FancyFit'],TruckVotes2012['RedPct'])"
"show_error(TruckVotes['RedPct'],TruckVotes2012['RedPct'])"
"from sklearn.svm import SVC
from sklearn.model_selection import GridSearchCV
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline

pipe_svc = Pipeline([('scl', StandardScaler()),
            ('clf', SVC(random_state=1))])

param_range = [0.0001, 0.001, 0.01, 0.1, 1.0, 10.0, 100.0, 1000.0]

param_grid = [{'clf__C': param_range, 
               'clf__kernel': ['linear']},
                 {'clf__C': param_range, 
                  'clf__gamma': param_range, 
                  'clf__kernel': ['rbf']}]

gs = GridSearchCV(estimator=pipe_svc, 
                  param_grid=param_grid, 
                  scoring='accuracy', 
                  cv=10,
                  n_jobs=-1)
gs = gs.fit(X_train, y_train)
print(gs.best_score_)
print(gs.best_params_)"
"import matplotlib.pyplot as plt
from sklearn.model_selection import learning_curve
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.pipeline import Pipeline
import numpy as np

pipe_lr = Pipeline([('scl', StandardScaler()),
                    ('clf', LogisticRegression(penalty='l2', random_state=0))])

train_sizes, train_scores, test_scores =\
                learning_curve(estimator=pipe_lr,
                               X=X_train,
                               y=y_train,
                               train_sizes=np.linspace(0.1, 1.0, 10),
                               cv=10,
                               n_jobs=1)

train_mean = np.mean(train_scores, axis=1)
train_std = np.std(train_scores, axis=1)
test_mean = np.mean(test_scores, axis=1)
test_std = np.std(test_scores, axis=1)

plt.plot(train_sizes, train_mean,
         color='blue', marker='o',
         markersize=5, label='training accuracy')

plt.fill_between(train_sizes,
                 train_mean + train_std,
                 train_mean - train_std,
                 alpha=0.15, color='blue')

plt.plot(train_sizes, test_mean,
         color='green', linestyle='--',
         marker='s', markersize=5,
         label='validation accuracy')

plt.fill_between(train_sizes,
                 test_mean + test_std,
                 test_mean - test_std,
                 alpha=0.15, color='green')

plt.grid()
plt.xlabel('Number of training samples')
plt.ylabel('Accuracy')
plt.legend(loc='lower right')
plt.ylim([0.8, 1.0])
plt.tight_layout()
plt.show()"
"from sklearn.model_selection import validation_curve

param_range = [0.001, 0.01, 0.1, 1.0, 10.0, 100.0]
train_scores, test_scores = validation_curve(
                estimator=pipe_lr, 
                X=X_train, 
                y=y_train, 
                param_name='clf__C', 
                param_range=param_range,
                cv=10)

train_mean = np.mean(train_scores, axis=1)
train_std = np.std(train_scores, axis=1)
test_mean = np.mean(test_scores, axis=1)
test_std = np.std(test_scores, axis=1)

plt.plot(param_range, train_mean, 
         color='blue', marker='o', 
         markersize=5, label='training accuracy')

plt.fill_between(param_range, train_mean + train_std,
                 train_mean - train_std, alpha=0.15,
                 color='blue')

plt.plot(param_range, test_mean, 
         color='green', linestyle='--', 
         marker='s', markersize=5, 
         label='validation accuracy')

plt.fill_between(param_range, 
                 test_mean + test_std,
                 test_mean - test_std, 
                 alpha=0.15, color='green')

plt.grid()
plt.xscale('log')
plt.legend(loc='lower right')
plt.xlabel('Parameter C')
plt.ylabel('Accuracy')
plt.ylim([0.8, 1.0])
plt.tight_layout()
plt.show()"
"s1 = pd.util.testing.makeTimeSeries(7)
s2 = s1 + np.random.randn(len(s1)) / 10
print(s1)
print(s2)"
s1.cov(s2)
"d1 = pd.util.testing.makeTimeDataFrame()
print (d1.head())
print (d1.cov())"
"s1.corr(s2, method='pearson')"
d1.corr()
[n for n in dir(pd) if n.startswith('rolling')]
"s3 = pd.Series(np.random.randn(1000), index=pd.date_range('1/1/2000', periods=1000))
s3 = s3.cumsum()
s3_max = s3.rolling(60).max()
s3_mean = s3.rolling(60).mean()
s3_min = s3.rolling(60).min()
data = {'cumsum':s3, 'max':s3_max, 'mean':s3_mean, 'min':s3_min}
df = pd.DataFrame(data)
df.tail()"
"import numpy as np

site_names = ['Alpha.com',
              'Bravo.com',
              'Charlie.com',
              'Delta.com',
              'Echo.com',
              'Foxtrot.com']

G = np.array([[0,0,0,1,0,1],
             [1,0,0,0,0,0],
             [0,1,0,0,0,0],
             [0,1,1,0,0,0],
             [0,0,1,0,0,0],
             [1,0,1,0,0,0]])

d = 0.85

def page_rank(G, d):
    
    # find the total number of pages
    N = len(G)
    
    # find the number of links on each page
    c = np.sum(G, axis=0)
    
    # create a weighting matrix based on number of links on each page
    with np.errstate(divide='ignore'): # watch out for divide by zero errors
        c = 1/c
        c[ ~ np.isfinite(c)] = 0 
    D = np.diag(c)

    # create a sparse identity matrix
    I = np.eye(N)
    
    # create a vector column of ones
    e = np.ones((N,1))
    
    # solve for the page ranks
    ranks = np.linalg.solve((I-d*np.dot(G,D)), e)
    ranks = ranks / np.sum(ranks)
    
    return ranks
	
R = page_rank(G, d)

print(""Site Name\t\tPageRank"")
print(""----------\t\t----------"")
for site in zip(site_names, R):
    print(str(site[0]) + '\t\t' + str(site[1]))"
df.head()
sim_trial_df.describe()
seq_trial_df.describe()
seq_trial_df.luminance_contrast.hist(bins=50)
sim_trial_df.luminance_contrast.hist(bins=50)
"subjs_df = df[ (df.catch_type      == 'no_catch')
             & (df.trial_order_num == max_sim_trials)]

cutoff = subjs_df.luminance_contrast.std() * 2
subjs_df = subjs_df[subjs_df.luminance_contrast < cutoff]

plt_sim = sns.barplot(x='subj_id', y='luminance_contrast'
                      , hue='trial_type'
                      , data=subjs_df)"
"plt_sim = sns.barplot(x='subj_id', y='luminance_contrast'
                      , hue='trial_type'
                      , data=subjs_df)
plt_sim.set(ylim=(0,50))"
"gauge(labels=['LOW','MEDIUM','HIGH','EXTREME'], \
      colors=['#007A00','#0063BF','#FFCC00','#ED1C24'], arrow=3, title='something here') "
"gauge(labels=['LOW','MEDIUM','HIGH','VERY HIGH','EXTREME','CRITICAL'], \
      colors='YlOrRd_r', arrow=3, title='drought severity index') "
"gauge(labels=['La Nina','Alert','Watch','Neutral','Watch','Alert','El Nino'], \
      colors='RdBu', arrow=7, title='NIWA ENSO TRACKER')"
"import numpy as np

x=np.array((1, 2, 3, 4, 5))
y=np.array((1.8, 3.8, 4.9, 6, 7.4))


#f=plt.figure(figsize=[8,6])  # Optional command to set figure size

plt.plot(x,y,'ro') # plot with red circles for points. Use 'ro-' to join points with lines.
plt.axis((0,6,0,10)) # optional - if you don't like the default axes limits

plt.xlabel('xdata')
plt.ylabel('ydata')
plt.title('My Data');"
"from scipy.optimize import curve_fit

# define fit function
def fitfnc(x,m,c):
    return m*x+c

pars, cov = curve_fit(fitfnc,x,y)

# pars contains the fitted parameters...
pars"
"plt.plot(x,y,'ro') 
plt.plot(x,fitfnc(x,*pars),'b-') # blue line
plt.axis((0,6,0,9))

plt.xlabel('xdata')
plt.ylabel('ydata')
plt.title(r'My Data + Linear Fit: $y=mx+c$');"
"height = np.array([12,11,13,17,24,22,25,33,31])
age = np.array([1,1,1,2,2,2,3,3,3])
df_1 = pd.DataFrame({'age':age,'height':height,'diet':'a'})
height = np.array([15,17,14,29,28,35,48,53,50])
age = np.array([1,1,1,2,2,2,3,3,3])
df_2 = pd.DataFrame({'age':age,'height':height,'diet':'b'})
df = pd.concat([df_1,df_2]).reset_index(drop=True)
df"
"z, c = np.polyfit(df[df.diet=='a'].age, df[df.diet=='a'].height, 1, cov=True)
slope_diet_a = z[0]
intercept_diet_a = z[1]
stdev_slope_diet_a = np.sqrt(c[0][0])
stdev_intercept_diet_a = np.sqrt(c[1][1])
print(""slope = %.2f +/- %.2f""%(slope_diet_a,stdev_slope_diet_a ))
print(""intercept = %.2f +/- %.2f""%(intercept_diet_a, stdev_intercept_diet_a ))"
"z, c = np.polyfit(df[df.diet=='b'].age, df[df.diet=='b'].height, 1, cov=True)
slope_diet_b = z[0]
intercept_diet_b = z[1]
stdev_slope_diet_b = np.sqrt(c[0][0])
stdev_intercept_diet_b = np.sqrt(c[1][1])
print(""slope = %.2f +/- %.2f""%(slope_diet_b,stdev_slope_diet_b ))
print(""intercept = %.2f +/- %.2f""%(intercept_diet_b, stdev_intercept_diet_b ))"
"ratio = slope_diet_a / slope_diet_b
stdev_ratio = ratio * np.sqrt( (stdev_slope_diet_a / slope_diet_a)**2 + (stdev_slope_diet_b/slope_diet_b)**2 )
print(""ratio = %.2f +/- %.2f""%(ratio,stdev_ratio))"
"formula = 'height ~ age * C(diet)'  # ANCOVA formula
lm = ols(formula, df)
fit = lm.fit()
print(fit.summary())"
"from sklearn.linear_model import LogisticRegression

clf = LogisticRegression(C=1e10) # some large number for C

feature = ['Sex']
clf.fit(x_train[feature], y_train)"
"print('intercept:', clf.intercept_)
print('coefficient:', clf.coef_[0])"
feature[100]
"# Class Version Test
TMP = Adaline()
TMP.set_training_data(feature,output)
for i in range(1,20):
    TMP.train()"
pprint(sys.path)
c_count(1000000)
count(1000000)
"random.normalvariate(10, 3)"
"p = ModelParameters(all_s0=10, all_time=1200, all_delta=1/252, all_sigma=1, gbm_mu=0)

plot_stochastic_processes([brownian_motion_levels(p)], 'testing')"
"p = ModelParameters(all_s0=10, all_time=1200, all_delta=1/252, all_sigma=1, gbm_mu=0.5)

plot_stochastic_processes([geometric_brownian_motion_levels(p)], 'testing')"
"p = ModelParameters(all_s0=10, all_time=1200, all_delta=1/252, all_sigma=0.9, gbm_mu=0.5,
                    jumps_lamda=0.005, jumps_sigma=0.02, jumps_mu=0.1)

plot_stochastic_processes([geometric_brownian_motion_jump_diffusion_levels(p)], 'testing')"
"p = ModelParameters(all_s0=10, all_time=1200, all_delta=1/252, all_sigma=0.9, gbm_mu=0.8,
                    jumps_lamda=0.005, jumps_sigma=0.02, jumps_mu=0.1,
                    cir_a=0.0, cir_mu=0.0, all_r0=0.0, cir_rho=0.0,
                    ou_a=0.0, ou_mu=0.0,
                    heston_a=1, heston_mu=1, heston_vol0=1)


proc, _ = heston_model_levels(p)
plot_stochastic_processes([proc], 'testing')"
"r = (Decimal(23000)/Decimal(14000))**(Decimal(1)/Decimal(4)) - Decimal(1)
# Now we need to convert interest rate to a percentage
r = r*100
print(r)"
"# First, find the trade in value of each model at end of 3 years, FV terms
fv_tradein_A = Decimal(24000*0.50)
fv_tradein_B = Decimal(18000*0.25)

# Next, find the trade in values in current time, PV terms
pv_tradein_A = present_value(r = 6, n = 3, fv = fv_tradein_A)
pv_tradein_B = present_value(r = 6, n = 3, fv = fv_tradein_B)

# Compute the difference with current car prices to find the cheapest option
net_cost_A = 24000 - pv_tradein_A
net_cost_B = 18000 - pv_tradein_B

print(net_cost_A)
print(net_cost_B)"
print(net_cost_B - net_cost_A)
"# Calculating the individual PV values for each year of college
pv_each_col_year = []

for i in range(9, 13):
    # New college tuition at the ith year 
    fv = future_value(r=7, n=i, pv=10600)
    # Already knowing what college tuition will cost at the ith year, 
    # how much then will it cost Andrea to pay it TODAY with his 
    # investments giving him 2% interest
    pv = present_value(r = 2, n = i, fv = fv)
    pv_each_col_year.append(pv)
    
print(sum(pv_each_col_year))"
"print(present_value(r=7, n=35, pmt = 11000))"
"pmt_month = payment(r=r_month, n=30*12, pv=mtg)
print(pmt_month)"
"r_ear = effective_annual_rate(qr = 5.75, m=12)
print(r_ear)
pmt_yr = payment(r=r_ear, n=30, pv = mtg) 
print(pmt_yr)"
"fv_month = future_value(r=r_month, n = 30*12, pmt=pmt_month)
fv_yr = future_value(r=r_ear, n=30, pmt=pmt_yr)
print(fv_month)
print(fv_yr)"
"car_loan_amt = Decimal(13000-2500)
r_month = Decimal(7)/Decimal(12) # This is the monthly rate of the original loan
pmt_original_month = payment(r=r_month, n=60, pv=car_loan_amt)
print(pmt_original_month)"
"# Since we know that two years is 24 months, find the loan balance 
# after two years have passed as a PV, as what was taught to us in 
# lecture 2.7
loan_bal_2years = present_value(r=r_month, n=60-24, pmt=pmt_month)
print(loan_bal_2years)"
"r_refin_month = Decimal(4)/Decimal(12)
pmt_refinance_month = payment(r=r_refin_month, n=36, pv=loan_bal_2years)
print(pmt_refinance_month)"
print(pmt_refinance_month - pmt_original_month)
"from IPython.display import Image
Image(filename='./images/assignment2_question9.png') "
"orig_rate = Decimal(9)/Decimal(12)
loan_amt = Decimal(300000)*Decimal(0.8)
print(loan_amt)
pmt_orig_month = payment(r=orig_rate, n=240, pv=loan_amt)
print(pmt_orig_month)"
"loan_bal_60m = present_value(r=orig_rate, n=240-60, pmt=pmt_orig_month)
print(loan_bal_60m)"
"refin_rate = Decimal(3.5)/Decimal(12)
pmt_refin_month = payment(r=refin_rate, n=240-60, pv=loan_bal_60m)
print(pmt_refin_month)"
"sav_monthly = pmt_orig_month - pmt_refin_month
print(sav_monthly)
total_sav = present_value(r=refin_rate, n=240-60, pmt=sav_monthly) - 4500
print(total_sav)"
"# Given the interest rate as 4%
pv_monthly_payments = present_value(r=4, n=36, pmt=225)
pv_final_payment = present_value(r=4, n=36, fv=10900)
# Total cost of buying from B
cost_B = Decimal(550) + pv_monthly_payments + pv_final_payment
print(cost_B)"
"r_month = Decimal(4)/Decimal(12)
pv_monthly_payments = present_value(r=r_month, n=36, pmt=225)
pv_final_payment = present_value(r=r_month, n=36, fv=10900)
# Total cost of buying from B
cost_B = Decimal(550) + pv_monthly_payments + pv_final_payment
print(cost_B)"
"pmt_C = Decimal(19000)/Decimal(36)
print(pmt_C)
cost_C = present_value(r=r_month, n=36, pmt=pmt_C)
print(cost_C)"
"pairings_df = mmt.create_pairings_df(todays_players)
pairings_df.head()"
"# Populate pairings_df with average scores and assign to score_matchups
score_matchups = mmt.generate_score_mmr(pairings_df, input_data)
score_matchups.head()"
"# Plot a heatmap of matchups, where diagonal is player personal average
plt.figure(figsize=(15,10))
ax = sns.heatmap(score_matchups, cmap='bwr_r', square=True, vmin=-45, vmax=45)
ax.xaxis.tick_top()"
"aggregate_data = mmt.playerstats(input_data, todays_players)
aggregate_data"
"subset['Mean rank'].plot(yerr=subset['Stdev rank'], kind='bar', figsize=(15,6))"
"x=np.arange(0,10,0.1)
y=np.sin(x)
plt.plot(x,y,'-o')"
"import warnings
warnings.filterwarnings('ignore')
import numpy as np
import pandas as pd
from datetime import datetime
from IPython.display import display

display(np.datetime64(datetime.utcnow(), 'ns'))
display(np.datetime64(datetime.utcnow()).astype(datetime))

display(pd.Timestamp(datetime.utcnow()))
display(pd.Timestamp(datetime.utcnow()).to_pydatetime())

display(pd.Timestamp(np.datetime64(datetime.utcnow())))
display(np.datetime64(pd.Timestamp(np.datetime64(datetime.utcnow()))))
"
"import random
import numpy as np

from sklearn.model_selection import train_test_split
from sklearn import svm


data_num = 100

data = np.empty(shape=(data_num, 4))

for i in range(0, data_num):
    data[i] = [random.choice([-1, 1]), random.randint(0, 10), random.uniform(0, 10), random.uniform(0, 10)]

x = data[:, 1:]
y = data[:, 0]

x_train, x_test, y_train, y_test = train_test_split(x, y, random_state=1)

clf = svm.SVC()
clf.fit(x_train, y_train)

test_prediction = clf.predict(x_test)
train_prediction = clf.predict(x_train)

test_prediction_result = []
for i in range(0, len(test_prediction)):
    if y_test[i] == test_prediction[i]:
        test_prediction_result.append(1)
    else:
        test_prediction_result.append(0)
        
train_prediction_result = []
for i in range(0, len(train_prediction)):
    if y_train[i] == train_prediction[i]:
        train_prediction_result.append(1)
    else:
        train_prediction_result.append(0)

print(sum(test_prediction_result) / len(test_prediction_result))
print(sum(train_prediction_result) / len(train_prediction_result))
"
"# this time series looks to have some yearly seasonality component to it
fig, ax = plt.subplots(figsize=(20, 10))
ax.plot_date(ts.index, ts, linestyle='--')"
stationary_test(ts)
"# try differencing to eliminate seasonality
ts_diff = ts - ts.shift(12) # not sure what arg to put here, get error with ADF anyways
plt.plot(ts_diff)"
stationary_test(ts_diff)
"# try log
ts_log = np.log(ts)
plt.plot(ts_log)"
stationary_test(ts_log)
"plt.subplot(411)
plt.plot(ts_log, label='Original')
plt.legend(loc='best')
plt.subplot(412)
plt.plot(trend, label='Trend')
plt.legend(loc='best')
plt.subplot(413)
plt.plot(seasonal,label='Seasonality')
plt.legend(loc='best')
plt.subplot(414)
plt.plot(residual, label='Residuals')
plt.legend(loc='best')
plt.tight_layout()"
"ts_log_residual = residual
ts_log_residual.dropna(inplace=True)
stationary_test(ts_log_residual)"
"# example plot
fig = plt.figure(figsize=(12,6)); ax = fig.add_subplot(1,1,1)
ax.set(xlim = (-2*R, 2*R), ylim = (-R - 0.1, R + 0.1), aspect = 1)
_tmp = ax.axis('off')

# circle
_tmp = ax.add_patch(patches.Circle((0,0), radius = R, fc = 'lightgrey', lw = 1))
# equilateral triangle inscribed to circle 
_tmp = ax.add_patch(patches.RegularPolygon((0,0), 3, R, lw = 3, fill = False))
# add two chords, one shorter & one longer than triangle's side
_tmp = add_chord(ax, R, np.pi/3, -np.pi/12, ""blue"", 3)
_tmp = add_chord(ax, R, 5*np.pi/12, -2*np.pi/3, ""green"", 3)"
"plot_length_distr_and_chords(lengths, R, solution_A_pct,
        triangle_edge_length, sol_A_coord_a, sol_A_coord_b, N)"
"plot_length_distr_and_chords(lengths, R, solution_B_pct,
        triangle_edge_length, sol_B_coord_a, sol_B_coord_b, N)"
"plot_length_distr_and_chords(lengths, R, solution_C_pct,
        triangle_edge_length, sol_C_coord_a, sol_C_coord_b, N)"
"import warnings
warnings.filterwarnings('ignore')
%run -i turing.py
init()"
"simulate(transitions_copy, input='10011', unary=False)"
"simulate(transitions_power, input_unary=16, step_to=200, unary=True)"
"# pmf and cdf for a pair of rolled dice.

def px(s):
    s1 = s-1
    s13 = 13-s
    return np.min([s1,s13]) / 36.

def Fx(pmf):
    """"""
    $F_x(x) = \textstyle \sum_{x}^x_i x_i`

    all that is really required:
    return np.cumsum(pmf)
    """"""
    cum = 0
    cumulative = []

    for fx in pmf:
        cum += fx
        cumulative.append(cum)

    return cumulative

#sample space
S = np.arange(2,13)

pmf = [px(s) for s in S]


## Plot it up..
f, ax1 = plt.subplots(figsize=[12,6])

## Axis 1, pmf
ax1.bar(S, pmf)
ax1.set_ylabel('$f_x(x)$')

## CDF
ax2 = ax1.twinx()
ax2.step(S+1,Fx(pmf), 'y', lw=3)
ax2.set_ylim([0,1.])
ax2.set_ylabel('$F_x(x)$')

## Title
plt.title('$pmf$ and $cdf$ for two rolled dice example')
plt.grid(True)"
"#Example 2/17, Hayler(2012)
#Suppose that the probability density function of the amount of milk deposited in a milk
#container is
from matplotlib.patches import Polygon

def pdf(X, f, a, b):
    """"""
    define a pdf based on func
    """"""
    val = []
    for x in X:
        if x < a:
            val.append(0)
        elif x > b:
            val.append(0)
        else:
            val.append(f(x))
    return np.array(val)

def f(x):
    return 40.976 - 16*x - 30*np.exp(-x)

#for 1.95 ≤ x ≤ 2.20 and f (x) = 0 elsewhere. This is a valid probability density function
#since it is positive within the state space [1.95, 2.20] and
#2.20

#(40.976 − 16x − 30e −x ) d x = [40.976x − 8x 2 + 30e −x ] 2.20
# 1.95
# 1.95
# = 54.751 − 53.751 = 1
#Figure 2.24 illustrates the area that corresponds to the probability that the actual amount
# of milk is less than the advertised 2.00 liters. The area can be calculated to be
#2.00
#(40.976 − 16x − 30e −x ) d x = [40.976x − 8x 2 + 30e −x ] 2.00
#1.95
#1.95
#= 54.012 − 53.751 = 0.261
#Consequently, about 26% of the milk containers are underweight.

#for 1.95 ≤ x ≤ 2.20 and f (x) = 0 elsewhere.

ax = plt.subplot(111)
a, b = 1.95, 2.20 # integral area
X = np.arange(0, 3, 0.001)
y = pdf(X, f, a, b)
plt.plot(X, y, linewidth=1)
# make the shaded region
ix = np.arange(a, b, 0.01)
iy = f(ix)
verts = [(a,0)] + list(zip(ix,iy)) + [(b,0)]
poly = Polygon(verts, facecolor='0.8', edgecolor='k')
ax.add_patch(poly)

plt.text(0.5 * (a + b), 1,
     r""$\int_a^b f(x)\mathrm{d}x$"", horizontalalignment='center',
     fontsize=20)

plt.axis([1.9,2.3, 0, 8])
plt.figtext(0.9, 0.05, 'x')
plt.figtext(0.1, 0.9, 'y')
ax.set_xticks((a,b))
ax.set_xticklabels(('a','b'))
ax.set_yticks([])
plt.title(r'example $p.d.f.$')
plt.show()"
"#Example 2/15 Hayler, 2012
#Suppose that the battery failure time, measured in hours, has a probability density function
#given by

from __future__ import division
def f(x):
    return 2 / ((x+1)**3)

def F(X):
    return 1 - (1 / (X + 1)**2)
#for x ≥ 0 and f (x) = 0 for x < 0. This is a valid probability density function because it is

ax = plt.subplot(111)
a, b = 0, 1000 # integral area
X = np.arange(0, 7, 0.1)
y = pdf(X, f, a, b)
plt.plot(X, y, linewidth=1)
plt.grid(True)
plt.title('PDF of battery failure')

plt.figure()
Y = F(X)
plt.plot(X, Y)
plt.xlabel('hours')
plt.grid(True)
plt.title('CDF of battery failure')"
"#!/usr/bin/env python
# source: http://matplotlib.org/1.2.1/examples/pylab_examples/integral_demo.html
# modified jfb to use as pdf example, but currently no accurate (e.g. f(x) does not satisfy all conditions)
# implement the example graphs/integral from pyx
from matplotlib.patches import Polygon

def pdf(X):
    """"""
    define a pdf based on func
    """"""
    val = []
    for x in X:
        if x <= 4:
            val.append(0)
        elif x >= 8:
            val.append(0)
        else:
            val.append(func(x))
    return np.array(val)

def func(x):
    return (x-3)*(x-5)*(x-7)+85

def Fx(pmf):
    return np.cumsum(pmf)

ax = plt.subplot(111)

a, b = 4, 8 # integral area
X = np.arange(0, 10, 0.01)
y = pdf(X)
plt.plot(X, y, linewidth=1)


# make the shaded region
ix = np.arange(a, b, 0.01)
iy = func(ix)
verts = [(a,0)] + list(zip(ix,iy)) + [(b,0)]
poly = Polygon(verts, facecolor='0.8', edgecolor='k')
ax.add_patch(poly)

plt.text(0.5 * (a + b), 30,
     r""$\int_a^b f(x)\mathrm{d}x$"", horizontalalignment='center',
     fontsize=20)

plt.axis([2,9, 0, 110])
plt.figtext(0.9, 0.05, 'x')
plt.figtext(0.1, 0.9, 'y')
ax.set_xticks((a,b))
ax.set_xticklabels(('a','b'))
ax.set_yticks([])
plt.title(r'example $p.d.f.$')
plt.show()"
"# Your answer is probably different
(20000 - (10 ** 2) / 12 * 34) - 19627.75"
2/3
"# fit and summarize OLS model
mod = sm.OLS(spector_data.endog, spector_data.exog)
res = mod.fit()
print(res.summary())"
"# fit and summary
model = sm.OLS(y, X)
results = model.fit()
print(results.summary())"
"print('Parameters: ', results.params)
print('R2: ', results.rsquared)"
"# fit and summary
res = sm.OLS(y, X).fit()
print(res.summary())"
"print('Parameters: ', res.params)
print('Standard errors: ', res.bse)
print('Predicted values: ', res.predict())"
"# plot to compare the true relationship to OLS predictions
prstd, iv_l, iv_u = wls_prediction_std(res)
fig, ax = plt.subplots(figsize=(8,6))

ax.plot(x, y, 'o', label=""data"")
ax.plot(x, y_true, 'b-', label=""True"")
ax.plot(x, res.fittedvalues, 'r--.', label=""OLS"")
ax.plot(x, iv_u, 'r--')
ax.plot(x, iv_l, 'r--')
ax.legend(loc='best');"
"# look at the tracts we got back
tracts[['GEOID', 'AREALAND', 'AREAWATER']].head()"
"# create a Series of true/false, indicating if each row in the column is equal to some value
df['city']=='Munich'"
not_bcn['city'].unique()
"# use bitwise OR to create a boolean vector of which rows are a weekend date
weekend_mask = (dt.index.weekday==6) | (dt.index.weekday==0)
weekend_mask"
"hourly_share = pd.DataFrame()

# calculate what share of the weekday observations each hour has
weekday_hourly = weekdays.groupby(weekdays.index.hour).size()
hourly_share['weekday'] = weekday_hourly / sum(weekday_hourly)

# calculate what share of the weekend observations each hour has
weekend_hourly = weekends.groupby(weekends.index.hour).size()
hourly_share['weekend'] = weekend_hourly / sum(weekend_hourly)

# format the x-axis ticks like 0:00 times and plot the data
hourly_share.index = [s + ':00' for s in hourly_share.index.astype(str)]
hourly_share.plot(figsize=[9, 4], kind='bar', stacked=False, alpha=0.7, title='Share of observations, by hour')"
"# calculate and plot the number of observations each day of the week has
daily_count = dt.groupby(dt.index.weekday).size()
daily_count.index = ['Sunday', 'Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday']
daily_count.plot(figsize=[8, 5], kind='bar', title='Number of observations, by day of week')"
"# create a new DataFrame with fake year data
df = pd.DataFrame({'start_year':[2001, 2002, 2005, 2005, 2006], 
                   'end_year':[2002, 2010, 2008, 2006, 2014]})
df"
"# you can easily create a new column to contain the results of the function mapping
df['new_year'] = df['start_year'].map(get_new_year)
df.head()"
"# applies a function to calculate the difference between the min and max values in each column (ie, row-wise)
def get_difference(vector):
    difference = vector.max() - vector.min()
    return difference

df.apply(get_difference, axis=0)"
"# same thing again, using a lambda function
df.apply(lambda x: x.max() - x.min(), axis=0)"
"# here .apply() finds the difference between the min and max values in each row (ie, column-wise) and saves to a new column
df['difference'] = df.apply(get_difference, axis=1)
df"
"# divide every value in the dataframe by 2 (use a float so you don't do rounded integer division)
df.applymap(lambda x: x / 2.)"
print(model)
"plt.plot(x, y, 'o');"
"# fit the model on our data
model.fit(X, y)"
"# underscore at the end indicates a fit parameter
print(model.coef_)
print(model.intercept_)"
"knn.predict_proba([[3, 5, 4, 2],])"
"from fig_code import plot_iris_knn
plot_iris_knn()"
"# Create some simple data
import numpy as np
np.random.seed(0)
X = np.random.random(size=(20, 1))
y = 3 * X.squeeze() + 2 + np.random.randn(20)

plt.plot(X.squeeze(), y, 'o');"
"model = LinearRegression()
model.fit(X, y)

# Plot the data and the model prediction
X_fit = np.linspace(0, 1, 100)[:, np.newaxis]
y_fit = model.predict(X_fit)

plt.plot(X.squeeze(), y, 'o')
plt.plot(X_fit.squeeze(), y_fit);"
"# Fit a Random Forest
from sklearn.ensemble import RandomForestRegressor
model = RandomForestRegressor()
model.fit(X, y)

# Plot the data and the model prediction
X_fit = np.linspace(0, 1, 100)[:, np.newaxis]
y_fit = model.predict(X_fit)

plt.plot(X.squeeze(), y, 'o')
plt.plot(X_fit.squeeze(), y_fit);"
"import pylab as plt
plt.scatter(X_reduced[:, 0], X_reduced[:, 1], c=y,
           cmap='RdYlBu')

print(""Meaning of the 2 components:"")
for component in pca.components_:
    print("" + "".join(""%.3f x %s"" % (value, name)
                     for value, name in zip(component,
                                            iris.feature_names)))"
"from sklearn.cluster import KMeans
k_means = KMeans(n_clusters=3, random_state=0) # Fixing the RNG in kmeans
k_means.fit(X)
y_pred = k_means.predict(X)

plt.scatter(X_reduced[:, 0], X_reduced[:, 1], c=y_pred,
           cmap='RdYlBu');"
"fig, axes = plt.subplots(10, 10, figsize=(8, 8))
fig.subplots_adjust(hspace=0.1, wspace=0.1)

for i, ax in enumerate(axes.flat):
    ax.imshow(digits.images[i], cmap='binary', interpolation='nearest')
    ax.text(0.05, 0.05, str(digits.target[i]),
            transform=ax.transAxes, color='green')
    ax.set_xticks([])
    ax.set_yticks([])"
"# The images themselves
print(digits.images.shape)
print(digits.images[0])"
"# The data for use in our algorithms
print(digits.data.shape)
print(digits.data[0])"
"# The target label
print(digits.target)"
"plt.scatter(data_projected[:, 0], data_projected[:, 1], c=digits.target,
            edgecolor='none', alpha=0.5, cmap=plt.cm.get_cmap('nipy_spectral', 10));
plt.colorbar(label='digit label', ticks=range(10))
plt.clim(-0.5, 9.5)"
"from sklearn.metrics import accuracy_score
accuracy_score(ytest, ypred)"
"plt.imshow(np.log(confusion_matrix(ytest, ypred)),
           cmap='Blues', interpolation='nearest')
plt.grid(False)
plt.ylabel('true')
plt.xlabel('predicted');"
"fig, axes = plt.subplots(10, 10, figsize=(8, 8))
fig.subplots_adjust(hspace=0.1, wspace=0.1)

for i, ax in enumerate(axes.flat):
    ax.imshow(Xtest[i].reshape(8, 8), cmap='binary')
    ax.text(0.05, 0.05, str(ypred[i]),
            transform=ax.transAxes,
            color='green' if (ytest[i] == ypred[i]) else 'red')
    ax.set_xticks([])
    ax.set_yticks([])"
"# Import the example plot from the figures directory
from fig_code import plot_sgd_separator
plot_sgd_separator()"
"from fig_code import plot_linear_regression
plot_linear_regression()"
iris.keys()
"n_samples, n_features = iris.data.shape
print((n_samples, n_features))
print(iris.data[0])"
"import numpy as np
import matplotlib.pyplot as plt

x_index = 0
y_index = 1

# this formatter will label the colorbar with the correct target names
formatter = plt.FuncFormatter(lambda i, *args: iris.target_names[int(i)])

plt.scatter(iris.data[:, x_index], iris.data[:, y_index],
            c=iris.target, cmap=plt.cm.get_cmap('RdYlBu', 3))
plt.colorbar(ticks=[0, 1, 2], format=formatter)
plt.clim(-0.5, 2.5)
plt.xlabel(iris.feature_names[x_index])
plt.ylabel(iris.feature_names[y_index]);"
"import fig_code
fig_code.plot_example_decision_tree()"
"from sklearn.datasets import make_blobs

X, y = make_blobs(n_samples=300, centers=4,
                  random_state=0, cluster_std=1.0)
plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='rainbow');"
"plot_tree_interactive(X, y);"
"from sklearn.tree import DecisionTreeClassifier
clf = DecisionTreeClassifier()

plt.figure()
visualize_tree(clf, X[:200], y[:200], boundaries=False)
plt.figure()
visualize_tree(clf, X[-200:], y[-200:], boundaries=False)"
"def fit_randomized_tree(random_state=0):
    X, y = make_blobs(n_samples=300, centers=4,
                      random_state=0, cluster_std=2.0)
    clf = DecisionTreeClassifier(max_depth=15)
    
    rng = np.random.RandomState(random_state)
    i = np.arange(len(y))
    rng.shuffle(i)
    visualize_tree(clf, X[i[:250]], y[i[:250]], boundaries=False,
                   xlim=(X[:, 0].min(), X[:, 0].max()),
                   ylim=(X[:, 1].min(), X[:, 1].max()))
    
from ipywidgets import interact
interact(fit_randomized_tree, random_state=(0, 100));"
"from sklearn.ensemble import RandomForestClassifier
clf = RandomForestClassifier(n_estimators=100, random_state=0)
visualize_tree(clf, X, y, boundaries=False);"
"from sklearn.ensemble import RandomForestRegressor

x = 10 * np.random.rand(100)

def model(x, sigma=0.3):
    fast_oscillation = np.sin(5 * x)
    slow_oscillation = np.sin(0.5 * x)
    noise = sigma * np.random.randn(len(x))

    return slow_oscillation + fast_oscillation + noise

y = model(x)
plt.errorbar(x, y, 0.3, fmt='o');"
"xfit = np.linspace(0, 10, 1000)
yfit = RandomForestRegressor(100).fit(x[:, None], y).predict(xfit[:, None])
ytrue = model(xfit, 0)

plt.errorbar(x, y, 0.3, fmt='o')
plt.plot(xfit, yfit, '-r');
plt.plot(xfit, ytrue, '-k', alpha=0.5);"
"# set up the figure
fig = plt.figure(figsize=(6, 6))  # figure size in inches
fig.subplots_adjust(left=0, right=1, bottom=0, top=1, hspace=0.05, wspace=0.05)

# plot the digits: each image is 8x8 pixels
for i in range(64):
    ax = fig.add_subplot(8, 8, i + 1, xticks=[], yticks=[])
    ax.imshow(digits.images[i], cmap=plt.cm.binary, interpolation='nearest')
    
    # label the image with the target value
    ax.text(0, 7, str(digits.target[i]))"
"metrics.accuracy_score(ypred, ytest)"
"plt.imshow(metrics.confusion_matrix(ypred, ytest),
           interpolation='nearest', cmap=plt.cm.binary)
plt.grid(False)
plt.colorbar()
plt.xlabel(""predicted label"")
plt.ylabel(""true label"");"
"import fig_code
fig_code.plot_example_decision_tree()"
"from sklearn.datasets import make_blobs

X, y = make_blobs(n_samples=300, centers=4,
                  random_state=0, cluster_std=1.0)
plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='rainbow');"
"plot_tree_interactive(X, y);"
"from sklearn.tree import DecisionTreeClassifier
clf = DecisionTreeClassifier()

plt.figure()
visualize_tree(clf, X[:200], y[:200], boundaries=False)
plt.figure()
visualize_tree(clf, X[-200:], y[-200:], boundaries=False)"
"def fit_randomized_tree(random_state=0):
    X, y = make_blobs(n_samples=300, centers=4,
                      random_state=0, cluster_std=2.0)
    clf = DecisionTreeClassifier(max_depth=15)
    
    rng = np.random.RandomState(random_state)
    i = np.arange(len(y))
    rng.shuffle(i)
    visualize_tree(clf, X[i[:250]], y[i[:250]], boundaries=False,
                   xlim=(X[:, 0].min(), X[:, 0].max()),
                   ylim=(X[:, 1].min(), X[:, 1].max()))
    
from ipywidgets import interact
interact(fit_randomized_tree, random_state=(0, 100));"
"from sklearn.ensemble import RandomForestClassifier
clf = RandomForestClassifier(n_estimators=100, random_state=0)
visualize_tree(clf, X, y, boundaries=False);"
"from sklearn.ensemble import RandomForestRegressor

x = 10 * np.random.rand(100)

def model(x, sigma=0.3):
    fast_oscillation = np.sin(5 * x)
    slow_oscillation = np.sin(0.5 * x)
    noise = sigma * np.random.randn(len(x))

    return slow_oscillation + fast_oscillation + noise

y = model(x)
plt.errorbar(x, y, 0.3, fmt='o');"
"xfit = np.linspace(0, 10, 1000)
yfit = RandomForestRegressor(100).fit(x[:, None], y).predict(xfit[:, None])
ytrue = model(xfit, 0)

plt.errorbar(x, y, 0.3, fmt='o')
plt.plot(xfit, yfit, '-r');
plt.plot(xfit, ytrue, '-k', alpha=0.5);"
"# set up the figure
fig = plt.figure(figsize=(6, 6))  # figure size in inches
fig.subplots_adjust(left=0, right=1, bottom=0, top=1, hspace=0.05, wspace=0.05)

# plot the digits: each image is 8x8 pixels
for i in range(64):
    ax = fig.add_subplot(8, 8, i + 1, xticks=[], yticks=[])
    ax.imshow(digits.images[i], cmap=plt.cm.binary, interpolation='nearest')
    
    # label the image with the target value
    ax.text(0, 7, str(digits.target[i]))"
"metrics.accuracy_score(ypred, ytest)"
"plt.imshow(metrics.confusion_matrix(ypred, ytest),
           interpolation='nearest', cmap=plt.cm.binary)
plt.grid(False)
plt.colorbar()
plt.xlabel(""predicted label"")
plt.ylabel(""true label"");"
"import warnings
warnings.filterwarnings('ignore')
!cat /biipy/version"
np.__config__.show()
"%matplotlib inline
import matplotlib.pyplot as plt
plt.plot(a);"
%timeit f(x)
%timeit fjit(x)
"p = figure()
p.circle(x, y, radius=radii, fill_color=colors, fill_alpha=.9, line_color=None)"
show(p)
"import warnings
warnings.filterwarnings('ignore')
%load_ext displaytools3
%reload_ext displaytools3"
"x = 2*pi*t
y1 = cos(x)
y2 = cos(x)*t
ydot1 = y1.diff(t) ##
ydot2 = y2.diff(t) ##
ydot1_obj = y1.diff(t, evaluate=False) ##"
"ydot1 = y1.diff(t) ##:
ydot2 = y2.diff(t) ##:
ydot1_obj = y1.diff(t, evaluate=False) ##:"
"ydot1 = y1.diff(t) ##:
ydot2 = y2.diff(t) ##:
ydot1_obj = y1.diff(t, evaluate=False) ##:"
"y1.diff(t,t) ##
y2.diff(t,t) ##"
"xx = sp.Matrix(sp.symbols('x1:11')) ##
yy = sp.Matrix(sp.symbols('y1:11')) ##:T

xx.shape, yy.shape ##"
"# combination with other comments
a = 3 # comment ##:"
"# Multiline statements and indended lines are not yet supported:
a = [1, 
     2] ##:

if 1:
    b = [10, 20] ##:


c = [100, 200] ##:"
"fig, ax = plt.subplots()
draw_stick_figure(ax, quote='Hello World!')
draw_stick_figure(ax, x=.8, y=.8, radius=.01, quote=""I'm small!"", lw=1)
draw_stick_figure(ax, x=.2, y=.7, radius=.05, quote=""I'm big!"", lw=2)"
"fig, ax = plt.subplots(figsize=(10, 6))
plot_optimism(ax, upto=2014+6.5/12)
ax.annotate('Phase 1 raw SNP data', xy=(2014+2/12, .5), xytext=(10, 40), 
            xycoords='data', textcoords='offset points', ha='right', va='bottom', 
            arrowprops=dict(arrowstyle='->', lw=2))
ax.set_yticks([0, 1])
ax.set_yticklabels(['0', 'hope'])
fig.tight_layout();"
"fig, ax = plt.subplots(figsize=(10, 6))
plot_optimism(ax, upto=2014+10/12)
ax.annotate('Phase 1 raw SNP data', xy=(2014+2/12, .5), xytext=(20, 40), 
            xycoords='data', textcoords='offset points', ha='right', va='bottom', 
            arrowprops=dict(arrowstyle='->', lw=2))
draw_stick_figure(ax, x=.4, y=.77, quote='Too ... much ... data ... ', radius=.02, xytext=(-20, 30))
fig.tight_layout();"
"fig, ax = plt.subplots(figsize=(10, 6))
plot_optimism(ax, upto=2015+5.8/12)
ax.annotate('Phase 1 raw SNP data', xy=(2014+2/12, .5), xytext=(20, 40), 
            xycoords='data', textcoords='offset points', ha='right', va='bottom', 
            arrowprops=dict(arrowstyle='->', lw=2))
ax.annotate('genome accessibility', xy=(2014+11/12, -1), xytext=(10, -40), 
            xycoords='data', textcoords='offset points', ha='left', va='top', 
            arrowprops=dict(arrowstyle='->', lw=2))
ax.annotate('SNP filtering', xy=(2014+12/12, -.8), xytext=(30, -30), 
            xycoords='data', textcoords='offset points', ha='left', va='top', 
            arrowprops=dict(arrowstyle='->', lw=2))
ax.annotate('haplotype phasing', xy=(2015+1/12, -.6), xytext=(30, -30), 
            xycoords='data', textcoords='offset points', ha='left', va='top', 
            arrowprops=dict(arrowstyle='->', lw=2))
ax.annotate('SNP validation', xy=(2015+2/12, -.4), xytext=(30, -30), 
            xycoords='data', textcoords='offset points', ha='left', va='top', 
            arrowprops=dict(arrowstyle='->', lw=2))
ax.annotate('haplotype validation', xy=(2015+4/12, -.2), xytext=(30, -30), 
            xycoords='data', textcoords='offset points', ha='left', va='top', 
            arrowprops=dict(arrowstyle='->', lw=2))
ax.annotate('Phase 1 AR3 public data release', xy=(2015+6/12, .2), xytext=(-20, 50), 
            xycoords='data', textcoords='offset points', ha='center', va='bottom', 
            arrowprops=dict(arrowstyle='->', lw=2))
fig.tight_layout();"
"fig, ax = plt.subplots(figsize=(10, 6))
plot_optimism(ax, upto=2015+9.5/12)
ax.annotate('Phase 1 raw SNP data', xy=(2014+2/12, .5), xytext=(20, 40), 
            xycoords='data', textcoords='offset points', ha='right', va='bottom', 
            arrowprops=dict(arrowstyle='->', lw=2))
ax.annotate('Phase 1 AR3 public data release', xy=(2015+6/12, .2), xytext=(-20, 50), 
            xycoords='data', textcoords='offset points', ha='center', va='bottom', 
            arrowprops=dict(arrowstyle='->', lw=2))
draw_stick_figure(ax, x=.6, y=.35, quote='Population genetics???', radius=.02, xytext=(25, -30))

fig.tight_layout();"
"fig, ax = plt.subplots(figsize=(10, 6))
plot_optimism(ax)
ax.annotate('Phase 1 raw SNP data', xy=(2014+2/12, .5), xytext=(10, 40), 
            xycoords='data', textcoords='offset points', ha='right', va='bottom', 
            arrowprops=dict(arrowstyle='->', lw=2))
ax.annotate('Phase 1 AR3 public data release', xy=(2015+6/12, .2), xytext=(-20, 50), 
            xycoords='data', textcoords='offset points', ha='center', va='bottom', 
            arrowprops=dict(arrowstyle='->', lw=2))
fig.tight_layout();"
"import warnings
warnings.filterwarnings('ignore')
%matplotlib inline
from scipy import special
from matplotlib import pyplot as plt


nums = []
n = 1000
for i in range(n):
    nums.append(special.binom(i, 2))

fig = plt.figure()
ax = fig.add_subplot(111)

x = range(n)
y = nums

ax.plot(x, y, 'k-')
plt.show()
print(x)
"
"plt.plot(k_list,Ein_record,""o"")
plt.ylabel(""Ein"")
plt.xlabel(""k"")"
"plt.plot(k_list,Eout_record,""o"")
plt.ylabel(""Eout"")
plt.xlabel(""k"")"
"plt.plot(gamma_list, Ein_record,""o"")
plt.ylabel(""Ein"")
plt.xlabel(""gamma"")
plt.xscale(""log"")"
"plt.plot(gamma_list, Eout_record,""o"")
plt.ylabel(""Eout"")
plt.xlabel(""gamma"")
plt.xscale(""log"")"
"plt.plot(k_list,Ein_record_avg,""o"")
plt.ylabel(""Ein"")
plt.xlabel(""k"")"
print(Ein_record_avg)
"plt.plot(range(1,iterations+1),Ein_g_record)
plt.ylabel('Ein(g_t)')
plt.xlabel('iterations')
print('Ein(g1):',Ein_g_record[0],'alpha 1:',alpha[0])"
"plt.plot(range(1,iterations+1),Ein_G_record)
plt.ylabel('Ein(G(t))')
plt.xlabel('iterations')"
"plt.plot(range(1,iterations+1),sum_u)
plt.ylabel('sum(u_t)')
plt.xlabel('iterations')
print('u(2):',sum_u[1],'u(T):',sum_u[iterations-1])"
"plt.plot(range(1,iterations+1),epsilon_t_record)
plt.ylabel('epsilon_t')
plt.xlabel('iterations')
print('minimum epsilon_t:',min(epsilon_t_record),'index:',epsilon_t_record.index(min(epsilon_t_record))+1)"
"plt.plot(range(1,iterations+1),Eout_g_record)
plt.ylabel('Eout(g_t)')
plt.xlabel('iterations')
print('Eout(g1):',Eout_g_record[0])"
"plt.plot(range(1,iterations+1),Eout_G_record)
plt.ylabel('Eout(G(t))')
plt.xlabel('iterations')"
"x = np.linspace(0, 4*np.pi)
plt.plot(x, np.sin(x))
plt.show()"
"def fib(n):
    if n == 0: return 1
    if n == 1: return 1
    return fib(n-1)+fib(n-2)
def lucas(n):
    if n == 0: return 2
    if n == 1: return 1
    return lucas(n-1)+lucas(n-2)
xvals = range(0,20)
data0 = [fib(n+1) for n in xvals]
data1 = [lucas(n) for n in xvals]
data2 = [fib(n) for n in xvals]
plt.figure()
plt.semilogy(data0, label='fib(n+1)')
plt.semilogy(data1, label='lucas(n)')
plt.semilogy(data2, label='fib(n))')
plt.legend()
plt.show()"
"df = pd.DataFrame({""f1"":data0,""l0"":data1,""f0"":data2})
df"
"%matplotlib inline

plt.figure(num=None, figsize=(8,6))

for t, marker, c in zip(range(3), "">ox"", ""rgb""):
    plt.scatter(features[target == t, 0], features[target == t, 1], marker=marker, c=c)"
"%matplotlib inline

fig,axes = plt.subplots(2, 3, figsize=(10,8))
pairs = [(0, 1), (0, 2), (0, 3), (1, 2), (1, 3), (2, 3)]

# Set up 3 different pairs of (color, marker)
color_markers = [
        ('r', '>'),
        ('g', 'o'),
        ('b', 'x'),
        ]
for i, (p0, p1) in enumerate(pairs):
    ax = axes.flat[i]

    for t in range(3):
        # Use a different color/marker for each class `t`
        c,marker = color_markers[t]
        ax.scatter(features[target == t, p0], features[
                    target == t, p1], marker=marker, c=c)
    ax.set_xlabel(feature_names[p0])
    ax.set_ylabel(feature_names[p1])
    ax.set_xticks([])
    ax.set_yticks([])

# fig.tight_layout()"
"%matplotlib inline

COLOUR_FIGURE = False

is_virginica = (labels == 'virginica')

# Hand fixed thresholds:
t = 1.65
t2 = 1.75

# Features to use: 3 & 2
f0, f1 = 3, 2

if COLOUR_FIGURE:
    area1c = (1., .8, .8)
    area2c = (.8, .8, 1.)
else:
    area1c = (1., 1, 1)
    area2c = (.7, .7, .7)

# Plot from 90% of smallest value to 110% of largest value
# (all feature values are positive, otherwise this would not work very well)

x0 = features[:, f0].min() * .9
x1 = features[:, f0].max() * 1.1

y0 = features[:, f1].min() * .9
y1 = features[:, f1].max() * 1.1

fig, ax = plt.subplots(figsize=(8,6))
ax.fill_between([t, x1], [y0, y0], [y1, y1], color=area2c)
ax.fill_between([x0, t], [y0, y0], [y1, y1], color=area1c)
ax.plot([t, t], [y0, y1], 'k--', lw=2)
ax.plot([t2, t2], [y0, y1], 'k:', lw=2)
ax.scatter(features[is_virginica, f0],
           features[is_virginica, f1], c='b', marker='o', s=40)
ax.scatter(features[~is_virginica, f0],
           features[~is_virginica, f1], c='r', marker='x', s=40)
ax.set_ylim(y0, y1)
ax.set_xlim(x0, x1)
ax.set_xlabel(feature_names[f0])
ax.set_ylabel(feature_names[f1])
fig.tight_layout()"
"fig,ax = plot_decision(features, labels)
# fig.legend([""%s"" % label for label in labels], loc=""upper left"")
# ax.legend(loc=""upper left"")
# fig.tight_layout()"
alpha_cm
"kd = KernelDensity(kernel='gaussian',bandwidth=0.02)
kd.fit(X)"
"plt.figure(figsize=(10,14))
plt.imshow(mapdata, 
           cmap=plt.get_cmap('gray'), 
           extent=lon_lat_box, 
           aspect=aspect)
plt.imshow(zv, 
           origin='lower', 
           cmap=alpha_cm, 
           extent=lon_lat_box, 
           aspect=aspect)

locations = traps[['Longitude', 'Latitude']].drop_duplicates().values
plt.scatter(locations[:,0], locations[:,1], marker='x')"
"# Code is borrowed from: https://www.kaggle.com/users/213536/vasco/predict-west-nile-virus/west-nile-heatmap

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.neighbors import KernelDensity
%matplotlib inline

mapdata = np.loadtxt(""../input/mapdata_copyright_openstreetmap_contributors.txt"")
traps = pd.read_csv('../input/train.csv')[['Date', 'Trap','Longitude', 'Latitude', 'WnvPresent']]

alpha_cm = plt.cm.Reds
alpha_cm._init()
alpha_cm._lut[:-3,-1] = abs(np.logspace(0, 1, alpha_cm.N) / 10 - 1)[::-1]"
alpha_cm
"kd = KernelDensity(kernel='gaussian',bandwidth=0.02)
kd.fit(X)"
"plt.figure(figsize=(10,14))
plt.imshow(mapdata, 
           cmap=plt.get_cmap('gray'), 
           extent=lon_lat_box, 
           aspect=aspect)
plt.imshow(zv, 
           origin='lower', 
           cmap=alpha_cm, 
           extent=lon_lat_box, 
           aspect=aspect)

locations = traps[['Longitude', 'Latitude']].drop_duplicates().values
plt.scatter(locations[:,0], locations[:,1], marker='x')"
"var('a b M')

A = a * b * M
A"
"dA_da2 = diff(A, a)**2
dA_db2 = diff(A, b)**2
dA_dM2 = diff(A, M)**2

sum_squares = dA_da2 + dA_db2 + dA_dM2
sum_squares

sqrt(sum_squares)"
"sqrt(sum(diff(A,x)**2 for x in ['a', 'b', 'M']))"
"var('R T M')
v_rms = sqrt(3*R*T/M)

delta_v2 = sum(diff(v_rms, x)**2 for x in ['T', 'M'])
sqrt(delta_v2)"
" # underscore is an alias for ""output from last line""; use with caution
_.simplify() "
"var('T V b a R')

P = R*T/(V-b) - (a/V**2)
P"
"state_vars = [T, V, b, a, R]

partials = sum(diff(P,v) for v in state_vars)
partials"
"deltaP = sqrt(partials**2)
deltaP"
"var(""P V T R"")
Z = P*V/(R*T)

deltaZ2 = sum(diff(Z,x)**2 for x in [P,V,T])
sqrt(deltaZ2)"
"var('I k T sigma h')

q_r = 2*I*k*T/(sigma*h**2)
deltaq_r2 = sum(diff(q_r, x)**2 for x in [I,T])
sqrt(deltaq_r2), sqrt(deltaq_r2).simplify()"
"# exponent is a rational number
var('m k T h')

q_tr = ( 2*pi*m*k*T/h**2 )**(S(3)/2)
q_tr"
"deltaq_tr = sqrt(sum(diff(q_tr,x)**2 for x in [m,T]))
deltaq_tr"
"var('p,v,r,t,a,b')"
"# compressibility of ideal gas
z = p*v/(r*t)
z, diff(z, p)"
"sqrt(sum([diff(z,x) for x in [p,v,t]]))"
"P = r*t/(v-b) - (a/v**2)
P"
"w = [diff(P, x)**2 for x in (a,b,v,t)]
sqrt(sum(w))"
"m=var('m')
vrms = sqrt(3*r*t/m)

w = sum([diff(vrms, x)**2 for x in (t,m)])
w**(S(1)/2)"
"# logistic regression
logreg = LogisticRegression()
logreg.fit(X,y)"
"# plot points first
scatter(X,y)"
"scatter(X,y)
plot_separator(logreg.predict)"
"# Load Iris
X,y = load_iris_onevsall()

s = find_best_stump(X,y)
print('meilleur stump : ',s)

print('prediction de ce stump en (6,3):',stumplist_predict( [[6,3]],[s]) )
print('prediction de ce stump en (3,2):',stumplist_predict( [[3,2]],[s]) )

plot_stumplist(X,y,[s])"
"stump_list = ['x0<5.5','x1>3.3','x0<4.9']
plot_stumplist(X,y,stump_list)"
"fileUrl = 'http://earthquake.usgs.gov/earthquakes/feed/v1.0/summary/2.5_day.csv'
eData = pd.read_csv(fileUrl)
dateDownloaded = !date
dateDownloaded"
eData
eData.head()
np.shape(eData)
eData.columns
eData.dtypes
eData.isnull().any()
"eData = eData.dropna()
eData.head()"
eData.isnull().any()
"url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/housing/housing.data'
cols =[""CRIM"",""ZN"",""INDUS"",""CHAS"",""NOX"",""RM"",""AGE"",""DIS"",""RAD"",""TAX"",""PTRATIO"",""B"",""LSTAT"",""TGT""]
boston = pd.read_csv(url, sep="" "", skipinitialspace=True, header=None, names=cols, index_col=False) # Dataframe
dateDownloaded = !date
dateDownloaded"
"Var_dict = {""Varianz"" :  Var, ""erklärte Varianz"" : Var_prop, ""kumulierte erklärte Varianz"" : Var_cum, ""Fehler"" : Var_error}
pd.DataFrame(data=Var_dict, index=[""a{}"".format(s) for s in range(0,14)])"
"# ploten zweier datensätze. 
#siehe: http://stackoverflow.com/questions/13872533/plot-different-dataframes-in-the-same-figure
ax = plot_smaller.plot(x=0, y=1, kind='scatter', color='blue', label='TGT < median')
plot_greater.plot(x=0, y=1, kind='scatter', color='yellow', label='TGT > median', ax=ax)"
"import warnings
warnings.filterwarnings('ignore')
"
"#####*******### NEMAZAT ######********######

from collections import defaultdict
from random import random, sample
from math import floor
print(""Node count: "")
nodeCount = 1000 # input()
positions = 4
candidates = [x for x in range(nodeCount)]
edges = defaultdict(list) # node: edges to other nodes
backtracks = defaultdict(list) # node: paths back
def process():    
    insurance = 0
    for n in range(nodeCount):        
        while len(edges[n]) < positions:
            insurance += 1
            if insurance > 10000:
                print(""Přiliš dlouho!"")
                return False
            if len(candidates) == 0:                
                return True
            if len(candidates) == 1 and n in candidates:
                print(""Asi se nepovedlo udělat bludiště pefektně."")
                return False
            
            edgeTo = sample(candidates, 1)[0]     
            #print(""random"", edgeTo)
            if edgeTo == n:
                continue            
            if edgeTo not in edges[n]:
                edges[n].append(edgeTo)                
                edges[edgeTo].append(n)                                
                
                if not len(edges[edgeTo]) < positions: 
                    candidates.remove(edgeTo)                                 
        if n in candidates:
            candidates.remove(n)
    
process()

#print(edges)
ciphers = len(str(nodeCount))
def intToCiph(i):
    return "" "" * (ciphers - len(str(i))) + str(i)
        
for key,val in edges.items():
    print(intToCiph(key),[intToCiph(x) for x in val])
"
"import warnings
warnings.filterwarnings('ignore')
from heapq import heapify, heappush, heappop
from collections import defaultdict

global codes
codes = dict()

class HuffNode(object):
    def __lt__(self, other):
        return self.weight < other.weight
    
    def __init__(self, weight, ltr=None):
        self.weight = weight
        self.ltr = ltr
        self.left = None
        self.right = None

def calculate_frequencies(string):
    freqs = defaultdict(int)
    for c in string:
        freqs[c] += 1
    return freqs

def build_tree(frequencies):
    
    heap = []
    for ltr, weight in frequencies.items():
        node = HuffNode(weight, ltr)
        heappush(heap, node)
    
    while len(heap) > 1:
        n1 = heappop(heap)
        n2 = heappop(heap)
        weight = n1.weight + n2.weight
        root = HuffNode(weight)
        root.left = n1
        root.right = n2
        heappush(heap, root)
    tree = heappop(heap)
    return tree

def generate_codes(node, code=''):
    if node.ltr: #leaf
        codes[node.ltr] = (node.weight, code)
    else:
        code += '0'
        generate_codes(node.left, code)
        code = code[:-1]
        
        code += '1'
        generate_codes(node.right, code)
        code = code[:-1]
        
def print_coding_table(code_dict):
    print(""Symbol\tWeight\tCode"")
    for symbol, code in sorted(code_dict.items(), key=lambda item: item[1][0], reverse=True):
        print(symbol, *code, sep='\t')
        
def encode(code_dict, msg):
    return ''.join([code_dict[ltr][1] for ltr in msg])

def decode(tree, compressed):
    current_node = tree
    uncompressed = ''
    for bit in compressed:
        if bit == '0':
            current_node = current_node.left
        elif bit == '1':
            current_node = current_node.right
        
        if current_node.ltr:
            uncompressed += current_node.ltr
            current_node = tree 
    return uncompressed

test_string = 'this is an example for huffman encoding'
freqs = calculate_frequencies(test_string)
tree = build_tree(freqs)
generate_codes(tree)
print_coding_table(codes)
compressed = encode(codes, test_string)

print(""\nCompressed:\n"", compressed, sep='')
decompressed = decode(tree, compressed)
print(""\nDecompressed:\n"", decompressed, sep='')"
"import warnings
warnings.filterwarnings('ignore')
import numpy as np
from numpy import pi,sqrt,sin,cos,tan,arcsin,arccos,arctan
from numpy import real as re
from numpy import imag as im
from numpy import conj as conj
from numpy import exp  as exp

import sympy as sp
from sympy import limit, Symbol, oo

import matplotlib.pyplot as plt

from IPython.display import display,Math,HTML,Latex # Used to display widgets in the notebook
sp.init_printing(use_unicode=True,use_latex=True)

import ipywidgets as widgets #If not install run <pip install ipywidgets> in DOS command prompt (Anaconda distribution)

import time

print('Init done:', time.ctime() )

%matplotlib inline 
#%connect_info"
"import warnings
warnings.filterwarnings('ignore')
%matplotlib inline
import numpy as np
import matplotlib.pyplot as plt

L = 64  # number of random samples
N = 1000  # number of sample functions

# generate input signal (white Gaussian noise)
np.random.seed(1)
x = np.random.normal(size=(N, L))
# generate output signal
h = 2*np.fft.irfft([1,1,1,0,0,0])
y = np.asarray([np.convolve(x[n,:], h, mode='same') for n in range(N)])

# compute and plot results
def compute_plot_results(x):

    # estimate linear mean by ensemble average
    mu = 1/N * np.sum(x, 0)
    # estimate the auto-correlation function
    acf = np.zeros((L, L))
    for n in range(L):
        for m in range(L):
            acf[n, m] = 1/N * np.sum(x[:, n]*x[:, m], 0)
    
    plt.subplot(121)
    plt.stem(mu)
    plt.title(r'Estimate of linear mean $\hat{\mu}[k]$')
    plt.xlabel(r'$k$')
    plt.ylabel(r'$\hat{\mu}[k]$')
    plt.axis([0, L, -1.5, 1.5])

    plt.subplot(122)
    plt.pcolor(np.arange(L), np.arange(L), acf, vmin=-2, vmax=2)
    plt.title(r'Estimate of ACF $\hat{\varphi}[k_1, k_2]$')
    plt.xlabel(r'$k_1$')
    plt.ylabel(r'$k_2$')
    plt.colorbar()
    plt.autoscale(tight=True)

    
plt.figure(figsize = (10, 5))
plt.gcf().suptitle(r'Input signal $x[k]$', fontsize=12)
compute_plot_results(x)

plt.figure(figsize = (10, 5))
plt.gcf().suptitle(r'Output signal $y[k]$', fontsize=12)
compute_plot_results(y)"
"import warnings
warnings.filterwarnings('ignore')
%matplotlib inline
import numpy as np
import matplotlib.pyplot as plt

L = 10000  # number of samples
K = 30  # limit for lags in ACF

# generate input signal (white Gaussian noise)
np.random.seed(2)
x = np.random.normal(size=L)
# compute system response
y = np.convolve(x, [1, 1, 1, 1, 1], mode='full')

# compute and truncate ACF
acf = 1/len(y) * np.correlate(y, y, mode='full')
acf = acf[len(y)-K-1:len(y)+K-1]
kappa = np.arange(-K, K)

# plot ACF
plt.figure(figsize = (10, 6))
plt.stem(kappa, acf)
plt.title('Estimated ACF of output signal $y[k]$')
plt.ylabel(r'$\hat{\varphi}_{yy}[\kappa]$')
plt.xlabel(r'$\kappa$')
plt.axis([-K, K, 1.2*min(acf), 1.1*max(acf)]);
plt.grid()"
"%matplotlib inline
import numpy as np
import matplotlib.pyplot as plt
import scipy.signal as sig


N = 10000  # number of samples for input signal
K = 50  # limit for lags in ACF

# generate input signal
np.random.seed(5)
x = np.random.normal(size=N)  # normally distributed (zero-mean, unit-variance) white noise
# impulse response of the system
h = np.concatenate((np.zeros(10), sig.triang(10), np.zeros(10)))
# output signal by convolution
y = np.convolve(h, x, mode='full')

# compute correlation functions
acfx = 1/len(x) * np.correlate(x, x, mode='full')
acfy = 1/len(y) * np.correlate(y, y, mode='full')
ccfyx = 1/len(y) * np.correlate(y, x, mode='full')

def plot_correlation_function(cf):
    cf = cf[N-K-1:N+K-1]
    kappa = np.arange(-len(cf)//2,len(cf)//2)
    plt.stem(kappa, cf)
    plt.xlabel(r'$\kappa$')
    plt.axis([-K, K, -0.2, 1.1*max(cf)])

# plot ACFs and CCF
plt.rc('figure', figsize=(10, 3))
plt.figure()
plot_correlation_function(acfx)
plt.title('Estimated ACF of input signal')
plt.ylabel(r'$\hat{\varphi}_{xx}[\kappa]$')

plt.figure()
plot_correlation_function(acfy)
plt.title('Estimated ACF of output signal')
plt.ylabel(r'$\hat{\varphi}_{yy}[\kappa]$')

plt.figure()
plot_correlation_function(ccfyx)
plt.plot(np.arange(len(h)), h, 'g-')
plt.title('Estimated and true impulse response')
plt.ylabel(r'$\hat{h}[k]$, $h[k]$');"
"import warnings
warnings.filterwarnings('ignore')
%matplotlib inline
import numpy as np
import matplotlib.pyplot as plt
import scipy.signal as sig

N = 8129  # number of samples
M = 256  # length of Wiener filter
Om0 = 0.1*np.pi  # frequency of original signal
N0 = 0.1  # PSD of additive white noise

# generate original signal
s = np.cos(Om0 * np.arange(N)) 
# generate observed signal
g = 1/20*np.asarray([1, 2, 3, 4, 5, 4, 3, 2, 1])
np.random.seed(1)
n = np.random.normal(size=N, scale=np.sqrt(N0))
x = np.convolve(s, g, mode='same') + n
# estimate (cross) PSDs using Welch technique
f, Pxx = sig.csd(x, x, nperseg=M)
f, Psx = sig.csd(s, x, nperseg=M)
# compute Wiener filter
H = Psx/Pxx
H = H * np.exp(-1j*2*np.pi/len(H)*np.arange(len(H))*(len(H)//2))  # shift for causal filter
h = np.fft.irfft(H)
# apply Wiener filter to observation
y = np.convolve(x, h, mode='same')

# plot (cross) PSDs
Om = np.linspace(0, np.pi, num=len(H))

plt.figure(figsize=(10, 4))
plt.subplot(121)
plt.plot(Om, 20*np.log10(np.abs(.5*Pxx)), label=r'$| \Phi_{xx}(e^{j \Omega}) |$ in dB')
plt.plot(Om, 20*np.log10(np.abs(.5*Psx)), label=r'$| \Phi_{sx}(e^{j \Omega}) |$ in dB')
plt.title('(Cross) PSDs')
plt.xlabel(r'$\Omega$')
plt.legend()
plt.axis([0, np.pi, -60, 40])
plt.grid()

# plot transfer function of Wiener filter
plt.subplot(122)
plt.plot(Om, 20*np.log10(np.abs(H)))
plt.title('Transfer function of Wiener filter')
plt.xlabel(r'$\Omega$')
plt.ylabel(r'$| H(e^{j \Omega}) |$ in dB')
plt.axis([0, np.pi, -150, 3])
plt.grid()
plt.tight_layout()

# plot signals
idx = np.arange(500, 600)
plt.figure(figsize=(10, 4))
plt.plot(idx, x[idx], label=r'observed signal $x[k]$')
plt.plot(idx, s[idx], label=r'original signal $s[k]$')
plt.plot(idx, y[idx], label=r'estimated signal $y[k]$')
plt.title('Signals')
plt.xlabel(r'$k$')
plt.axis([idx[0], idx[-1], -1.5, 1.5])
plt.legend()
plt.grid()"
"N = 8129  # number of samples
M = 256  # length of Wiener filter
Om0 = 0.1*np.pi  # frequency of original signal
N0 = .1  # PSD of additive white noise

# generate original signal
s = np.cos(Om0 * np.arange(N)) 
# generate observed signal
g = 1/20*np.asarray([1, 2, 3, 4, 5, 4, 3, 2, 1])
np.random.seed(1)
n = np.random.normal(size=N, scale=np.sqrt(N0))
x = np.convolve(s, g, mode='same') + n
# estimate PSD
f, Pss = sig.csd(s, s, nperseg=M)
f, Pnn = sig.csd(n, n, nperseg=M)
# compute Wiener filter
G = np.fft.rfft(g, M)
H = 1/G * (np.abs(G)**2 / (np.abs(G)**2 + N0/Pss))
H = H * np.exp(-1j*2*np.pi/len(H)*np.arange(len(H))*(len(H)//2-8))  # shift for causal filter
h = np.fft.irfft(H)
# apply Wiener filter to observation
y = np.convolve(x, h, mode='same')

# plot (cross) PSDs
Om = np.linspace(0, np.pi, num=len(H))

plt.figure(figsize=(10, 4))
plt.subplot(121)
plt.plot(Om, 20*np.log10(np.abs(.5*Pss)), label=r'$| \Phi_{ss}(e^{j \Omega}) |$ in dB')
plt.plot(Om, 20*np.log10(np.abs(.5*Pnn)), label=r'$| \Phi_{nn}(e^{j \Omega}) |$ in dB')
plt.title('PSDs')
plt.xlabel(r'$\Omega$')
plt.legend()
plt.axis([0, np.pi, -60, 40])
plt.grid()

# plot transfer function of Wiener filter
plt.subplot(122)
plt.plot(Om, 20*np.log10(np.abs(H)))
plt.title('Transfer function of Wiener filter')
plt.xlabel(r'$\Omega$')
plt.ylabel(r'$| H(e^{j \Omega}) |$ in dB')
plt.axis([0, np.pi, -150, 10])
plt.grid()
plt.tight_layout()

# plot signals
idx = np.arange(500, 600)
plt.figure(figsize=(10, 4))
plt.plot(idx, x[idx], label=r'observed signal $x[k]$')
plt.plot(idx, s[idx], label=r'original signal $s[k]$')
plt.plot(idx, y[idx], label=r'estimated signal $y[k]$')
plt.title('Signals')
plt.xlabel(r'$k$')
plt.axis([idx[0], idx[-1], -1.5, 1.5])
plt.legend()
plt.grid()"
"import warnings
warnings.filterwarnings('ignore')
%matplotlib inline
import numpy as np
import matplotlib.pyplot as plt

L = 32  # number of random samples
N = 10000  # number of sample functions

# generate input signal (white Gaussian noise)
np.random.seed(2)
x = np.random.normal(size=(N, L))
x[:, L//2] += 1 
# generate output signal
h = 2*np.fft.irfft([1,1,1,0,0,0])
y = np.asarray([np.convolve(x[n,:], h, mode='full') for n in range(N)])

# estimate and plot linear mean
def estimate_plot_linear_mean(x):
    # estimate linear mean by ensemble average
    mu = 1/N * np.sum(x, 0)
    # plot linear mean
    plt.stem(mu)
    plt.xlabel(r'$k$')
    plt.ylabel(r'$\hat{\mu}[k]$')
    plt.axis([0, x.shape[1], -1.2, 1.2])

plt.figure(figsize = (10, 3))
plt.title(r'Estimated linear mean $\hat{\mu}_x[k]$ of input signal')
estimate_plot_linear_mean(x)

plt.figure(figsize = (10, 3))
plt.title(r'Estimated linear mean $\hat{\mu}_y[k]$ of output signal')
estimate_plot_linear_mean(y)"
"import warnings
warnings.filterwarnings('ignore')
%matplotlib inline
import numpy as np
import matplotlib.pyplot as plt
import scipy.signal as sig

w = 8  # wordlength of quantized coefficients
A = 1  # attenuation of filter coefficients
N = 256  # number of coefficients for filter
Q = 1/(2**(w-1))  # quantization stepsize

def uniform_midtread_quantizer(x, Q):
    # limiter
    x = np.copy(x)
    idx = np.where(x <= -1)
    x[idx] = -1
    idx = np.where(x > 1 - Q)
    x[idx] = 1 - Q
    # linear uniform quantization
    xQ = Q * np.floor(x/Q + 1/2)
    
    return xQ

# design lowpass
h = A * sig.firwin(N, .5)
# quantize coefficients
hQ = uniform_midtread_quantizer(h, Q)

# plot frequency response
Om, H = sig.freqz(h)
Om, HQ = sig.freqz(hQ)
Om, E = sig.freqz(hQ-h)

plt.figure(figsize=(10, 4))
plt.plot(Om, 20*np.log10(np.abs(H)), label=r'$| H(e^{j \Omega}) |$ in dB (Designed)')
plt.plot(Om, 20*np.log10(np.abs(HQ)), label=r'$| H_Q(e^{j \Omega}) |$ in dB (Quantized coefficients)')
plt.title('Magnitude of the transfer function w and w/o quantization of coefficients')
plt.xlabel(r'$\Omega$')
plt.axis([0, np.pi, -130, 10])
plt.legend(loc=3)
plt.grid()"
"w = 16  # wordlength of quantized coefficients/operations
N = 32  # number of coefficients for filter
L = 8192  # length of input signal
Q = 1/(2**(w-1))  # quantization stepsize


def uniform_midtread_quantizer(x, Q):
    xQ = Q * np.floor(x/Q + 1/2)
    
    return xQ


# random impulse response
h = np.random.uniform(size=N, low=-1, high=1)
hQ = uniform_midtread_quantizer(h, Q)
# input signal
x = np.random.uniform(size=L, low=-1, high=1-Q)
xQ = uniform_midtread_quantizer(x, Q)
# output signal by convolution
y = np.zeros(L+N-1)
yQ = np.zeros(L+N-1)
for k in np.arange(L):
    for kappa in np.arange(N):
        if (k-kappa) >= 0:
            y[k] += hQ[kappa] * xQ[k-kappa]
            yQ[k] += uniform_midtread_quantizer(hQ[kappa] * xQ[k-kappa], Q)

# overall round-off error
e = yQ - y

# estimate power of round-off error
sx = 10*np.log10(np.var(e))
print('Power of overall round-off noise is %f dB' %sx)
# estimated PDF of round-off error
pe, bins = np.histogram(e, bins=50, density=True, range=(-10*Q, 10*Q))
# estimate PSD of round-off error
nf, Pee = sig.welch(e, nperseg=128)


# plot statistical properties of error signal
plt.figure(figsize=(10,6))

plt.subplot(121)
plt.bar(bins[:-1]/Q, pe*Q, width = 20/len(pe))
plt.title('Estimated PDF of the round-off noise')
plt.xlabel(r'$\theta / Q$')
plt.ylabel(r'$\hat{p}_x(\theta) / Q$')
#plt.axis([-1, 1, 0, 1.2])

plt.subplot(122)
plt.plot(nf*2*np.pi, Pee*6/Q**2/N)
plt.title('Estimated PSD of the round-off noise')
plt.xlabel(r'$\Omega$')
plt.ylabel(r'$\hat{\Phi}_{ee}(e^{j \Omega}) / \sigma_e^2$')
plt.axis([0, np.pi, 0, 2])
plt.grid();"
"L = 1024  # length of signals x[k]
A = 1/7  # amplitude of signal
M = 2*L-1


# generate signals
x = A * np.ones(L)
y = A**2 * L * sig.triang(M)

# linear convolution
y1 = np.convolve(x, x, 'full')
e1 = y - y1
# fast convolution
y2 = np.fft.irfft(np.fft.rfft(x, M+1) * np.fft.rfft(x, M+1))[0:M]
e2 = y - y2

plt.figure(figsize=(10, 4))
plt.plot(np.abs(e1), label='conventional convolution')
plt.plot(np.abs(e2), label='fast convolution')
plt.xlabel(r'$k$')
plt.ylabel(r'$|e[k]|$')
plt.legend()
plt.grid()"
"import warnings
warnings.filterwarnings('ignore')
%matplotlib inline
import numpy as np
import matplotlib.pyplot as plt
import scipy.signal as sig


L = 64  # length of input signal
N = 8  # length of impulse response
P = 16  # length of segments


# generate input signal
x = sig.triang(L)
# generate impulse response
h = sig.triang(N)

# overlap-add convolution
xp = np.zeros((L//P, P))
yp = np.zeros((L//P, N+P-1))
y = np.zeros(L+P-1)
for p in range(L//P):
    xp[p, :] = x[p*P:(p+1)*P]
    yp[p, :] = np.convolve(xp[p,:], h, mode='full')
    y[p*P:(p+1)*P+N-1] += yp[p, :]
y = y[0:N+L]


# plot signals
plt.figure(figsize = (10,2))

plt.subplot(121)
plt.stem(x)
for n in np.arange(L//P)[::2]:
    plt.axvspan(n*P, (n+1)*P-1, facecolor='g', alpha=0.5)
plt.title(r'Signal $x[k]$ and segments')
plt.xlabel(r'$k$')
plt.ylabel(r'$x[k]$')
plt.axis([0, L, 0, 1])

plt.subplot(122)
plt.stem(h)
plt.title(r'Impulse response $h[k]$')
plt.xlabel(r'$k$')
plt.ylabel(r'$h[k]$')
plt.axis([0, L, 0, 1])

for p in np.arange(L//P):
    plt.figure(figsize = (10,2))
    
    plt.stem(np.concatenate((np.zeros(p*P), yp[p, :])))
    plt.title(r'Result of segment $p=%d$' %(p))
    plt.xlabel(r'$k$')
    plt.ylabel(r'$y_%d[k - %d P]$' %(p,p))
    plt.axis([0, L+P, 0, 4])
    

plt.figure(figsize = (10,2))
plt.stem(y)
plt.title(r'Result $y[k] = x[k] * h[k]$')
plt.xlabel(r'$k$')
plt.ylabel(r'$y[k]$')
plt.axis([0, L+P, 0, 4]);"
"L = 64  # length of input signal
N = 8  # length of impulse response
P = 24  # length of segments


# generate input signal
x = sig.triang(L)
# generate impulse response
h = sig.triang(N)

# overlap-save convolution
nseg = (L+N-1)//(P-N+1) + 1
x = np.concatenate((np.zeros(N-1), x, np.zeros(P)))
xp = np.zeros((nseg, P))
yp = np.zeros((nseg, P))
y = np.zeros(nseg*(P-N+1))

for p in range(nseg):
    xp[p, :] = x[p*(P-N+1):p*(P-N+1)+P]
    yp[p, :] = np.fft.irfft(np.fft.rfft(xp[p, :]) * np.fft.rfft(h, P))
    y[p*(P-N+1):p*(P-N+1)+P-N+1] = yp[p, N-1:]
y = y[0:N+L]
    
plt.figure(figsize = (10,2))

plt.subplot(121)
plt.stem(x[N-1:])
plt.title(r'Signal $x[k]$')
plt.xlabel(r'$k$')
plt.ylabel(r'$x[k]$')
plt.axis([0, L, 0, 1])

plt.subplot(122)
plt.stem(h)
plt.title(r'Impulse response $h[k]$')
plt.xlabel(r'$k$')
plt.ylabel(r'$h[k]$')
plt.axis([0, L, 0, 1])

for p in np.arange(nseg):
    plt.figure(figsize = (10,2))
    plt.stem(yp[p, :])
    plt.axvspan(0, N-1+.5, facecolor='r', alpha=0.5)
    plt.title(r'Result of periodic convolution of $x_%d[k]$ and $h_N[k]$' %(p))
    plt.xlabel(r'$k$')
    plt.axis([0, L+P, 0, 4])
    

plt.figure(figsize = (10,2))
plt.stem(y)
plt.title(r'Result $y[k] = x[k] * h[k]$')
plt.xlabel(r'$k$')
plt.ylabel(r'$y[k]$')
plt.axis([0, L+P, 0, 4]);"
"import warnings
warnings.filterwarnings('ignore')
%matplotlib inline
import numpy as np
import matplotlib.pyplot as plt
import scipy.signal as sig

N = 32  # length of filter
Omc = np.pi/3  # corner frequency of low-pass

# specify desired frequency response
Ommu = 2*np.pi/N*np.arange(N)
Hd = np.zeros(N)
Hd[Ommu <= Omc] = 1
Hd[Ommu >= (2*np.pi-Omc)] = 1

# compute impulse response of filter
h = np.fft.ifft(Hd)
h = np.real(h)  # due to round-off errors
# compute frequency response of filter
Om, H = sig.freqz(h, worN=8192)

# plot impulse response
plt.figure(figsize = (10,3))
plt.stem(h)
plt.title('Impulse response')
plt.xlabel(r'$k$')
plt.ylabel(r'$h[k]$')
# plot transfer functions
plt.figure(figsize = (10,3))
plt.plot(Om, np.abs(H), 'b-', label=r'designed $|H(e^{j \Omega})|$')
plt.stem(Ommu, np.abs(Hd), 'g', label=r'desired $|H_d[\mu]|$')
plt.plot([0, Omc, Omc], [1, 1, 0], 'r--')
plt.title('Magnitude response of desired/designed filter')
plt.xlabel(r'$\Omega$')
plt.legend()
plt.axis([0, np.pi, -0.05, 1.5])
# plot phase
plt.figure(figsize = (10,3))
plt.plot(Om, np.unwrap(np.angle(H)))
plt.title('Phase of designed filter')
plt.xlabel(r'$\Omega$')
plt.ylabel(r'$\varphi(\Omega)$')
plt.grid()"
"N = 33  # length of filter
Omc = np.pi/3  # corner frequency of low-pass

# specify desired frequency response
Ommu = 2*np.pi/N*np.arange(N)
Hd = np.zeros(N)
Hd[Ommu <= Omc] = 1
Hd[Ommu >= (2*np.pi-Omc)] = 1
Hd = Hd * np.exp(-1j*Ommu*(N-1)/2)

# compute impulse response of filter
h = np.fft.ifft(Hd)
h = np.real(h)  # due to round-off errors
# compute frequency response of filter
Om, H = sig.freqz(h, worN=8192)

# plot impulse response
plt.figure(figsize = (10,3))
plt.stem(h)
plt.title('Impulse response')
plt.xlabel(r'$k$')
plt.ylabel(r'$h[k]$')
# plot frequency response
plt.figure(figsize = (10,3))
plt.plot(Om, np.abs(H), 'b-', label=r'designed $|H(e^{j \Omega})|$')
plt.stem(Ommu, np.abs(Hd), 'g', label=r'desired $|H_d[\mu]|$')
plt.plot([0, Omc, Omc], [1, 1, 0], 'r--')
plt.title('Magnitude response of desired/designed filter')
plt.xlabel(r'$\Omega$')
plt.legend()
plt.axis([0, np.pi, -0.05, 1.5])
# plot phase
plt.figure(figsize = (10,3))
plt.plot(Om, np.unwrap(np.angle(H)))
plt.title('Phase of designed filter')
plt.xlabel(r'$\Omega$')
plt.ylabel(r'$\varphi(\Omega)$')
plt.grid()"
"N = 33  # length of filter
M = 8192  # number of frequency samples
Omc = np.pi/2  # corner frequency of low-pass

# specify desired frequency response
Ommu = 2*np.pi/M*np.arange(M)
Hd = np.zeros(M)
Hd[Ommu <= Omc] = 1
Hd[Ommu >= (2*np.pi-Omc)] = 1
Hd = Hd * np.exp(-1j*Ommu*(N-1)/2)

# compute impulse response of filter
h = np.fft.ifft(Hd)
h = np.real(h)  # due to round-off errors
h = h[0:N]  # rectangular window
# compute frequency response of filter
Om, H = sig.freqz(h, worN=8192)

# plot impulse response
plt.figure(figsize = (10,3))
plt.stem(h)
plt.title('Impulse response')
plt.xlabel(r'$k$')
plt.ylabel(r'$h[k]$')
# plot frequency response
plt.figure(figsize = (10,3))
plt.plot(Om, 20 * np.log10(abs(H)), label='rectangular window')
plt.plot([0, Omc, Omc], [0, 0, -100], 'r--')
plt.title('Magnitude response of designed filter')
plt.xlabel(r'$\Omega$')
plt.ylabel(r'$|H(e^{j \Omega})|$ in dB')
plt.axis([0, np.pi, -100, 3])
plt.legend()
plt.grid()
# plot phase
plt.figure(figsize = (10,3))
plt.plot(Om, np.unwrap(np.angle(H)))
plt.title('Phase of designed filter')
plt.xlabel(r'$\Omega$')
plt.ylabel(r'$\varphi(\Omega)$')
plt.grid()"
"import warnings
warnings.filterwarnings('ignore')
%matplotlib inline
import numpy as np
import matplotlib.pyplot as plt

om = np.linspace(-10, 10, 200)
Om = 2*np.arctan(om*1/2)

plt.figure(figsize=(10,4))
plt.plot(om, Om, label=r'$2 \cdot \arctan(\frac{\omega T}{2})$')
plt.plot(om, om, 'k--', label=r'$\omega T$')
plt.xlabel(r'$\omega$')
plt.ylabel(r'$\Omega$')
plt.axis([-10, 10, -np.pi, np.pi])
plt.legend(loc=2)
plt.grid()"
"%matplotlib inline
import numpy as np
import matplotlib.pyplot as plt
import scipy.signal as sig


fs = 44100  # sampling frequency
fc = 1000  # corner frequency of the lowpass

# coefficients of analog lowpass filter
Qinf = 0.8
sinf = 2*np.pi*fc
C = 1e-6
L = 1/(sinf**2*C)
R = sinf*L/Qinf

B = [0, 0, 1]
A = [L*C, R*C, 1]

# cofficients of digital filter
T = 1/fs
b = [T**2, 2*T**2, T**2]
a = [(4*L*C+2*T*R*C+T**2), (-8*L*C+2*T**2), (4*L*C-2*T*R*C+T**2)]

# compute frequency responses
Om, Hd = sig.freqz(b, a, worN=1024)
tmp, H = sig.freqs(B, A, worN=fs*Om)

# plot results
f = Om*fs/(2*np.pi)
plt.figure(figsize = (10, 4))
plt.semilogx(f, 20*np.log10(np.abs(H)), label=r'$|H(j \omega)|$ of analog filter')
plt.semilogx(f, 20*np.log10(np.abs(Hd)), label=r'$|H_d(e^{j \Omega})|$ of digital filter')
plt.xlabel(r'$f$ in Hz')
plt.ylabel(r'dB')
plt.axis([100, fs/2, -70, 3])
plt.legend()
plt.grid()"
"omc = 2*np.pi*np.array([5000, 6000])  # corner frequencies of bandpass
N = 2  # order of filter

# pre-warping of corner frequencies
omcp = 2*fs*np.tan(omc/(2*fs))

# design of analog filters with and without pre-warping
B, A = sig.butter(N, omc, btype='bandpass', analog=True)
Bp, Ap = sig.butter(N, omcp, btype='bandpass', analog=True)

# bilinear transform of analog prototypes
b, a = sig.bilinear(B, A, fs)
bp,ap = sig.bilinear(Bp, Ap, fs)

# compute frequency responses
Om, Hdp = sig.freqz(bp, ap, worN=1024)
Om, Hd = sig.freqz(b, a, worN=1024)
tmp, H = sig.freqs(B, A, worN=fs*Om)

# plot results
np.seterr(divide='ignore')
f = Om*fs/(2*np.pi)
plt.figure(figsize = (12, 8))
plt.semilogx(f, 20*np.log10(np.abs(H)), label=r'$|H(j \omega)|$ of analog prototype')
plt.semilogx(f, 20*np.log10(np.abs(Hd)), label=r'$|H_d(e^{j \Omega})|$ of digital filter without pre-warping')
plt.semilogx(f, 20*np.log10(np.abs(Hdp)), label=r'$|H_d(e^{j \Omega})|$ of digital filter with pre-warping')
plt.xlabel(r'$f$ in Hz')
plt.ylabel(r'dB')
plt.axis([100, fs/2, -70, 3])
plt.legend()
plt.grid()"
"import warnings
warnings.filterwarnings('ignore')
%matplotlib inline
import numpy as np
import matplotlib.pyplot as plt
import scipy.signal as sig

N = 32  # length of filter
Omc = np.pi/2

# compute impulse response
k = np.arange(N)
hd = Omc/np.pi * np.sinc(k*Omc/np.pi)
# windowing
w = np.ones(N)
h = hd * w

# frequency response
Om, H = sig.freqz(h)

# plot impulse response
plt.figure(figsize=(10, 3))
plt.stem(h)
plt.title('Impulse response')
plt.xlabel(r'$k$')
plt.ylabel(r'$h[k]$')
# plot magnitude responses
plt.figure(figsize=(10, 3))
plt.plot([0, Omc, Omc], [0, 0, -100], 'r--')
plt.plot(Om, 20 * np.log10(abs(H)))
plt.title('Magnitude response')
plt.xlabel(r'$\Omega$')
plt.ylabel(r'$|H(e^{j \Omega})|$ in dB')
plt.axis([0, np.pi, -20, 3])
plt.grid()
# plot phase responses
plt.figure(figsize=(10, 3))
plt.plot(Om, np.unwrap(np.angle(H)))
plt.title('Phase')
plt.xlabel(r'$\Omega$')
plt.ylabel(r'$\varphi (\Omega)$ in rad')
plt.grid()"
"N = 33  # length of filter
Omc = np.pi/2

# compute impulse response
k = np.arange(N)
hd = Omc/np.pi * np.sinc((k-(N-1)/2)*Omc/np.pi)
# windowing
w1 = np.ones(N)
w2 = np.blackman(N)
h1 = hd * w1
h2 = hd * w2

# frequency responses
Om, H1 = sig.freqz(h1)
Om, H2 = sig.freqz(h2)

# plot impulse response
plt.figure(figsize=(10, 3))
plt.stem(h1)
plt.title('Impulse response (rectangular window)')
plt.xlabel(r'$k$')
plt.ylabel(r'$h[k]$')
# plot magnitude responses
plt.figure(figsize=(10, 3))
plt.plot([0, Omc, Omc], [0, 0, -300], 'r--')
plt.plot(Om, 20 * np.log10(abs(H1)), label='rectangular window')
plt.plot(Om, 20 * np.log10(abs(H2)), label='Blackmann window')
plt.title('Magnitude response')
plt.xlabel(r'$\Omega$')
plt.ylabel(r'$|H(e^{j \Omega})|$ in dB')
plt.axis([0, np.pi, -120, 3])
plt.legend(loc=3)
plt.grid()
# plot phase responses
plt.figure(figsize=(10, 3))
plt.plot(Om, np.unwrap(np.angle(H1)), label='rectangular window')
plt.plot(Om, np.unwrap(np.angle(H2)), label='Blackmann window')
plt.title('Phase')
plt.xlabel(r'$\Omega$')
plt.ylabel(r'$\varphi (\Omega)$ in rad')
plt.legend(loc=3)
plt.grid()"
"import warnings
warnings.filterwarnings('ignore')
%matplotlib inline
import numpy as np
import matplotlib.pyplot as plt
from matplotlib.markers import MarkerStyle
from matplotlib.patches import Circle
import scipy.signal as sig

N = 5  # order of recursive filter
L = 128  # number of computed samples


def zplane(z, p, title='Poles and Zeros'):
    ""Plots zero and pole locations in the complex z-plane""
    ax = plt.gca()
    
    ax.plot(np.real(z), np.imag(z), 'bo', fillstyle='none', ms = 10)
    ax.plot(np.real(p), np.imag(p), 'rx', fillstyle='none', ms = 10)
    unit_circle = Circle((0,0), radius=1, fill=False,
                         color='black', ls='solid', alpha=0.9)
    ax.add_patch(unit_circle)
    ax.axvline(0, color='0.7')
    ax.axhline(0, color='0.7')
    
    plt.title(title)
    plt.xlabel(r'Re{$z$}')
    plt.ylabel(r'Im{$z$}')
    plt.axis('equal')
    plt.xlim((-2, 2))
    plt.ylim((-2, 2))
    plt.grid()


# compute coefficients of recursive filter
b, a = sig.butter(N, 0.2, 'low')
# compute transfer function
Om, H = sig.freqz(b, a)
# compute impulse response
k = np.arange(L)
x = np.where(k==0, 1.0, 0)
h = sig.lfilter(b, a, x)

# plot pole/zero-diagram
plt.figure(figsize=(5, 5))
zplane(np.roots(b), np.roots(a))
# plot magnitude response
plt.figure(figsize=(10, 3))
plt.plot(Om, 20 * np.log10(abs(H)))
plt.xlabel(r'$\Omega$')
plt.ylabel(r'$|H(e^{j \Omega})|$ in dB')
plt.grid()
plt.title('Magnitude response')
# plot phase response
plt.figure(figsize=(10, 3))
plt.plot(Om, np.unwrap(np.angle(H)))
plt.xlabel(r'$\Omega$')
plt.ylabel(r'$\varphi (\Omega)$ in rad')
plt.grid()
plt.title('Phase response')
# plot impulse response (magnitude)
plt.figure(figsize=(10, 3))
plt.stem(20*np.log10(np.abs(np.squeeze(h))))
plt.xlabel(r'$k$')
plt.ylabel(r'$|h[k]|$ in dB')
plt.grid()
plt.title('Impulse response (magnitude)');"
"import warnings
warnings.filterwarnings('ignore')
%matplotlib inline
import numpy as np
import matplotlib.pyplot as plt
import scipy.signal as sig

p = 0.90*np.exp(-1j*np.pi/4)
a = np.poly([p, np.conj(p)])  # denominator coefficients
b = [1, 0, 0]  # numerator coefficients
N = 40  # number of computed samples

# generate input signal (= Dirac impulse)
k = np.arange(N)
x = np.where(k==0, 1.0, 0.0)

# filter signal using transposed direct form II
h = sig.lfilter(b, a, x)

# plot output signal
plt.figure(figsize=(8, 4))
plt.stem(h)
plt.title('Impulse response')
plt.xlabel(r'$k$')
plt.ylabel(r'$h[k]$')
plt.axis([-1, N, -1.5, 1.5])
plt.grid()"
"import warnings
warnings.filterwarnings('ignore')
%matplotlib inline
import numpy as np
import matplotlib.pyplot as plt
from matplotlib.markers import MarkerStyle
from matplotlib.patches import Circle
import scipy.signal as sig

N = 9  # order of recursive filter

def zplane(z, p, title='Poles and Zeros'):
    ""Plots zero and pole locations in the complex z-plane""
    ax = plt.gca()
    
    ax.plot(np.real(z), np.imag(z), 'bo', fillstyle='none', ms = 10)
    ax.plot(np.real(p), np.imag(p), 'rx', fillstyle='none', ms = 10)
    unit_circle = Circle((0,0), radius=1, fill=False,
                         color='black', ls='solid', alpha=0.9)
    ax.add_patch(unit_circle)
    ax.axvline(0, color='0.7')
    ax.axhline(0, color='0.7')
    
    plt.title(title)
    plt.xlabel(r'Re{$z$}')
    plt.ylabel(r'Im{$z$}')
    plt.axis('equal')
    plt.xlim((-2, 2))
    plt.ylim((-2, 2))
    plt.grid()


# design filter
b, a = sig.butter(N, 0.2)
# decomposition into SOS
sos = sig.tf2sos(b, a, pairing='nearest')


# print filter coefficients
print('Coefficients of the recursive part \n')
print(['%1.2f'%ai for ai in a])
print('\n')
print('Coefficients of the recursive part of the individual SOS \n')
print('Section \t a1 \t\t a2')
for n in range(sos.shape[0]):
    print('%d \t\t %1.5f \t %1.5f'%(n, sos[n, 4], sos[n, 5]))

# plot pole and zero locations
plt.figure(figsize=(5,5))
zplane(np.roots(b), np.roots(a), 'Poles and Zeros - Overall')

plt.figure(figsize=(10, 7))
for n in range(sos.shape[0]):  
    plt.subplot(231+n)
    zplane(np.roots(sos[n, 0:3]), np.roots(sos[n, 3:6]), title='Poles and Zeros - Section %d'%n)
plt.tight_layout()

# compute and plot frequency response of sections
plt.figure(figsize=(10,5))
for n in range(sos.shape[0]):
    Om, H = sig.freqz(sos[n, 0:3], sos[n, 3:6])
    plt.plot(Om, 20*np.log10(np.abs(H)), label=r'Section %d'%n)

plt.xlabel(r'$\Omega$')
plt.ylabel(r'$|H_n(e^{j \Omega})|$ in dB')
plt.legend()
plt.grid()"
"import warnings
warnings.filterwarnings('ignore')
%matplotlib inline
import numpy as np
import matplotlib.pyplot as plt
import scipy.signal as sig


N = 8192  # length of signals
w = 8  # wordlength for requantization of multiplications


def uniform_midtread_quantizer(x):
    # linear uniform quantization
    xQ = Q * np.floor(x/Q + 1/2)
    
    return xQ


def no_quantizer(x):
    
    return x


def sos_df1(x, a, requantize=None):
    y = np.zeros(len(x)+2)  # initial value appended
    for k in range(len(x)):
        y[k] = x[k] - requantize(a[1]*y[k-1])  - requantize(a[2]*y[k-2])
        
    return y[0:-2]


# cofficients of the SOS
p = 0.90*np.array([np.exp(1j*np.pi/3), np.exp(-1j*np.pi/3)])
a = np.poly(p)
# quantization step
Q = 1/(2**(w-1))

# compute input signal
x = np.random.uniform(low=-1, high=1, size=N)
# compute output signals w and w/o requantization
yQ = sos_df1(x, a, requantize=uniform_midtread_quantizer)
y = sos_df1(x, a, requantize=no_quantizer)
# compute requantization error
e = yQ-y
# Signal-to-noise ratio
SNR = 10*np.log10(np.var(y)/np.var(e))
print('SNR due to requantization: %f dB'%SNR)

# estimate PSD of requantization error
nf, Pxx = sig.welch(e, window='hamming', nperseg=256, noverlap=128)
Pxx = .5*Pxx  # due to normalization in scipy.signal
Om = 2*np.pi*nf
# compute frequency response of system
w, H = sig.freqz([1,0,0], a)


# plot results
plt.figure(figsize=(10,4))
plt.plot(Om, Pxx/Q**2 * 12, 'b', label=r'$|\hat{\Phi}_{ee}(e^{j \Omega})|$')
plt.plot(w, np.abs(H)**2 * 2, 'g', label=r'$|H(e^{j \Omega})|^2$')
plt.title('Estimated PSD and transfer function of requantization noise')
plt.xlabel(r'$\Omega$')
plt.ylabel(r'$Q^2/12$')
plt.axis([0, np.pi, 0, 100])
plt.legend()
plt.grid();"
"# compute input signal
x = np.random.uniform(low=-1, high=1, size=256)
x = np.concatenate((x, np.zeros(1024)))
# compute output signal
yQ = sos_df1(x, a, requantize=uniform_midtread_quantizer)

# plot results
np.seterr(divide='ignore')
plt.figure(figsize=(10, 3))
plt.plot(20*np.log10(np.abs(yQ)))
plt.title('Level of output signal')
plt.xlabel(r'$k$')
plt.ylabel(r'$|y_Q[k]|$ in dB')
plt.grid()

plt.figure(figsize=(10, 3))
k = np.arange(1000, 1050)
plt.stem(k, yQ[k]/Q)
plt.title('Output signal for zero input')
plt.xlabel(r'$k$')
plt.ylabel(r'$y_Q[k] / Q$ ')
plt.axis([k[0], k[-1], -3, 3])
plt.grid();"
"def uniform_midtread_quantizer(x, xmin=1):
    # limiter
    x = np.copy(x)
    if x <= -xmin:
        x = -1
    if x > xmin - Q:
        x = 1 - Q
    # linear uniform quantization
    xQ = Q * np.floor(x/Q + 1/2)
    
    return xQ


# compute input signal
x = np.random.uniform(low=-1, high=1, size=256)
x = np.concatenate((x, np.zeros(1024)))
# compute output signal
yQ = sos_df1(x, 2*a, requantize=uniform_midtread_quantizer)

# plot results
plt.figure(figsize=(10, 3))
plt.plot(20*np.log10(np.abs(yQ)))
plt.title('Level of output signal')
plt.xlabel(r'$k$')
plt.ylabel(r'$|y_Q[k]|$ in dB')
plt.grid()

plt.figure(figsize=(10, 3))
k = np.arange(1000, 1050)
plt.stem(k, yQ[k])
plt.title('Output signal for zero input')
plt.xlabel(r'$k$')
plt.ylabel(r'$y_Q[k]$ ')
plt.grid();"
"import warnings
warnings.filterwarnings('ignore')
%matplotlib inline
import numpy as np
import matplotlib.pyplot as plt

K = 32  # number of temporal samples
N = 10000  # number of sample functions
bins = 100  # number of bins for the histogram


# draw sample functions from a random process
np.random.seed(2)
x = np.random.normal(size=(N, K))
x += np.tile(np.cos(2*np.pi/K*np.arange(K)), [N, 1])

# compute the histogram
px = np.zeros((bins, K))
for k in range(K):
    px[:, k], edges = np.histogram(x[:, k], bins=bins, range=(-4,4), density=True)
    
# compute the CDF
Px = np.cumsum(px, axis=0) * 8/bins

# plot the PDF
plt.figure(figsize=(10,6))
plt.pcolor(np.arange(K), edges, px)
plt.title(r'Estimated PDF $\hat{p}_x(\theta, k)$')
plt.xlabel(r'$k$')
plt.ylabel(r'$\theta$')
plt.colorbar()
plt.autoscale(tight=True)

# plot the CDF
plt.figure(figsize=(10,6))
plt.pcolor(np.arange(K), edges, Px, vmin=0, vmax=1)
plt.title(r'Estimated CDF $\hat{P}_x(\theta, k)$')
plt.xlabel(r'$k$')
plt.ylabel(r'$\theta$')
plt.colorbar()
plt.autoscale(tight=True)"
"import warnings
warnings.filterwarnings('ignore')
%matplotlib inline
import numpy as np
import matplotlib.pyplot as plt

N = 5  # number of sample functions


# draw N sample functions from a random process
np.random.seed(0)
x = np.random.normal(size=(N, 32))

# plot sample functions
fig = plt.figure(figsize=(10, 12))
for n in range(N):
    plt.subplot(N, 1, n+1)
    plt.tight_layout()
    plt.stem(x[n,:], basefmt='k-')
    plt.title('Sample Function %d' %n)
    plt.xlabel(r'$k$')
    plt.ylabel(r'$x_%d[k]$' %n)
    plt.axis([-1, 32, -3, 3])
    plt.grid()"
"plot_pdf_cdf(np.linspace(-.5, 1.5, num=1000), stats.uniform(loc=0, scale=1))"
"np.random.seed(1)
estimate_plot_pdf_cdf(stats.uniform.rvs(size=100000, loc=0, scale=1), nbins=100)"
"plot_pdf_cdf(np.linspace(-5, 5, num=100), stats.norm(loc=0, scale=1))"
"estimate_plot_pdf_cdf(stats.norm.rvs(size=100000, loc=0, scale=1), nbins=100)"
"plot_pdf_cdf(np.linspace(-5, 5, num=100), stats.laplace(loc=0, scale=1/np.sqrt(2)))"
"estimate_plot_pdf_cdf(stats.laplace(scale=1/np.sqrt(2)).rvs(size=10000), nbins=100)"
"from scipy.io import wavfile

fs, x = wavfile.read('../data/speech_8k.wav')
x = np.asarray(x, dtype=float)/2**15
estimate_plot_pdf_cdf(x, nbins=100)"
"import warnings
warnings.filterwarnings('ignore')
%matplotlib inline
import numpy as np
import matplotlib.pyplot as plt

N = 8192  # total length of signal
P = 128  # period
K = 275  # upper/lower limit for lag in ACF

# generate periodic random signal
np.random.seed(1)
x0 = np.random.normal(size=P)
x = np.tile(x0, N//P)

# compute and truncate ACF
acf = 1/len(x) * np.correlate(x, x, mode='full')
acf = acf[(len(x)-1)-(K-1):(len(x)-1)+K]
kappa = np.arange(-(K-1), K)

# plot signal and its ACF
plt.figure(figsize = (10, 4))
plt.stem(x[:512])
plt.xlabel(r'$k$')
plt.ylabel(r'$x[k]$')
plt.grid()

plt.figure(figsize = (10, 4))
plt.stem(kappa, acf)
plt.xlabel(r'$\kappa$')
plt.ylabel(r'$\hat{\varphi}_{xx}[\kappa]$')
plt.axis([-K, K, 1.1*min(acf), 1.1*max(acf)]);
plt.grid()"
"from scipy.io import wavfile

K = 30  # upper/lower limit for lag in ACF

# read audio file 
fs, x = wavfile.read('../data/speech_8k.wav')
x = np.asarray(x, dtype=float)/2**15

# compute and truncate ACF
acf = 1/len(x) * np.correlate(x, x, mode='full')
acf = acf[(len(x)-1)-(K-1):(len(x)-1)+K]
kappa = np.arange(-(K-1), K)

# plot ACF
plt.figure(figsize = (10, 8))
plt.stem(kappa, acf)
plt.xlabel(r'$\kappa$')
plt.ylabel(r'$\hat{\varphi}_{xx}[\kappa]$')
plt.axis([-K, K, 1.1*min(acf), 1.1*max(acf)]);
plt.grid()"
"plt.figure(figsize = (10, 4))

plt.subplot(121)
compute_plot_CCF(prn[10, :], prn[10, :], r'$\hat{\varphi}_{x_n x_n}[\kappa]$')
plt.title('ACF')

plt.subplot(122)
compute_plot_CCF(prn[10, :], prn[11, :], r'$\hat{\varphi}_{x_n x_m}[\kappa]$')
plt.title('CCF between two PRN sequences')
plt.ylim([-1, 1])
plt.tight_layout()"
"k0 = 10  # true TOA

x = prn[10, :]  # pick one PRN sequence
y = x[k0:] # delay transmitted signal by k0 samples

# compute and plot CCF
plt.figure(figsize = (10, 4))
ccf = compute_plot_CCF(x, y, r'$\hat{\varphi}_{xx}[\kappa]$')
plt.title('CCF between transmitted and received signal')

# estimate the TOA
print('Estimated TOA is {:2.0f} samples'.format(np.argmax(ccf) - K))"
"N = 1024  # length of random signals

# generate two uncorrelated random signals
np.random.seed(2)
x = 2 + np.random.normal(size=N)
y = 1 + np.random.normal(size=2*N)

# compute CCF
ccf = 1/len(x) * np.correlate(x, y, mode='full')
kappa = np.arange(-(N-1), 2*N)

# print mean values of signals
print('Mean of signal x[k]: %f' %np.mean(x))
print('Mean of signal y[k]: %f' %np.mean(y))

# plot CCF
plt.figure(figsize = (10, 8))
plt.stem(kappa, ccf)
plt.title('Estimated cross-correlation function')
plt.ylabel(r'$\hat{\varphi}_{xy}[\kappa]$')
plt.xlabel(r'$\kappa$')
plt.axis([kappa[0], kappa[-1], 0, 1.1*max(ccf)]);
plt.grid()"
"import warnings
warnings.filterwarnings('ignore')
%matplotlib inline
import numpy as np
import matplotlib.pyplot as plt

K = 100000  # length of random signals

# generate random signals
np.random.seed(2)
x = np.random.uniform(size=K)
n = np.random.uniform(size=K)
y = x + n

# plot estimated pdf
plt.figure(figsize = (10, 6))
plt.hist(y, 100, density=True)
plt.title('Estimated PDF')
plt.xlabel(r'$\theta$')
plt.ylabel(r'$\hat{p}_y(\theta)$')
plt.grid()"
"N = 1000  # number of independent random signals

# generate random signals
np.random.seed(2)
y = np.sum(np.random.uniform(size=(N, K)), axis=0)

# plot estimated pdf
plt.figure(figsize = (10, 6))
plt.hist(y, 100, density=True)
plt.title('Estimated PDF')
plt.xlabel(r'$\theta$')
plt.ylabel(r'$\hat{p}_y(\theta)$')
plt.grid()"
"N = 1024  # length of compound signals
M = 25  # period of cosine signal
K = 2*M  # maximum lag for ACF/CCF

# generate signals
x = np.cos(2*np.pi/M * np.arange(N))
np.random.seed(2)
n = np.random.normal(size=N)
# superposition of signals
y = x + n

# compute and truncate ACF of superposition
acf = 1/N * np.correlate(y, y, mode='full')
acf = acf[(len(y)-1)-K:len(y)+K]
# compute and truncate CCF of superposition and noise
ccf = 1/N * np.correlate(n, y, mode='full')
ccf = ccf[(len(y)-1)-K:len(y)+K]


# plot results
kappa = np.arange(-K, K+1)

plt.figure(figsize=(10, 10))

plt.subplot(311)
plt.stem(y)
plt.title('Signal')
plt.xlabel(r'$k$')
plt.ylabel(r'$x[k]$')
plt.axis([0, K, -3, 3])

plt.subplot(312)
plt.stem(kappa, acf)
plt.title('ACF of superposition')
plt.xlabel(r'$\kappa$')
plt.ylabel(r'$\varphi_{yy}[\kappa]$')
plt.axis([-K, K, -.75, 1.1*np.max(acf)])
plt.grid()

plt.subplot(313)
plt.stem(kappa, ccf)
plt.title('CCF between noise and superposition')
plt.xlabel(r'$\kappa$')
plt.ylabel(r'$\varphi_{ny}[\kappa]$')
plt.axis([-K, K, -.2, 1.1])
plt.grid()

plt.tight_layout()"
"K = 1024  # number of samples
Nmax = 50  # number of observations

# generate signals
x = np.cos(2*np.pi/50 * np.arange(N))
np.random.seed(2)
n = np.random.normal(size=(Nmax, K))
# AWGN model
y = np.tile(x, (Nmax,1)) + n
# repeated averaging up to Nmax
xhat = np.zeros_like(y)
for i in range(Nmax):
    xhat[i, :] = 1/(i+1) * np.sum(y[:i+1, :], axis=0)
# compute SNR for all averages
Px = np.var(x)
Pn = np.var(xhat - x, axis=1)
SNR = 10*np.log10(Px/Pn)

# plot results
plt.figure(figsize=(10, 10))

plt.subplot(311)
plt.stem(y[0, :100])
plt.title('One observation')
plt.xlabel(r'$k$')
plt.ylabel(r'$y_0[k]$')
plt.ylim([-3, 3])

plt.subplot(312)
plt.stem(xhat[Nmax-1, :100])
plt.title('Average over {:2.0f} observations'.format(Nmax))
plt.xlabel(r'$k$')
plt.ylabel(r'$\hat{x}[k]$')
plt.ylim([-3, 3])

plt.subplot(313)
plt.plot(range(1, Nmax+1), 10*np.log10(Px * range(1, Nmax+1)), '--', label='theory')
plt.plot(range(1, Nmax+1), SNR, label='simulated')
plt.title('SNR')
plt.xlabel('number of averages $N$')
plt.ylabel('SNR in dB')
plt.grid()
plt.legend()

plt.tight_layout()"
"import warnings
warnings.filterwarnings('ignore')
%matplotlib inline
import numpy as np
import matplotlib.pyplot as plt

L = 64  # number of random samples
N = 1000  # number of sample functions

# generate sample functions
np.random.seed(1)
x = np.random.normal(size=(N, L))
x1 = x + np.tile(np.cos(2*np.pi/L*np.arange(L)), [N,1])
h = 2*np.fft.irfft([1,1,1,0,0,0])
x2 = np.asarray([np.convolve(x[n,:], h, mode='same') for n in range(N)])

# compute and plot results
def compute_plot_results(x):
    
    # estimate linear mean by ensemble average
    mu = 1/N * np.sum(x, 0)
    # estimate the auto-correlation function
    acf = np.zeros((L, L))
    for n in range(L):
        for m in range(L):
            acf[n, m] = 1/N * np.sum(x[:, n]*x[:, m], 0)
    
    plt.subplot(121)
    plt.stem(mu)
    plt.title(r'Estimate of linear mean $\hat{\mu}_x[k]$')
    plt.xlabel(r'$k$')
    plt.ylabel(r'$\hat{\mu}[k]$')
    plt.axis([0, L, -1.5, 1.5])

    plt.subplot(122)
    plt.pcolor(np.arange(L), np.arange(L), acf, vmin=-2, vmax=2)
    plt.title(r'Estimate of ACF $\hat{\varphi}_{xx}[k_1, k_2]$')
    plt.xlabel(r'$k_1$')
    plt.ylabel(r'$k_2$')
    plt.colorbar()
    plt.autoscale(tight=True)

    
plt.figure(figsize = (10, 4))
plt.gcf().suptitle('Random Process 1', fontsize=12, y=1.05)
compute_plot_results(x1)

plt.figure(figsize = (10, 4))
plt.gcf().suptitle('Random Process 2', fontsize=12, y=1.05)
compute_plot_results(x2)"
compute_plot_results(x1)
compute_plot_results(x2)
compute_plot_results(x3)
"import warnings
warnings.filterwarnings('ignore')
%matplotlib inline
import numpy as np
import matplotlib.pyplot as plt

K = 64  # number of random samples
N = 1000  # number of sample functions


# generate the sample functions
np.random.seed(5)
x = np.random.normal(size=(N, K))
x += np.tile(np.cos(2*np.pi/K*np.arange(K)), [N, 1])

# estimate the linear mean as ensemble average
mu = 1/N * np.sum(x, 0)
# estimate the quadratic mean
qu = 1/N * np.sum(x**2, 0)
# estimate the variance
sigma = 1/N * np.sum((x-mu)**2, 0)


# plot results
plt.rc('figure', figsize=(10, 3))

plt.figure()
plt.stem(x[0, :])
plt.title(r'Sample function $x_0[k]$')
plt.xlabel(r'$k$')
plt.ylabel(r'$x_0[k]$')
plt.axis([0, K, -3, 3])

plt.figure()
plt.stem(mu)
plt.title(r'Estimate of linear mean')
plt.xlabel(r'$k$')
plt.ylabel(r'$\hat{\mu}_x[k]$')
plt.axis([0, K, -1.5, 1.5])

plt.figure()
plt.stem(qu)
plt.title(r'Estimate of quadratic mean')
plt.xlabel(r'$k$')
plt.ylabel(r'$\hat{E}\{x^2[k]\}$')
plt.axis([0, K, 0, 2.5])

plt.figure()
plt.stem(sigma)
plt.title(r'Estimate of variance')
plt.xlabel(r'$k$')
plt.ylabel(r'$\hat{\sigma}^2_x[k]$')
plt.axis([0, K, 0, 1.5]);"
"L = 64  # number of random samples
N = 1000  # number of sample functions

# generate sample functions
np.random.seed(1)
r = np.random.normal(size=(N, L))
h = np.random.normal(size=(N, 10))
x = np.asarray([np.convolve(r[n,:], h[n,:], mode='same') for n in range(N)]) \
    + np.tile(np.cos(2*np.pi/L*np.arange(L)), [N,1])

# estimate the auto-correlation function (ACF)
acf = np.zeros((L, L))
for n in range(L):
    for m in range(L):
        acf[n, m] = 1/N * np.sum(x[:, n]*x[:, m], axis=0)

# plot ACF
plt.figure(figsize = (7, 5))
plt.pcolor(np.arange(L), np.arange(L), acf, vmin=-2, vmax=2)
plt.title(r'Estimate of ACF $\hat{\varphi}_{xx}[k_1, k_2]$')
plt.xlabel(r'$k_1$')
plt.ylabel(r'$k_2$')
plt.colorbar()
plt.axis('tight');"
"import warnings
warnings.filterwarnings('ignore')
%matplotlib inline
import numpy as np
import matplotlib.pyplot as plt
from scipy.io import wavfile

# read audio file 
fs, x = wavfile.read('../data/speech_8k.wav')
x = np.asarray(x, dtype=float)
N = len(x)

# compute ACF
acf = 1/len(x) * np.correlate(x, x, mode='full')
# compute PSD
psd = np.fft.fft(acf)
psd = psd * np.exp(1j*np.arange(2*N-1)*2*np.pi*(N-1)/(2*N-1))
f = np.fft.fftfreq(2*N-1, d=1/fs)

# plot PSD
plt.figure(figsize = (10, 8))
plt.plot(f, np.real(psd))
plt.title('Estimated power spectral density')
plt.ylabel(r'$\hat{\Phi}_{xx}(e^{j \Omega})$')
plt.xlabel(r'$f$')
plt.axis([0, 2000, 0, 1.1*max(np.abs(psd))]);
plt.grid()"
"N = 1024  # length of random signal x

# generate two random signals
np.random.seed(2)
x = 2 + np.random.normal(size=N)
y = 1 + np.random.normal(size=2*N)

# compute cross PSD via CCF
acf = 1/N * np.correlate(x, y, mode='valid')
psd = np.fft.fft(acf)
psd = psd * np.exp(1j*np.arange(N+1)*2*np.pi*(N-1)/(2*N-1))

# plot results
f = np.fft.fftfreq(len(psd), d=1/2)
plt.figure(figsize = (10, 4))
plt.stem(f, np.real(psd))
plt.title('Estimated cross power spectral density')
plt.ylabel(r'$\hat{\Phi}_{xy}(e^{j \Omega})$')
plt.xlabel(r'$\Omega/ \pi$')
plt.grid()"
"import warnings
warnings.filterwarnings('ignore')
%matplotlib inline
import numpy as np
import matplotlib.pyplot as plt

A = 1.2  # amplitude of signal
Q = 1/10  # quantization stepsize
N = 2000  # number of samples

def uniform_midtread_quantizer(x, Q):
    # limiter
    x = np.copy(x)
    idx = np.where(np.abs(x) >= 1)
    x[idx] = np.sign(x[idx])
    # linear uniform quantization
    xQ = Q * np.floor(x/Q + 1/2)
    
    return xQ

def plot_signals(x, xQ):
    e = xQ - x
    plt.figure(figsize=(10,6))
    plt.plot(x, label=r'signal $x[k]$')
    plt.plot(xQ, label=r'quantized signal $x_Q[k]$')
    plt.plot(e, label=r'quantization error $e[k]$')
    plt.xlabel(r'$k$')
    plt.axis([0, N, -1.1*A, 1.1*A])
    plt.legend()
    plt.grid()

# generate signal
x = A * np.sin(2*np.pi/N * np.arange(N))
# quantize signal
xQ = uniform_midtread_quantizer(x, Q)
# plot signals
plot_signals(x, xQ)"
"A = 1.2  # amplitude of signal
Q = 1/10  # quantization stepsize
N = 2000  # number of samples

def uniform_midrise_quantizer(x, Q):
    # limiter
    x = np.copy(x)
    idx = np.where(np.abs(x) >= 1)
    x[idx] = np.sign(x[idx])
    # linear uniform quantization
    xQ = Q * (np.floor(x/Q) + .5)
    
    return xQ

# generate signal
x = A * np.sin(2*np.pi/N * np.arange(N))
# quantize signal
xQ = uniform_midrise_quantizer(x, Q)
# plot signals
plot_signals(x, xQ)"
"import warnings
warnings.filterwarnings('ignore')
%matplotlib inline
import numpy as np
import matplotlib.pyplot as plt
import scipy.signal as sig

w = 8  # wordlength of the quantized signal
xmin = -1  # minimum of input signal
N = 32768  # number of samples


def uniform_midtread_quantizer_w_ns(x, Q):
    # limiter
    x = np.copy(x)
    idx = np.where(x <= -1)
    x[idx] = -1
    idx = np.where(x > 1 - Q)
    x[idx] = 1 - Q
    # linear uniform quantization with noise shaping
    xQ = Q * np.floor(x/Q + 1/2)
    e = xQ - x
    xQ = xQ - np.concatenate(([0], e[0:-1]))
    
    return xQ[1:]


# quantization step
Q = 1/(2**(w-1))
# compute input signal
np.random.seed(5)
x = np.random.uniform(size=N, low=xmin, high=(-xmin-Q))
# quantize signal
xQ = uniform_midtread_quantizer_w_ns(x, Q)
e = xQ - x[1:]
# estimate PSD of error signal
nf, Pee = sig.welch(e, nperseg=64)
# estimate SNR
SNR = 10*np.log10((np.var(x)/np.var(e)))
print('SNR = {:2.1f} dB'.format(SNR))


plt.figure(figsize=(10,5))
Om = nf*2*np.pi
plt.plot(Om, Pee*6/Q**2, label='estimated PSD')
plt.plot(Om, np.abs(1 - np.exp(-1j*Om))**2, label='theoretic PSD')
plt.plot(Om, np.ones(Om.shape), label='PSD w/o noise shaping')
plt.title('PSD of quantization error')
plt.xlabel(r'$\Omega$')
plt.ylabel(r'$\hat{\Phi}_{e_H e_H}(e^{j \Omega}) / \sigma_e^2$')
plt.axis([0, np.pi, 0, 4.5]);
plt.legend(loc='upper left')
plt.grid()"
"import warnings
warnings.filterwarnings('ignore')
%matplotlib inline
import numpy as np
import matplotlib.pyplot as plt

N = 1024  # length of signal

# generate signal
x = np.sin(2*np.pi/N * np.arange(N))
# quantize signal
xi = np.round(3 * x)
xQ = 1/3 * xi
e = xQ - x

# plot (quantized) signals
fig, ax1 = plt.subplots(figsize=(10,4))
ax2 = ax1.twinx()

ax1.plot(x, 'r', label=r'signal $x[k]$')
ax1.plot(xQ, 'b', label=r'quantized signal $x_Q[k]$')
ax1.plot(e, 'g', label=r'quantization error $e[k]$')
ax1.set_xlabel('k')
ax1.set_ylabel(r'$x[k]$, $x_Q[k]$, $e[k]$')
ax1.axis([0, N, -1.2, 1.2])
ax1.legend()

ax2.set_ylim([-3.6, 3.6])
ax2.set_ylabel('quantization index')
ax2.grid()"
"import warnings
warnings.filterwarnings('ignore')
%matplotlib inline
import numpy as np
import matplotlib.pyplot as plt
import scipy.signal as sig

w = 16  # wordlength of the quantized signal
L = 2**np.arange(1,10)  # oversampling factors

N = 8192  # length of signal
Om0 = 100*2*np.pi/N  # frequency of harmonic signal
Q = 1/(2**(w-1))  # quantization step


def uniform_midtread_quantizer(x, Q):
    # limiter
    x = np.copy(x)
    idx = np.where(x <= -1)
    x[idx] = -1
    idx = np.where(x > 1 - Q)
    x[idx] = 1 - Q
    # linear uniform quantization
    xQ = Q * np.floor(x/Q + 1/2)
    
    return xQ

def SNR_oversampled_ADC(L):
    x = (1-Q)*np.cos(Om0*np.arange(N))
    xu = (1-Q)*np.cos(Om0*np.arange(N*L)/L)
    # quantize signal
    xQu = uniform_midtread_quantizer(xu, Q)
    # low-pass filtering and decimation
    xQ = sig.resample(xQu, N)
    # estimate SNR
    e = xQ - x
    
    return 10*np.log10((np.var(x)/np.var(e)))


# compute SNR for oversampled ADC
SNR = [SNR_oversampled_ADC(l) for l in L]

# plot result
plt.figure(figsize=(10, 4))
plt.semilogx(L, SNR, label='SNR with oversampling')
plt.plot(L, (6.02*w+1.76)*np.ones(L.shape), label='SNR w/o oversampling' )
plt.xlabel(r'oversampling factor $L$')
plt.ylabel(r'SNR in dB')
plt.legend(loc='upper left')
plt.grid()"
"import warnings
warnings.filterwarnings('ignore')
%matplotlib inline
import numpy as np
import matplotlib.pyplot as plt
import scipy.signal as sig

N = 128  # length of segment
M = 64  # stepsize
L = 100  # total number of segments

# generate random signal
np.random.seed(5)
x = np.random.normal(size=L*M)

# compute periodogram by Welch's method
nf, Pxx = sig.welch(x, window='hamming', nperseg=N, noverlap=(N-M))
Pxx = .5*Pxx  # due to normalization in scipy.signal
Om = 2*np.pi*nf

# plot results
plt.figure(figsize=(10,4))
plt.stem(Om, Pxx, 'b', label=r'$\hat{\Phi}_{xx}(e^{j \Omega})$', basefmt=' ')
plt.plot(Om, np.ones_like(Pxx), 'r', label=r'$\Phi_{xx}(e^{j \Omega})$')
plt.title('Estimated and true PSD')
plt.xlabel(r'$\Omega$')
plt.axis([0, np.pi, 0, 2])
plt.legend()

# compute mean value of the periodogram
print('Mean value of the periodogram: %f' %np.mean(np.abs(Pxx)))"
"import warnings
warnings.filterwarnings('ignore')
%matplotlib inline
import numpy as np
import matplotlib.pyplot as plt
import scipy.signal as sig

N = 256  # number of samples
M = 5  # number of sample functions

# generate random signal
np.random.seed(1)
x = np.random.normal(size=(M, N))
h = sig.firwin2(N, [0, .4, .42, .65, .67, 1], [0, 0, 1, 1, 0, 0])
x = [np.convolve(xi, h, mode='same') for xi in x]

# DFT of signal
X = np.fft.rfft(x, axis=1)
Om = np.linspace(0, np.pi, X.shape[1])

# plot signal and its spectrum
plt.figure(figsize=(10,4))
plt.plot(Om, np.abs(X.T))
plt.title('Magnitude spectrum')
plt.xlabel(r'$\Omega[\mu]$')
plt.ylabel(r'$|X[\mu]|$')
plt.axis([0, np.pi, 0, 30]);"
"import warnings
warnings.filterwarnings('ignore')
%matplotlib inline
import numpy as np
import matplotlib.pyplot as plt

N = 128  # number of samples

# generate random signal
np.random.seed(5)
x = np.random.normal(size=N)

# compute magnitude of the periodogram
x = np.concatenate((x, np.zeros_like(x)))
X = np.fft.rfft(x)
Om = np.linspace(0, np.pi, len(X))
Sxx = 1/N * abs(X)**2

# plot results
plt.figure(figsize=(10,4))
plt.stem(Om, Sxx, 'b', label=r'$|\hat{\Phi}_{xx}(e^{j \Omega})|$')
plt.plot(Om, np.ones_like(Sxx), 'r', label=r'$\Phi_{xx}(e^{j \Omega})$')
plt.title('Estimated and true PSD')
plt.xlabel(r'$\Omega$')
plt.axis([0, np.pi, 0, 4])
plt.legend()

# compute mean value of the periodogram
print('Mean value of the periodogram: {0:1.3f}'.format(np.mean(np.abs(Sxx))))
print('Variance of the periodogram: {0:1.3f}'.format(np.var(np.abs(Sxx))))"
"import warnings
warnings.filterwarnings('ignore')
%matplotlib inline
import numpy as np
import matplotlib.pyplot as plt
import statsmodels.api as sm
import scipy.signal as sig


K = 4096  # length of random signal
N = 3  # order of AR model
a = np.array((1, -1, .5))  # coefficients of AR model

# generate random signal n[k]
np.random.seed(2)
n = np.random.normal(size=K)

# AR model for random signal x[k]
x = np.zeros(K)
for k in np.arange(3, K):
    x[k] = a[0]*x[k-1] + a[1]*x[k-2] + a[2]*x[k-3] + n[k]
    
# estimate AR parameters by Yule-Walker method
rho, sigma = sm.regression.yule_walker(x, order=N, method='mle')

# compute true and estimated transfer function
Om, H = sig.freqz(1, np.insert(-a, 0, 1))
Om, He = sig.freqz(1, np.insert(-rho, 0, 1))
# compute PSD by Welch method
Om2, Pxx = sig.welch(x, return_onesided=True)

# plot PSDs
plt.figure(figsize=(10,5))
plt.plot(Om, np.abs(H)**2, label=r'$\Phi_{xx}(e^{j\Omega})$')
plt.plot(Om2*2*np.pi, .5*np.abs(Pxx), 'k-', alpha=.5 , label=r'$\hat{\Phi}_{xx}(e^{j\Omega})$ (Welch)')
plt.plot(Om, np.abs(He)**2, label=r'$\hat{\Phi}_{xx}(e^{j\Omega})$ (parametric)')

plt.xlabel(r'$\Omega$')
plt.axis([0, np.pi, 0, 20])
plt.legend()
plt.grid()"
"import warnings
warnings.filterwarnings('ignore')
%matplotlib inline
import numpy as np
import matplotlib.pyplot as plt

N = 16  # length of the signal
M = 32  # length of zero-padded signal
Om0 = 5.33*(2*np.pi/N)  # frequency of exponential signal


# DFT of the exponential signal
xN = np.exp(1j*Om0*np.arange(N))
XN = np.fft.fft(xN)
# DFT of the zero-padded exponential signal
xM = np.concatenate((xN, np.zeros(M-N)))
XM = np.fft.fft(xM)


# plot spectra
plt.figure(figsize = (10, 6))

plt.subplot(121)
plt.stem(np.arange(N),np.abs(XN))
plt.title(r'DFT$_{%d}$ of $e^{j \Omega_0 k}$ without zero-padding' %N)
plt.xlabel(r'$\mu$')
plt.ylabel(r'$|X_N[\mu]|$')
plt.axis([0, N, 0, 18])
plt.grid()

plt.subplot(122)
plt.stem(np.arange(M),np.abs(XM))
plt.title(r'DFT$_{%d}$ of $e^{j \Omega_0 k}$ with zero-padding' %M)
plt.xlabel(r'$\mu$')
plt.ylabel(r'$|X_M[\mu]|$')
plt.axis([0, M, 0, 18])
plt.grid()"
"N = 16  # order of periodic sinc function
M = 1024  # number of frequency points
Om = np.linspace(-np.pi, np.pi, M)

# definition of periodic sinc function
def psinc(x, N):
    x = np.asanyarray(x)
    y = np.where(x == 0, 1.0e-20, x)
    return 1/N * np.sin(N/2*y)/np.sin(1/2*y)

# plot psinc
plt.figure(figsize = (10, 8))
plt.plot(Om, psinc(Om, 16))
plt.xlabel(r'$\Omega$')
plt.ylabel(r'$\mathrm{psinc}_N (\Omega)$')
plt.grid()"
"N = 16  # length of the signal
M = 1024  # number of frequency points for DTFT
Om0 = 5.33*(2*np.pi/N)  # frequency of exponential signal


# DFT of the exponential signal
xN = np.exp(1j*Om0*np.arange(N))
XN = np.fft.fft(xN)

# interpolation of DTFT from DFT coefficients
Xi = np.asarray(np.zeros(M), dtype=complex)
for mu in np.arange(M):
    Omd = 2*np.pi/M*mu-2*np.pi*np.arange(N)/N
    interpolator = psinc(Omd, N) * np.exp(-1j*Omd*(N-1)/2)
    Xi[mu] = np.sum(XN * interpolator)

# plot spectra
plt.figure(figsize = (10, 8))
ax1 = plt.gca()

plt.plot(np.arange(M)*2*np.pi/M, abs(Xi), 'r', label=r'$|X_N(e^{j \Omega})|$')
plt.stem(np.arange(N)*2*np.pi/N, abs(XN), basefmt = ' ', label=r'$|X_N[\mu]|$')
plt.title(r'DFT $X_N[\mu]$ and interpolated DTFT $X_N(e^{j \Omega})$', y=1.08)
plt.ylim([-0.5, N+2]);
plt.legend()

ax1.set_xlabel(r'$\Omega$')
ax1.set_xlim([0, 2*np.pi])
ax1.grid()

ax2 = ax1.twiny()
ax2.set_xlim([0, N])
ax2.set_xlabel(r'$\mu$', color='C0')
ax2.tick_params('x', colors='C0')"
"N = 16  # length of the signal
M = 32  # number of points for interpolated DFT
Om0 = 5.33*(2*np.pi/N)  # frequency of exponential signal


# periodic sinc function
def psinc(x, N):
    x = np.asanyarray(x)
    y = np.where(x == 0, 1.0e-20, x)
    return 1/N * np.sin(N/2*y)/np.sin(1/2*y)

# DFT of the exponential signal
xN = np.exp(1j*Om0*np.arange(N))
XN = np.fft.fft(xN)

# interpolation of DFT coefficients
XM = np.asarray(np.zeros(M), dtype=complex)
for mu in np.arange(M):
    Omd = 2*np.pi/M*mu-2*np.pi*np.arange(N)/N
    interpolator = psinc(Omd, N) * np.exp(-1j*Omd*(N-1)/2)
    XM[mu] = np.sum(XN * interpolator)

# plot spectra
plt.figure(figsize = (10, 6))

plt.subplot(121)
plt.stem(np.arange(N),np.abs(XN))
plt.title(r'DFT of $e^{j \Omega_0 k}$ without zero-padding')
plt.xlabel(r'$\mu$')
plt.ylabel(r'$|X_N[\mu]|$')
plt.axis([0, N, 0, 18])
plt.grid()

plt.subplot(122)
plt.stem(np.arange(M),np.abs(XM))
plt.title(r'Interpolated spectrum')
plt.xlabel(r'$\mu$')
plt.ylabel(r'$|X_M[\mu]|$')
plt.axis([0, M, 0, 18])
plt.grid()"
dft_window_function(np.ones(64))
dft_window_function(sig.triang(63))
dft_window_function(np.hanning(64))
dft_window_function(np.hamming(64))
dft_window_function(np.blackman(64))
"dft_signal_mixture_window(32, 1, 10.3, 0.1, 15.2, np.blackman(32))"
"import warnings
warnings.filterwarnings('ignore')
%matplotlib inline
import numpy as np
import matplotlib.pyplot as plt

Om0 = 1  # frequency of exponential signal
N = 32  # length of signal


# DTFT of finite length exponential signal (analytic)
Om = np.linspace(-np.pi, np.pi, num=1024)
XN = np.exp(-1j * (Om-Om0) * (N-1) / 2) * (np.sin(N * (Om-Om0) / 2)) / (np.sin((Om-Om0) / 2))

# plot spectrum
plt.figure(figsize = (10, 8))
plt.plot(Om, abs(XN), 'r')
plt.title(r'Absolute value of the DTFT of a truncated exponential signal $e^{j \Omega_0 k}$ with $\Omega_0=$%2.2f' %Om0)
plt.xlabel(r'$\Omega$')
plt.ylabel(r'$|X_N(e^{j \Omega})|$')
plt.axis([-np.pi, np.pi, -0.5, N+5])
plt.grid()"
"N = 32  # length of the signal
P = 10.33  # periodicity of the exponential signal
Om0 = P * (2*np.pi/N)  # frequency of exponential signal


# truncated exponential signal
k = np.arange(N)
x = np.exp(1j*Om0*k)

# DTFT of finite length exponential signal (analytic)
Om = np.linspace(0, 2*np.pi, num=1024)
Xw = np.exp(-1j*(Om-Om0)*(N-1)/2)*(np.sin(N*(Om-Om0)/2))/(np.sin((Om-Om0)/2))

# DFT of the exponential signal by FFT
X = np.fft.fft(x)
mu = np.arange(N) * 2*np.pi/N

# plot spectra
plt.figure(figsize = (10, 8))
ax1 = plt.gca()

plt.plot(Om, abs(Xw), 'r', label=r'$|X_N(e^{j \Omega})|$')
plt.stem(mu, abs(X), label=r'$|X_N[\mu]|$', basefmt=' ')
plt.ylim([-0.5, N+5]);
plt.title(r'Absolute value of the DTFT/DFT of a truncated exponential signal $e^{j \Omega_0 k}$ with $\Omega_0=$%2.2f' %Om0, y=1.08)
plt.legend()

ax1.set_xlabel(r'$\Omega$')
ax1.set_xlim([Om[0], Om[-1]])
ax1.grid()

ax2 = ax1.twiny()
ax2.set_xlim([0, N])
ax2.set_xlabel(r'$\mu$', color='C0')
ax2.tick_params('x', colors='C0')"
"dft_signal_mixture(32, 1, 10.3, 1, 15.2)"
"dft_signal_mixture(32, 1, 10.3, 1, 10.9)"
"dft_signal_mixture(32, 1, 10.3, 0.1, 15.2)"
"import warnings
warnings.filterwarnings('ignore')
import numpy as np
import matplotlib.pylab as plt
from notes_utilities import pnorm_ball_points

Steps = 16
th = 2*np.pi/Steps
R = np.mat([[np.cos(th),np.sin(th)],[-np.sin(th),np.cos(th)]])

A = 0.5*np.mat([[1,0.99],[0.99, 2]])
s0 = np.mat(np.random.randn(2,1))
s0 = s0/np.linalg.norm(s0)

dx,dy = pnorm_ball_points(A=np.linalg.cholesky(A))
ln_e = plt.Line2D(dx, dy)

plt.figure(figsize=(6,6))
ax = plt.gca()

ax.set_ylim([-4,4])
ax.set_xlim([-4,4])


for i in range(Steps):
    s0 = R*s0
    g = np.mat(np.random.rand(2,1))
    c = s0.T*A*g/(s0.T*A*s0)
    s1 = -g + s0*c

    s1 = s1/np.linalg.norm(s1)

    ln1 = plt.Line2D([0,s0[0]],[0,s0[1]],color='r')
    ln2 = plt.Line2D([0,s1[0]],[0,s1[1]],color='b')

    #ax.add_line(ln1)
    ax.add_line(ln2)
    ax.add_line(ln_e)
    
plt.show()


"
"import numpy as np
np.set_printoptions(precision=5)
# Generate a random problem
N = 30

A = np.matrix(np.random.randn(N,5))
A = A*A.T + 0.01*np.eye(N)
b = np.matrix(np.random.randn(N,1))

#x_true = A\b;
x_true = np.linalg.solve(A,b)

print(np.c_[A*x_true,b])"
"np.set_printoptions(precision=3)
# Conjugate Gradients

# random start
x = np.random.randn(N,1)
s_past = np.zeros((N,1))
gt_g_past = 1 # avoid NaN

for t in range(N-1):
    # Gradient
    g = A*x - b;
    
    # Search direction
    gt_g = g.T*g
    c = gt_g/gt_g_past
    s = -g + s_past*c
    
    # Stepsize
    gam = - s.T*g/(s.T*A*s)
    
    # Update
    x = x + s*gam
    
    print(np.c_[x, x_true])
    print(10*'*')
    s_past = s
    gt_g_past = gt_g
"
"import warnings
warnings.filterwarnings('ignore')
%matplotlib inline

import matplotlib as mpl
import matplotlib.pyplot as plt
import numpy as np

M = 5
N = 6
K = 2 

W_true = np.random.randn(M,K)
H_true = np.random.randn(K,N)

X = W_true.dot(H_true)
X = X+0.05*np.random.randn(M,N)

p_on = 0.6
Mask = (np.random.rand(M,N)<p_on)



W = np.random.randn(M,K)
H = np.random.randn(K,N)

EPOCH = 2000

eta = 0.05

for i in range(EPOCH):
    dW = -(Mask*(X-W.dot(H))).dot(H.T)
    W = W - eta*dW
    dH = -W.T.dot((Mask*(X-W.dot(H))))
    H = H - eta*dH

    if (i%100 == 0):
        print(0.5*np.sum((Mask*(X-W.dot(H)))**2))


plt.imshow(Mask, interpolation='nearest',cmap=plt.cm.gray_r)
plt.title('Mask')
plt.show()

MX = X.copy()
MX[Mask==0] = np.nan

plt.imshow(MX, interpolation='nearest')
plt.title('Observed Data')
plt.show()
plt.imshow(W.dot(H), interpolation='nearest')
plt.title('Approximation')
plt.show()
plt.imshow(X, interpolation='nearest')
plt.title('True')
plt.show()

"
"import scipy.sparse as sp

m = sp.coo.coo_matrix(Mask)

I,J = m.nonzero()

for i,j in zip(I,J):
    print('[%d,%d,%2.3f],' % (i, j, X[i,j]))
    
print('---')
m2 = sp.coo.coo_matrix(1-Mask)

I,J = m2.nonzero()

for i,j in zip(I,J):
    print('[%d,%d, %2.2f],' % (i, j, X[i,j]))"
"import warnings
warnings.filterwarnings('ignore')
%matplotlib inline
import numpy as np
import matplotlib.pylab as plt 

A = 0.9
C = 1.0

R = 0.01
Q = 0.05

x0 = 0

T = 100

x = np.zeros((T))
y = np.zeros((T))

for t in range(T):
    if t==0:
        x[0] = x0
    else:
        x[t] = A*x[t-1] + np.sqrt(Q)*np.random.randn(1)

    y[t] = C*x[t] + np.sqrt(R)*np.random.randn(1)


    
plt.plot(y,'r')
plt.plot(x,'b')
"
"
m = np.zeros_like(x)
Sig = np.zeros_like(x)

m0 = 0
Sg0 = 1

for t in range(T):
    if t==0:
        m_pred = A*m0
        SG_pred = A*Sg0*A + Q
    else:
        m_pred = A*m[t-1]
        SG_pred = A*Sig[t-1]*A + Q
    

    Gt = SG_pred*C/(C*SG_pred*C+R)
    m[t] = m_pred + Gt*(y[t]-C*m_pred)
    Sig[t] = SG_pred - Gt*C*SG_pred
    

plt.plot(x,'.r')
plt.plot(m,'ob')
"
"%connect_info
"
"import numpy as np

N = 2
M = 3

A = np.matrix(np.ceil(5*np.random.randn(N,N)))
C = np.matrix(np.ceil(5*np.random.randn(M,N)))
R = np.matrix(2*np.eye(M))
Q = np.matrix(0.1*np.eye(N))

mu = np.matrix(10*np.ones((N,1)))
P = np.matrix(100*np.eye(N))

print('mu=\n',mu)
print('P=\n',P)

print('A=\n',A)
print('C=\n',C)
print('Q=\n',Q)
print('R=\n',R)

T = 3;

y = np.matrix(np.ceil(5*np.random.randn(M,T)))

print('observations=\n',y)"
"
m_pred = mu
Sig_pred = P
l = 0

for t in range(T):
    S = C*Sig_pred*C.T + R 
    G = Sig_pred*C.T*(S.I)
    e = y[:,t] - C*m_pred
    m = m_pred + G*e
    Sig = Sig_pred - G*C*Sig_pred
    l = l - 0.5*np.log(np.linalg.det(2*np.pi*S)) - 0.5*e.T*S.I*e
    
    print('m_{}'.format(t+1), '=\n', m)
    print('Sig_{}'.format(t+1), '=\n',Sig)
    print('l_{}'.format(t+1), '=\n',l)
    print('\n')
    
    Sig_pred = A*Sig*A.T + Q
    m_pred = A*m
"
"m_pred = mu
Sig_pred = P
l = 0

for t in range(T):
    W = Sig_pred
    v = m_pred
    lam = l
    for i in range(M):
        S = C[i,:]*W*C[i,:].T + R[i,i]
        G = W*C[i,:].T/S
        e = y[i,t] - C[i,:]*v
        v = v + G*e
        W = W - G*C[i,:]*W
        lam = lam - 0.5*np.log(2*np.pi*S) - e*e/(2*S)
    Sig = W
    m = v
    l = lam
    
    print('m_{}'.format(t+1), '=\n', m)
    print('Sig_{}'.format(t+1), '=\n',Sig)
    print('l_{}'.format(t+1), '=\n',l)
    print('\n')
    
    Sig_pred = A*Sig*A.T + Q
    m_pred = A*m"
"import warnings
warnings.filterwarnings('ignore')
%matplotlib inline
import scipy as sc
import numpy as np

import scipy.linalg as la
import matplotlib.pyplot as plt

A = np.mat('[0,1;-1,0]')

dt = 0.05
T = 100

z = np.mat(np.zeros((2,T)))
H = la.expm(dt*A)

z[:,0] = np.mat('[2.4;0]')
for i in range(1,T):
    z[:,i] = H*z[:,i-1]

    
plt.plot(z[0,:], z[1,:],'.-r')
ax = plt.gcf().gca()
ax.set_aspect('equal')
plt.show()"
"epsilon = 0.05
T = 1000
z_euler = np.mat(np.zeros((2,T)))

def dHdx(x):
    return x

def dHdp(p):
    return p

z_euler[:,0] = np.mat('[2.4;0]')
for i in range(1,T):
    #z_euler[:,i] = z_euler[:,i-1] + epsilon*dH(z_euler[:,i-1])
    
    z_euler[0,i] = z_euler[0,i-1] + epsilon*dHdp(z_euler[1,i-1])
    z_euler[1,i] = z_euler[1,i-1] - epsilon*dHdx(z_euler[0,i-1])

    
plt.plot(z_euler[0,:], z_euler[1,:],'.-r')
ax = plt.gcf().gca()
ax.set_aspect('equal')
plt.show()"
"epsilon = 0.01
T = 600
z_euler2 = np.mat(np.zeros((2,T)))

def dHdx(x):
    return x

def dHdp(p):
    return p

z_euler2[:,0] = np.mat('[2.4;0]')
for i in range(1,T):    
    z_euler2[0,i] = z_euler2[0,i-1] + epsilon*dHdp(z_euler2[1,i-1])
    z_euler2[1,i] = z_euler2[1,i-1] - epsilon*dHdx(z_euler2[0,i])

    
plt.plot(z_euler2[0,:], z_euler2[1,:],'.-r')
ax = plt.gcf().gca()
ax.set_aspect('equal')
plt.show()"
"epsilon = 0.5
T = 50

def dHdx(x):
    return x

def dHdp(p):
    return p


#def dHdx(x):
#    A = np.mat('[1;-1]')
#    b = np.mat('[1;3]')
#    u = (b - A*x)
#    if np.all(u > 0):
#        g = A.T*(1/u)
#    else:
#        g = np.inf*u
#    return g[0,0]

T = 100
z_lf = np.mat(np.zeros((2,T)))

z_lf[:,0] = np.mat('[0.1;0]')
for i in range(1,T):
    p_mid = z_lf[1,i-1] - (epsilon/2)*dHdx(z_lf[0,i-1])
    z_lf[0,i] = z_lf[0,i-1] + epsilon*dHdp(p_mid)
    z_lf[1,i] = p_mid - (epsilon/2)*dHdx(z_lf[0,i])

    
plt.plot(z_lf[0,:].T, z_lf[1,:].T,'or-')
ax = plt.gcf().gca()
ax.set_aspect('equal')
plt.show()"
"A = np.hstack([
        np.matrix(df.sl).T, 
        np.matrix(df.sw).T, 
        np.matrix(df.pl).T, 
        np.matrix(df.pw).T])
A[:5] # sample view"
"## Use a heap to store the smallest items

import heapq as hp

# Define an object and overload custom comparison operators
class tup:
    def __init__(self, val, idx):
        self.val = val
        self.idx = idx
        
    def __lt__(self, other):
        '''Redefine for max-heap'''
        return self.val > other.val
    
    def __le__(self, other):
        return self.val <= other.val
 
    def __eq__(self, other):
        return self.val == other.val
    
    def __ne__(self, other):
        return self.val != other.val

    def __gt__(self, other):
        return self.val > other.val

    def __ge__(self, other):
        return self.val >= other.val

    def __str__(self):
        return '{:.3},{:d}'.format(self.val,self.idx)
    
        
## Search for the nearest K   
K = 20
heap = []
N = A.shape[0]   

for j in range(N):
    heap = []
    ## Fill in the heap with dummy nodes
    for k in range(K):
        hp.heappush(heap, tup(np.inf, -1))

    for i in range(N):
        if i != j:
            e = A[i,:]- A[j,:]
            e = e.reshape((4,1))
            tp = tup(float(e.T*e), i)
            if tp <= heap[0]:
                hp.heapreplace(heap, tp)

    for t in range(len(heap)):
        h = hp.heappop(heap)
        print(h, float(c[h.idx]))
    print('--'*5)
"
"amplitude_slider = FloatSlider(value=2.0, min=0, max=3.0, step=.05)
phase_slider = FloatSlider(value=0.0, min=-np.pi, max=np.pi, step=.05)

interactive(simple_example,
            amplitude=amplitude_slider,
            phase=phase_slider
           )"
"def spline_demo(num=14, smooth=0, seed=10, brush_strokes=30, alpha=0.5):
    a = np.random.RandomState(seed=seed)
    x = a.rand(num)
    y = a.rand(num)
    t = np.arange(0, 1.1, .1)
    plt.rcParams['figure.figsize'] = 8,8
    plt.figure()
    for _ in range(brush_strokes):
        tck, u = interpolate.splprep([x+a.rand(num)/10.0,y+a.rand(num)/10.0], s=smooth)
        unew = np.arange(0, 1.01, 0.001)
        out = interpolate.splev(unew, tck)
        plt.plot(out[0], out[1], alpha=alpha, c='black', linewidth=3.0)
    plt.xlim(-1.5, 2.5)
    plt.ylim(-1.5, 2.5)
    plt.axis('off')

smooth_slider = FloatSlider(value=0, min=0, max=20.0, step=.1)
num_points_slider = IntSlider(value=8, min=4, max=20)
seed_slider = IntSlider(value=4, min=4, max=20)
brush_slider = IntSlider(value=1, min=1, max=20)
alpha_slider = FloatSlider(value=.5, min=0, max=1.0, step=.05)

w=interactive(spline_demo,
              smooth=smooth_slider, 
              num=num_points_slider, 
              seed=seed_slider, 
              brush_strokes=brush_slider,
              alpha=alpha_slider)
w"
"N = 31

matplotlib.rcParams.update({'font.size': 18})
fig,axes = plt.subplots( 4, 2, figsize=(16,16) )

kernel = lpKernel(N, 0.4)
amp2D = np.fft.fftshift(np.abs(np.fft.rfft2(kernel)),axes=0)
axes[0,0].plot(np.linspace(-N,N,2*N+1), kernel[:,N])
axes[0,0].set_xlabel('Sample')
axes[0,0].set_ylabel('Amplitude')
axes[0,0].set_title('Lowpass - Impulse Response')
axes[0,1].plot(np.linspace(0,1,N+1),amp2D[N,:])
axes[0,1].set_xlabel('Normalised Frequency')
axes[0,1].set_ylabel('Amplitude')
axes[0,1].set_title('Lowpass - Amplitude Spectrum')

kernel = hpKernel(N, 0.4)
amp2D = np.fft.fftshift(np.abs(np.fft.rfft2(kernel)),axes=0)
axes[1,0].plot(np.linspace(-N,N,2*N+1), kernel[:,N])
axes[1,0].set_xlabel('Sample')
axes[1,0].set_ylabel('Amplitude')
axes[1,0].set_title('Highpass - Impulse Response')
axes[1,1].plot(np.linspace(0,1,N+1),amp2D[N,:])
axes[1,1].set_xlabel('Normalised Frequency')
axes[1,1].set_ylabel('Amplitude')
axes[1,1].set_title('Highpass - Amplitude Spectrum')

kernel = bpKernel(N, 0.4)
amp2D = np.fft.fftshift(np.abs(np.fft.rfft2(kernel)),axes=0)
axes[2,0].plot(np.linspace(-N,N,2*N+1), kernel[:,N])
axes[2,0].set_xlabel('Sample')
axes[2,0].set_ylabel('Amplitude')
axes[2,0].set_title('Bandpass - Impulse Response')
axes[2,1].plot(np.linspace(0,1,N+1),amp2D[N,:])
axes[2,1].set_xlabel('Normalised Frequency')
axes[2,1].set_ylabel('Amplitude')
axes[2,1].set_title('Bandpass - Amplitude Spectrum')

kernel = brKernel(N, 0.4)
amp2D = np.fft.fftshift(np.abs(np.fft.rfft2(kernel)),axes=0)
axes[3,0].plot(np.linspace(-N,N,2*N+1), kernel[:,N])
axes[3,0].set_xlabel('Sample')
axes[3,0].set_ylabel('Amplitude')
axes[3,0].set_title('Bandreject - Impulse Response')
axes[3,1].plot(np.linspace(0,1,N+1),amp2D[N,:])
axes[3,1].set_xlabel('Normalised Frequency')
axes[3,1].set_ylabel('Amplitude')
axes[3,1].set_title('Bandreject - Amplitude Spectrum')


fig.tight_layout()"
"import warnings
warnings.filterwarnings('ignore')
import numpy as np
import scipy.signal as sig
import matplotlib.pyplot as plt
from numba import jit
%pylab inline

def make_signal(nsamp):
    ref = np.random.rand(nsamp)
    wav = sig.ricker(80,5)
    filtered = np.convolve(ref, wav,'same')
    return filtered
    
res = make_signal(1000)
fig = plt.figure(figsize=(12,1))
plt.plot(res)
"
"
def response(inp, outp):
    tmp = np.asarray(np.where(inp<0)[0])
    inegs = np.split(tmp, np.where(np.diff(tmp) != 1)[0]+1)
    tmp = np.asarray(np.where(inp>0)[0])
    ipos = np.split(tmp, np.where(np.diff(tmp) != 1)[0]+1)
    for i in range(np.shape(inegs)[0]):
        outp[inegs[i]] = np.min(inp[inegs[i]])
    for i in range(np.shape(ipos)[0]):
        outp[ipos[i]] = np.max(inp[ipos[i]])
    
outp = np.zeros(res.shape)
response(res,outp)
fig = plt.figure(figsize=(12,1))
plt.plot(res)
plt.plot(outp)"
"@jit(nopython=True)
def response_numba(inp, outp):
    ns = inp.shape[0]
    start = 0
    pos = inp[0]>0
    for i in range(ns):
        if inp[i]<0 and pos:
            if start<i-1:
                outp[start:i-1] = np.max(inp[start:i-1])
            else:
                outp[start] = inp[start]
            pos = False
            start = i
        if inp[i]>0 and not pos:
            if start<i-1:
                outp[start:i-1] = np.min(inp[start:i-1])
            else:
                outp[start] = inp[start]
            pos=True
            start = i
    if pos:
        if start<ns-1:
            outp[start:ns] = np.max(inp[start:ns])
        else:
            outp[start] = inp[start]
    else:
        if start<ns-1:
            outp[start:ns] = np.min(inp[start:ns])
        else:
            outp[start] = inp[start]
        
            
outp = np.zeros(res.shape)
response_numba(res,outp)
fig = plt.figure(figsize=(12,1))
plt.plot(res)
plt.plot(outp)         "
"%timeit -o response(res,outp)"
"%timeit -o response_numba(res,outp)"
"from pwc_tvdip import pwc_tvdip
outp = pwc_tvdip(res,[0.4],0)[:,0]
fig = plt.figure(figsize=(12,1))
plt.plot(res)
plt.plot(outp)   "
"from pwc_bilateral import pwc_bilateral
outp = pwc_bilateral(res,beta=100.0,display=False)
fig = plt.figure(figsize=(12,1))
plt.plot(res)
plt.plot(outp)"
"import warnings
warnings.filterwarnings('ignore')
import numpy as np
import scipy.signal as sig
import matplotlib.pyplot as plt
from numba import jit
%pylab inline

def make_signal(nsamp):
    ref = np.random.rand(nsamp)
    wav = sig.ricker(80,5)
    filtered = np.convolve(ref, wav,'same')
    return filtered
    
res = make_signal(1000)
fig = plt.figure(figsize=(12,1))
plt.plot(res)
"
"
def response(inp, outp):
    tmp = np.asarray(np.where(inp<0)[0])
    inegs = np.split(tmp, np.where(np.diff(tmp) != 1)[0]+1)
    tmp = np.asarray(np.where(inp>0)[0])
    ipos = np.split(tmp, np.where(np.diff(tmp) != 1)[0]+1)
    for i in range(np.shape(inegs)[0]):
        outp[inegs[i]] = np.min(inp[inegs[i]])
    for i in range(np.shape(ipos)[0]):
        outp[ipos[i]] = np.max(inp[ipos[i]])
    
outp = np.zeros(res.shape)
response(res,outp)
fig = plt.figure(figsize=(12,1))
plt.plot(res)
plt.plot(outp)"
"@jit(nopython=True)
def response_numba(inp, outp):
    ns = inp.shape[0]
    start = 0
    pos = inp[0]>0
    for i in range(ns):
        if inp[i]<0 and pos:
            if start<i-1:
                outp[start:i-1] = np.max(inp[start:i-1])
            else:
                outp[start] = inp[start]
            pos = False
            start = i
        if inp[i]>0 and not pos:
            if start<i-1:
                outp[start:i-1] = np.min(inp[start:i-1])
            else:
                outp[start] = inp[start]
            pos=True
            start = i
    if pos:
        if start<ns-1:
            outp[start:ns] = np.max(inp[start:ns])
        else:
            outp[start] = inp[start]
    else:
        if start<ns-1:
            outp[start:ns] = np.min(inp[start:ns])
        else:
            outp[start] = inp[start]
        
            
outp = np.zeros(res.shape)
response_numba(res,outp)
fig = plt.figure(figsize=(12,1))
plt.plot(res)
plt.plot(outp)         "
"%timeit -o response(res,outp)"
"%timeit -o response_numba(res,outp)"
"from pwc_tvdip import pwc_tvdip
outp = pwc_tvdip(res,[0.4],0)[:,0]
fig = plt.figure(figsize=(12,1))
plt.plot(res)
plt.plot(outp)   "
"from pwc_bilateral import pwc_bilateral
outp = pwc_bilateral(res,beta=100.0,display=False)
fig = plt.figure(figsize=(12,1))
plt.plot(res)
plt.plot(outp)"
"import warnings
warnings.filterwarnings('ignore')
import numpy as np
import scipy.signal as sig
import matplotlib.pyplot as plt
from numba import jit
%pylab inline

def make_signal(nsamp):
    ref = np.random.rand(nsamp)
    wav = sig.ricker(80,5)
    filtered = np.convolve(ref, wav,'same')
    return filtered
    
res = make_signal(1000)
fig = plt.figure(figsize=(12,1))
plt.plot(res)
"
"
def response(inp, outp):
    tmp = np.asarray(np.where(inp<0)[0])
    inegs = np.split(tmp, np.where(np.diff(tmp) != 1)[0]+1)
    tmp = np.asarray(np.where(inp>0)[0])
    ipos = np.split(tmp, np.where(np.diff(tmp) != 1)[0]+1)
    for i in range(np.shape(inegs)[0]):
        outp[inegs[i]] = np.min(inp[inegs[i]])
    for i in range(np.shape(ipos)[0]):
        outp[ipos[i]] = np.max(inp[ipos[i]])
    
outp = np.zeros(res.shape)
response(res,outp)
fig = plt.figure(figsize=(12,1))
plt.plot(res)
plt.plot(outp)"
"@jit(nopython=True)
def response_numba(inp, outp):
    ns = inp.shape[0]
    start = 0
    pos = inp[0]>0
    for i in range(ns):
        if inp[i]<0 and pos:
            if start<i-1:
                outp[start:i-1] = np.max(inp[start:i-1])
            else:
                outp[start] = inp[start]
            pos = False
            start = i
        if inp[i]>0 and not pos:
            if start<i-1:
                outp[start:i-1] = np.min(inp[start:i-1])
            else:
                outp[start] = inp[start]
            pos=True
            start = i
    if pos:
        if start<ns-1:
            outp[start:ns] = np.max(inp[start:ns])
        else:
            outp[start] = inp[start]
    else:
        if start<ns-1:
            outp[start:ns] = np.min(inp[start:ns])
        else:
            outp[start] = inp[start]
        
            
outp = np.zeros(res.shape)
response_numba(res,outp)
fig = plt.figure(figsize=(12,1))
plt.plot(res)
plt.plot(outp)         "
"%timeit -o response(res,outp)"
"%timeit -o response_numba(res,outp)"
"from pwc_tvdip import pwc_tvdip
outp = pwc_tvdip(res,[0.4],0)[:,0]
fig = plt.figure(figsize=(12,1))
plt.plot(res)
plt.plot(outp)   "
"from pwc_bilateral import pwc_bilateral
outp = pwc_bilateral(res,beta=100.0,display=False)
fig = plt.figure(figsize=(12,1))
plt.plot(res)
plt.plot(outp)"
"import warnings
warnings.filterwarnings('ignore')
import numpy as np
import scipy.signal as sig
import matplotlib.pyplot as plt
from numba import jit
%pylab inline

def make_signal(nsamp):
    ref = np.random.rand(nsamp)
    wav = sig.ricker(80,5)
    filtered = np.convolve(ref, wav,'same')
    return filtered
    
res = make_signal(1000)
fig = plt.figure(figsize=(12,1))
plt.plot(res)
"
"
def response(inp, outp):
    tmp = np.asarray(np.where(inp<0)[0])
    inegs = np.split(tmp, np.where(np.diff(tmp) != 1)[0]+1)
    tmp = np.asarray(np.where(inp>0)[0])
    ipos = np.split(tmp, np.where(np.diff(tmp) != 1)[0]+1)
    for i in range(np.shape(inegs)[0]):
        outp[inegs[i]] = np.min(inp[inegs[i]])
    for i in range(np.shape(ipos)[0]):
        outp[ipos[i]] = np.max(inp[ipos[i]])
    
outp = np.zeros(res.shape)
response(res,outp)
fig = plt.figure(figsize=(12,1))
plt.plot(res)
plt.plot(outp)"
"@jit(nopython=True)
def response_numba(inp, outp):
    ns = inp.shape[0]
    start = 0
    pos = inp[0]>0
    for i in range(ns):
        if inp[i]<0 and pos:
            if start<i-1:
                outp[start:i-1] = np.max(inp[start:i-1])
            else:
                outp[start] = inp[start]
            pos = False
            start = i
        if inp[i]>0 and not pos:
            if start<i-1:
                outp[start:i-1] = np.min(inp[start:i-1])
            else:
                outp[start] = inp[start]
            pos=True
            start = i
    if pos:
        if start<ns-1:
            outp[start:ns] = np.max(inp[start:ns])
        else:
            outp[start] = inp[start]
    else:
        if start<ns-1:
            outp[start:ns] = np.min(inp[start:ns])
        else:
            outp[start] = inp[start]
        
            
outp = np.zeros(res.shape)
response_numba(res,outp)
fig = plt.figure(figsize=(12,1))
plt.plot(res)
plt.plot(outp)         "
"%timeit -o response(res,outp)"
"%timeit -o response_numba(res,outp)"
"from pwc_tvdip import pwc_tvdip
outp = pwc_tvdip(res,[0.4],0)[:,0]
fig = plt.figure(figsize=(12,1))
plt.plot(res)
plt.plot(outp)   "
"from pwc_bilateral import pwc_bilateral
outp = pwc_bilateral(res,beta=100.0,display=False)
fig = plt.figure(figsize=(12,1))
plt.plot(res)
plt.plot(outp)"
"print_kwargs(wine=""merlot"",entree=""mutton"",dessert=""macaroon"")"
"import warnings
warnings.filterwarnings('ignore')
#Jasdeep Nijjar 100493157

import numpy as np
import matplotlib.pyplot as plt

def count (arr):
    count = 0
    for c in arr:
        count += 1
    return count
    
# Use logical indexing to identify all the entries of x and y that satisfy the inequalities x < −2 and
# −5 ≤ y ≤ 0 simultaneously. Draw blue, filled circles for every point identified in that region

xdata, ydata = np.loadtxt(""xy_points.dat"")
# print (xdata)
# print (ydata)


print ('x before restrictions ', count(xdata))

print ('y before restrictions ', count(ydata))

x = ([xdata < -2]) #every x less then 2

print ('x after restrictions ', count(xdata[x]))

y1 = [-5 <= ydata]
y2 = [(ydata <= 0)]
count2 = 0
if (np.isnan(y2).all()):
    count2 = count2+1

print ('y after restrictions ',count(ydata[y2]), count2)
print (count(xdata[x])) #the numbers we dont want to plot

# print ('y data', y2)
# print (x[0][1])
# print (y2[0][1])
# print (xdata[x])
# print (ydata[y2])

print(x)
print('\n\n\n\n')
print (xdata)

print('\n\n\n\n')
# if np.logical_and(xdata[x], ydata[y2]):
#     print ('test')

# plt.scatter(xdata[x], ydata[y2], color='blue')

# # plt.scatter(x, y)



plt.show()"
"import warnings
warnings.filterwarnings('ignore')
import matplotlib.pyplot as plt
sizes = [46,38,29,24,13,11,11,8,8,7,107]
labels = (""USA"", ""CHN"", ""GBR"", ""RUS"", ""KOR"", ""GER"", ""FRA"", ""ITA"", ""HUN""
          , ""AUS"" , ""OTHER"")
colors = ['blue', 'red', 'yellow', 'purple', 'brown', 'orange', 'gold'
          , 'green' , 'maroon', 'teal', 'navy']
plt.pie(sizes, labels=labels, colors=colors, autopct='%1.1f%%', shadow=True
        , startangle=90)
plt.title(""2012 Olympic Gold Medals (100486809)"")
plt.show()"
"import warnings
warnings.filterwarnings('ignore')
#import numpy for use in functions
import numpy as np
def Pn(n):
    #make a np array a, that stores float
    a = np.array([],float)
    for x in range (1, n+1):
        y = ((-1)**(x+1))/(2*x-1)
        #add each new number to the array using concat
        a = np.concatenate((a,[y]))
    #return the final result for display purposes
    return 4*(np.sum(a))
def Tn(n):
    a = np.array([],float)
    for x in range (1, n+1):
        y = ((1)/(x*x))
        a = np.concatenate((a,[y]))
    return (6*(np.sum(a)))**(1/2)
def Un(n):
    a = np.array([],float)
    for x in range (1, n+1):
        y = ((1)/(x**4))
        a = np.concatenate((a,[y]))
    return (90*(np.sum(a)))**(1/4)
"
"import matplotlib.pyplot as plt

%matplotlib inline

np.random.seed(1)
c = np.random.random(2)
A = np.random.random((4, 2))
b = np.random.random(4)

A = np.vstack((A, np.array([1, 0])))
A = np.vstack((A, np.array([-1, 0])))
A = np.vstack((A, np.array([0, -1])))
A = np.vstack((A, np.array([0, 1])))

b = np.hstack((b, np.array([1])))
b = np.hstack((b, np.array([1])))
b = np.hstack((b, np.array([1])))
b = np.hstack((b, np.array([1])))

traj_x1 = []
traj_x2 = []

def track(x):
    traj_x1.append(x[0])
    traj_x2.append(x[1])

solver = IPSolver()
X = solver.solve(c, A, b, track)

plt.plot(traj_x1, traj_x2, 'ro')

x1, x2 = np.mgrid[-1:1:0.05, -1:1:0.05]
x1 = x1.reshape(-1)
x2 = x2.reshape(-1)

X1 = []
X2 = []

for i in range(x1.shape[0]):
    constr = A.dot(np.array([x1[i], x2[i]])) - b
    if constr[constr > 0].any():
        continue
    else:
        X1.append(x1[i])
        X2.append(x2[i])

plt.scatter(X1, X2)"
"import matplotlib.pyplot as plt

%matplotlib inline

np.random.seed(2)
c = np.random.random(2)
A = np.random.random((4, 2))
b = np.random.random(4)

A = np.vstack((A, np.array([1, 0])))
A = np.vstack((A, np.array([-1, 0])))
A = np.vstack((A, np.array([0, -1])))
A = np.vstack((A, np.array([0, 1])))

b = np.hstack((b, np.array([1])))
b = np.hstack((b, np.array([1])))
b = np.hstack((b, np.array([1])))
b = np.hstack((b, np.array([1])))

traj_x1 = []
traj_x2 = []

def track(x):
    traj_x1.append(x[0])
    traj_x2.append(x[1])

solver = IPSolver()
X = solver.solve(c, A, b, track)

plt.plot(traj_x1, traj_x2, 'ro')

x1, x2 = np.mgrid[-1:1:0.05, -1:1:0.05]
x1 = x1.reshape(-1)
x2 = x2.reshape(-1)

X1 = []
X2 = []

for i in range(x1.shape[0]):
    constr = A.dot(np.array([x1[i], x2[i]])) - b
    if constr[constr > 0].any():
        continue
    else:
        X1.append(x1[i])
        X2.append(x2[i])

plt.scatter(X1, X2)"
A
"import matplotlib.pyplot as plt

%matplotlib inline

np.random.seed(42)
c = np.random.random(2)
A = np.random.random((8, 2)) - 0.5
b = np.random.random(8)

A = np.vstack((A, np.array([1, 0])))
A = np.vstack((A, np.array([-1, 0])))
A = np.vstack((A, np.array([0, -1])))
A = np.vstack((A, np.array([0, 1])))

b = np.hstack((b, np.array([1])))
b = np.hstack((b, np.array([1])))
b = np.hstack((b, np.array([1])))
b = np.hstack((b, np.array([1])))

traj_x1 = []
traj_x2 = []

def track(x):
    traj_x1.append(x[0])
    traj_x2.append(x[1])

# solver = IPSolver()
# X = solver.solve(c, A, b, track)

# plt.plot(traj_x1, traj_x2, 'ro')

x1, x2 = np.mgrid[-1:1:0.05, -1:1:0.05]
x1 = x1.reshape(-1)
x2 = x2.reshape(-1)

X1 = []
X2 = []

for i in range(x1.shape[0]):
    constr = A.dot(np.array([x1[i], x2[i]])) - b
    if constr[constr > 0].any():
        continue
    else:
        X1.append(x1[i])
        X2.append(x2[i])

plt.scatter(X1, X2)"
"import warnings
warnings.filterwarnings('ignore')
%matplotlib inline
import matplotlib.pyplot as plt
import numpy as np
import random
from IPython import display
from sklearn import datasets, preprocessing

(X, y) = datasets.make_circles(n_samples=1024, shuffle=True, noise=0.2, factor=0.4)
ind = np.logical_or(y==1, X[:,1] > X[:,0] - 0.5)
X = X[ind,:]
m = np.array([[1, 1], [-2, 1]])
X = preprocessing.scale(X)
y = y[ind]
y = 2*y - 1
plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.Paired)
plt.show()"
"w = np.array([1,0,0,0,0,0])

alpha = 0.1 # learning rate

n_iter = 50
batch_size = 4
loss = np.zeros(n_iter)
plt.figure(figsize=(12,5))
for i in range(n_iter):
    ind = random.sample(range(X.shape[0]), batch_size)
    loss[i] = compute_loss(X, y, w)
    visualize(X[ind,:], y[ind], w, loss, n_iter)
    
    w = w - alpha * compute_grad(X[ind,:], y[ind], w)

visualize(X, y, w, loss, n_iter)
plt.clf()"
"w = np.array([1,0,0,0,0,0])

alpha = 0.5 # learning rate

n_iter = 50
batch_size = 4
loss = np.zeros(n_iter)
plt.figure(figsize=(12,5))
for i in range(n_iter):
    ind = random.sample(range(X.shape[0]), batch_size)
    loss[i] = compute_loss(X, y, w)
    visualize(X[ind,:], y[ind], w, loss, n_iter)
    
    w = w - alpha * compute_grad(X[ind,:], y[ind], w)

visualize(X, y, w, loss, n_iter)
plt.clf()"
"w = np.array([1,0,0,0,0,0])

alpha = 0.05 # learning rate

n_iter = 50
batch_size = 4
loss = np.zeros(n_iter)
plt.figure(figsize=(12,5))
for i in range(n_iter):
    ind = random.sample(range(X.shape[0]), batch_size)
    loss[i] = compute_loss(X, y, w)
    visualize(X[ind,:], y[ind], w, loss, n_iter)
    
    w = w - alpha * compute_grad(X[ind,:], y[ind], w)

visualize(X, y, w, loss, n_iter)
plt.clf()"
"w = np.array([1,0,0,0,0,0])
v = w.copy()

alpha = 0.05 # learning rate
mu    = 0.5 # momentum

n_iter = 50
batch_size = 4
loss = np.zeros(n_iter)
plt.figure(figsize=(12,5))

for i in range(n_iter):
    ind = random.sample(range(X.shape[0]), batch_size)
    loss[i] = compute_loss(X, y, w)
    visualize(X[ind,:], y[ind], w, loss, n_iter)
    
    v = mu * v - alpha * compute_grad(X[ind,:], y[ind], w)
    w = w + v

visualize(X, y, w, loss, n_iter)
plt.clf()"
"w = np.array([1,0,0,0,0,0])
v = w.copy()

alpha = 0.05 # learning rate
mu    = 0.9 # momentum

n_iter = 50
batch_size = 4
loss = np.zeros(n_iter)
plt.figure(figsize=(12,5))

for i in range(n_iter):
    ind = random.sample(range(X.shape[0]), batch_size)
    loss[i] = compute_loss(X, y, w)
    visualize(X[ind,:], y[ind], w, loss, n_iter)
    
    v = mu * v - alpha * compute_grad(X[ind,:], y[ind], w)
    w = w + v

visualize(X, y, w, loss, n_iter)
plt.clf()"
"w = np.array([1,0,0,0,0,0])
v = w.copy()

alpha = 0.1 # learning rate
mu    = 0.5 # momentum

n_iter = 50
batch_size = 4
loss = np.zeros(n_iter)
plt.figure(figsize=(12,5))

for i in range(n_iter):
    ind = random.sample(range(X.shape[0]), batch_size)
    loss[i] = compute_loss(X, y, w)
    visualize(X[ind,:], y[ind], w, loss, n_iter)
    
    v = mu * v - alpha * compute_grad(X[ind,:], y[ind], w)
    w = w + v

visualize(X, y, w, loss, n_iter)
plt.clf()"
"w = np.array([1,0,0,0,0,0])
v = w.copy()

alpha = 0.5 # learning rate
mu    = 0.5 # momentum

n_iter = 50
batch_size = 4
loss = np.zeros(n_iter)
plt.figure(figsize=(12,5))

for i in range(n_iter):
    ind = random.sample(range(X.shape[0]), batch_size)
    loss[i] = compute_loss(X, y, w)
    visualize(X[ind,:], y[ind], w, loss, n_iter)
    
    v = mu * v - alpha * compute_grad(X[ind,:], y[ind], w)
    w = w + v

visualize(X, y, w, loss, n_iter)
plt.clf()"
"w = np.array([1,0,0,0,0,0])
v = w.copy()

alpha = 0.9 # learning rate
mu    = 0.9 # momentum

n_iter = 50
batch_size = 4
loss = np.zeros(n_iter)
plt.figure(figsize=(12,5))

for i in range(n_iter):
    ind = random.sample(range(X.shape[0]), batch_size)
    loss[i] = compute_loss(X, y, w)
    visualize(X[ind,:], y[ind], w, loss, n_iter)
    
    v = mu * v - alpha * compute_grad(X[ind,:], y[ind], w)
    w = w + v

visualize(X, y, w, loss, n_iter)
plt.clf()"
"w = np.array([1,0,0,0,0,0])

alpha = 0.05 # learning rate
mu    = 0.5 # momentum

n_iter = 50
batch_size = 4
loss = np.zeros(n_iter)
plt.figure(figsize=(12,5))
for i in range(n_iter):
    ind = random.sample(range(X.shape[0]), batch_size)
    loss[i] = compute_loss(X, y, w)
    visualize(X[ind,:], y[ind], w, loss, n_iter)
    
    v = mu * v - alpha * compute_grad(X[ind,:], y[ind], w + mu * v)
    w = w + v

visualize(X, y, w, loss, n_iter)
plt.clf()"
"w = np.array([1,0,0,0,0,0])

alpha = 0.05 # learning rate
mu    = 0.9 # momentum

n_iter = 50
batch_size = 4
loss = np.zeros(n_iter)
plt.figure(figsize=(12,5))
for i in range(n_iter):
    ind = random.sample(range(X.shape[0]), batch_size)
    loss[i] = compute_loss(X, y, w)
    visualize(X[ind,:], y[ind], w, loss, n_iter)
    
    v = mu * v - alpha * compute_grad(X[ind,:], y[ind], w + mu * v)
    w = w + v

visualize(X, y, w, loss, n_iter)
plt.clf()"
"w = np.array([1,0,0,0,0,0])

alpha = 0.1 # learning rate
mu    = 0.9 # momentum

n_iter = 50
batch_size = 4
loss = np.zeros(n_iter)
plt.figure(figsize=(12,5))
for i in range(n_iter):
    ind = random.sample(range(X.shape[0]), batch_size)
    loss[i] = compute_loss(X, y, w)
    visualize(X[ind,:], y[ind], w, loss, n_iter)
    
    v = mu * v - alpha * compute_grad(X[ind,:], y[ind], w + mu * v)
    w = w + v

visualize(X, y, w, loss, n_iter)
plt.clf()"
"w = np.array([1,0,0,0,0,0])
v = w.copy()

alpha = 0.05 # learning rate
mu    = 0.1 # momentum

n_iter = 50
batch_size = 4
loss = np.zeros(n_iter)
plt.figure(figsize=(12,5))

for i in range(n_iter):
    ind = random.sample(range(X.shape[0]), batch_size)
    loss[i] = compute_loss(X, y, w)
    visualize(X[ind,:], y[ind], w, loss, n_iter)
    
    v = mu * v - alpha * compute_grad(X[ind,:], y[ind], w)
    w = w + v

visualize(X, y, w, loss, n_iter)
plt.clf()"
plt.imshow(plt.imread('./res/generic.png'))
plt.imshow(plt.imread('./res/kru.png'))
"plt.figure(figsize=(8,12))
plt.imshow(plt.imread('./res/fig23_4.png'))"
plt.imshow(plt.imread('./res/prim.png'))
"plt.figure(figsize=(8,12))
plt.imshow(plt.imread('./res/fig23_5.png'))"
plt.imshow(plt.imread('./res/bellman_ford.png'))
plt.imshow(plt.imread('./res/fig24_4.png'))
plt.imshow(plt.imread('./res/dag.png'))
plt.imshow(plt.imread('./res/fig24_5.png'))
plt.imshow(plt.imread('./res/dijkstra.png'))
plt.imshow(plt.imread('./res/fig24_6.png'))
plt.imshow(plt.imread('./res/inequ.png'))
plt.imshow(plt.imread('./res/fig24_8.png'))
"x = np.linspace(-1.5, 1.5, 1000)

y1 = 0.5 * x + 0.5
y2 = sp.special.expit(5 * x)

pd.DataFrame({'linear': y1, 'logistic regression': y2}).plot()"
"# 演示一轮损失函数和导数值
logit_loss_and_grad(w0, X, y)"
"# 调过数值寻优方法，求解得到w 

(w, loss, info) = sp.optimize.fmin_l_bfgs_b(logit_loss_and_grad, w0, args=(X, y))

w"
"# 预测准确度
accuracy_score(y, y_pred)"
"auc(y, y_pred_probability, reorder=True)"
"samples[""y"", ""target""].value_counts()"
"samples[""x""].describe()"
"# 右子集
x_r = split[""right_nodes""].loc[:, ""target""].value_counts()
x_r"
"calc_class_proportion(split[""right_nodes""])"
"misclassification_error(split[""left_nodes""])"
"binary_class[""misclass""] = binary_class.apply(lambda x: 1 - x.max(), axis=1)
binary_class.plot(x=""p"", y=""misclass"")"
"binary_class[""gini""] = (binary_class[""p""] * binary_class[""1-p""] * 2)
binary_class.plot(x=""p"", y=""gini"")"
"x = binary_class[[""p"", ""1-p""]]
binary_class[""cross_entropy""] = -(x * np.log(x)).sum(axis=1)
binary_class.plot(x=""p"", y=""cross_entropy"")"
"binary_class.plot(x=""p"", y=[""misclass"", ""gini"", ""cross_entropy""])"
"binary_class[""cross_entropy_scaled""] = binary_class[""cross_entropy""] / binary_class[""cross_entropy""].max() * 0.5
binary_class.plot(x=""p"", y=[""misclass"", ""gini"", ""cross_entropy_scaled""], ylim=[0,0.55])"
"find_best_threshold(samples, ""sepal_width"", gini_index)"
"find_best_threshold(samples, ""sepal_length"", gini_index)"
"best_split = find_best_split(samples, gini_index)
[best_split[x] for x in [""feature"", ""threshold""]]"
"show_image(""./res/ClassificationCriterion.jpg"", figsize=(8,15))"
"show_image(""./res/gradient_descent.jpg"", figsize=(12,8))"
"show_image(""./res/iterator.jpg"")"
"show_image(""./res/incr_opt.png"", figsize=(10,5))"
"show_image(""./res/approx.png"", figsize=(10,5))"
"show_image(""./res/model.png"", figsize=(10,5))"
"show_image(""./res/gbdt.png"", figsize=(10,5))"
"show_image(""./res/rho_m.png"")"
"N(g1s, subs={x:1, s:2, k:2})"
"import warnings
warnings.filterwarnings('ignore')
!dir data"
"import warnings
warnings.filterwarnings('ignore')
#Austin Oligario 100554418

from pylab import *

figure(1, figsize=(10,10))
list = (46, 38, 29, 24, 13, 11, 11, 8, 8, 7, 107)
medal_sum = sum(list)
cs=cm.Set1(np.arange(40)/40.)
labels = 'USA', 'CHN', 'GBR', 'RUS', 'KOR', 'GER', 'FRA', 'ITA', 'HUN', 'AUS', 'OTHER'
fracs = [46/medal_sum, 38/medal_sum, 29/medal_sum, 24/medal_sum, 13/medal_sum, 11/medal_sum, 11/medal_sum, 8/medal_sum, 8/medal_sum, 7/medal_sum, 107/medal_sum]

_, _, autotexts = pie(fracs,  labels=labels, autopct='%1.1f%%', startangle=90, colors=cs)
for autotext in autotexts:
    autotext.set_color('white')

legend(labels,loc='lower right',fontsize=9)
title('2012 Olympic gold medals (100554418)')
show()"
"n = 256
X = np.linspace(-np.pi,np.pi,n,endpoint=True)
Y = np.sin(2*X)

plt.plot (X, Y+1, color='blue', alpha=1.00)
plt.plot (X, Y-1, color='blue', alpha=1.00)
plt.fill_between(X, 1, Y+1, alpha=0.3)
plt.fill_between(X, -1, Y-1, where=Y-1 >= -1, facecolor='blue', alpha=0.3)
plt.fill_between(X, -1, Y-1, where=Y-1 <= -1, facecolor='red', alpha=0.3)
plt.show()"
"n = 21
Z = np.random.vonmises(20,20,n)
plt.pie(Z)
plt.show()"
"n = 300
X = np.random.normal(0,1,n)
Y = np.random.normal(0,1,n)
Z = np.random.uniform(0,1,n)

plt.scatter(X,Y, c=X, alpha=0.6, s=35)
plt.show()"
"plt.hist(gen_frv, 50, normed=True)
rv = sts.f(dfn, dfd)
x = np.linspace(0.0001, 10, 500) # задание x

plt.plot(x, rv.pdf(x)) # описание графика
plt.title('F-distribution plot')
plt.ylabel('$probability$')
plt.xlabel('$x$');
plt.show()"
"split_s = [5, 10, 25, 50, 100] # задание объемов выборок для проверки =
means = calc_sample_mean(split_s, dfn, dfd)
plot_sample_means(means, split_s, dfn, dfd)"
"plot_clt(means, dfn, dfd)"
"plt.hist(gen_frv, 50, normed=True)
rv = sts.f(dfn, dfd)
x = np.linspace(0.0001, 10, 500) # задание x

plt.plot(x, rv.pdf(x)) # описание графика
plt.title('F-distribution plot')
plt.ylabel('$probability$')
plt.xlabel('$x$');
plt.show()"
"split_s = [5, 10, 50, 100] # задание объемов выборок для проверки =
means = calc_sample_mean(split_s, dfn, dfd)
plot_sample_means(means, split_s, dfn, dfd)"
"plot_clt(means, dfn, dfd)"
"print('Cramer\'s V = ',np.sqrt(chi2(t1_group_ct, correction = False)[0] / len(t1_group) * (np.minimum(t1_group_ct.shape[1], t1_group_ct.shape[0]) - 1)))"
"perform_churn_eff_analysis(data, [0, 2])"
"mpl.rcParams['legend.fontsize'] = 10

fig3 = plt.figure()
ax3 = fig3.gca(projection='3d')
theta = np.linspace(-4 * np.pi, 4 * np.pi, 100)
z = np.linspace(-2, 2, 100)
r = z**2 + 1
x = r * np.sin(theta)
y = r * np.cos(theta)
ax3.view_init(30,10)
ax3.plot(x, y, z)
#ax.plot(x, y, z, label='parametric curve')
#ax.legend()

plt.show()"
"fig3 = plt.figure(figsize=(20,20),frameon=False)
ax3 = fig3.gca(projection='3d')
ax3.view_init(0,90)
ax3.plot(xf, yf, zf,'.')
plt.show()"
"fig = plt.figure(figsize=(10,10))
plt.axes(frameon = True)
plt.axis('off')
plt.plot(xf,yf,'.')"
"fig, ax = plt.subplots()
lines = ax.plot(a,'r',b,'g')
plt.ioff()
plt.draw()
plt.show()"
"plt.draw()
plt.show()"
"mpl.rcParams['legend.fontsize'] = 10

fig = plt.figure()
ax = fig.gca(projection='3d')
theta = np.linspace(-4 * np.pi, 4 * np.pi, 100)
z = np.linspace(-2, 2, 100)
r = z**2 + 1
x = r * np.sin(theta)
y = r * np.cos(theta)
ax.view_init(30,10)
ax.plot(x, y, z)
#ax.plot(x, y, z, label='parametric curve')
#ax.legend()

plt.show()"
"fig = plt.figure(figsize=(12,6))

theta = np.linspace(-4 * np.pi, 4 * np.pi, 100)
z = np.linspace(-20, 20, 100)
r = z**2 * 0.01
x = r * np.sin(theta)
y = r * np.cos(theta)

ax = fig.add_subplot(1,2,1, projection='3d')
ax.plot(x, y, z,'.')
ax.view_init(30, 85)

ax = fig.add_subplot(1,2,2, projection='3d')
ax.plot(x, y, z,'.')
ax.view_init(30, 95)

fig.tight_layout()"
"# Convert data from a file to a big list
capacitor_data = (pd.read_csv('first_example.csv')
                    .as_matrix().flatten())
capacitor_data"
"# Get figure objects to keep and work on
fig, ax = plt.subplots();
ax.set_xlim((cmin - bin_width, cmax + bin_width));
ax.hist(capacitor_data, bins=bins);"
"# Add fitting
area = N * bin_width
std = np.sqrt(variance)
fit = area * st.norm(loc=mu, scale=std).pdf(bins)
ax.plot(bins, fit, 'r--', linewidth=3);
fig"
"import warnings
warnings.filterwarnings('ignore')
import numpy as np

random_data = np.random.rand(500)
counts, bin_edges = np.histogram(random_data, bins=3)
print(bin_edges)
# bin_centers = ([1, ..., N] + [0, ..., N-1]) * 0.5
bin_centers = (bin_edges[1:] + bin_edges[:-1]) * 0.5
print(bin_centers)"
"import warnings
warnings.filterwarnings('ignore')
%matplotlib inline
import matplotlib.pyplot as plt
import numpy as np
import seaborn as sns

x = np.linspace(0, 3, 100)
rates = [10**6, 2 * 10**6, 4 * 10**6, 6 * 10**6, 8 * 10**6, 10**7]
mus = [rate * 200 * 10**-9 for rate in rates]
efficiencies = [ 1 / (1 + mu) for mu in mus]

plt.plot(x, 1 / (1 + x));
plt.scatter(mus, efficiencies, s=50, lw=2, marker='x')
plt.xlim([0, 3])
plt.ylim([0, 1])
plt.ylabel('efficiency');
plt.xlabel('particles per dead time');"
"def matrix_to_euler(Re):
    A31 = Re[2,0]
    A32 = Re[2,1]
    A33 = Re[2,2]
    A13 = Re[0,2]
    A23 = Re[1,2]
    beta = np.arccos(A33)
    gamma = np.arctan2(A31,A32)
    alpha = np.pi - np.arctan2(A13,A23)
    omega = [alpha,beta,gamma]
    return omega

omega = [1.9, 2.3, 0.1]
Re = euler_to_matrix(omega)
omeganew = matrix_to_euler(Re)
Renew = euler_to_matrix(omeganew)
print (omega)
print (omeganew)
print (Re)
print (Renew)

# Checking that we understand these rotations
unitx = matrix([1,0,0]).T
unity = matrix([0,1,0]).T
unitz = matrix([0,0,1]).T
omega = [0,.1,0]
unitz_rotated = euler_to_matrix(omega)*unitz; print (""beta rotates unit z to:\n"", unitz_rotated)
omega = [.1,0,0]
unitz_rotated = euler_to_matrix(omega)*unitz; print (""alpha rotates unit z to:\n"", unitz_rotated)
"
"# This is the original formulation

# Specify the atomic configurations
omega1i_bar = [0.1,1.5,0]  
omega2j_bar = [0.1,0.5,0]

# Expand
print (omega1i_bar)
print (omega2j_bar)
rij, brackets = expand_in_R(l1max,l2max,ri,rj,omega1i_bar,omega2j_bar,R_num)
print (rij)"
"# Here, start with atoms along the z-axis, and rotate the frames to match the above
# This should equal the cell above

# Specify the atomic configurations
omega1i_bar = [0,0,0]
omega2j_bar = [0,0,0]

# Specify a frame rotation for each atom
omega1i_num = [0.1,1.5,0]
omega2j_num = [0.1,0.5,0]

# Translate to rotation matrices, combine them, and get back the euler angles
R_omega1i_bar = euler_to_matrix(omega1i_bar)
R_omega2j_bar = euler_to_matrix(omega2j_bar)
R_omega1i = euler_to_matrix(omega1i_num)
R_omega2j = euler_to_matrix(omega2j_num)
R_omega1i_omega1i_bar = R_omega1i*R_omega1i_bar
R_omega2j_omega2j_bar = R_omega2j*R_omega2j_bar
omega1i_bar_p = matrix_to_euler(R_omega1i_omega1i_bar)
omega2j_bar_p = matrix_to_euler(R_omega2j_omega2j_bar)

# Expand using these new euler angles (which should be the same as the previous ones)
print (omega1i_bar_p)
print (omega2j_bar_p)
rij, brackets = expand_in_R(l1max,l2max,ri,rj,omega1i_bar_p,omega2j_bar_p,R_num)
print (rij)"
"# Now try an omega-R expansion of the Wigner D-matrices*omega-bar instead
# This should equal the cell above
rij, brackets = expand_in_omega_R(l1max,l2max,ri,rj,omega1i_bar,omega2j_bar,omega1i_num,omega2j_num,R_num)
print (rij)"
"# Now try an R-expansion of the Wigner D-matrices alone
# This should equal the cell above
rij, brackets = expand_in_R(l1max,l2max,ri,rj,omega1i_num,omega2j_num,R_num)
print (rij)"
"# Next we'll try rotating each with a common omega1, omega2
# This will be different from the above because each molecule is rotated
omega1_num = [.1,.2,.3]
omega2_num = [.2,.3,.4]
rij, brackets = expand_in_omega_R(l1max,l2max,ri,rj,omega1i_num,omega2j_num,omega1_num,omega2_num,R_num)
print (rij)"
"# Now we try an expansion of the Wigner D-matrices, with angle psi applied to both
# This result should be the same as the cell above since psi=0

# This is the angle 
psi_num = 0

# Expand 
rij, brackets = \
    expand_in_omega_R_psi(l1max,l2max,ri,rj,omega1i_num,omega2j_num,omega1_num,omega2_num,R_num,psi_num)
print (rij)"
"# Now we try an expansion of the Wigner D-matrices, with angle psi applied to both
# This result will be different from the cell above (because psi != 0)

# This is the angle 
psi_num = .2

# Expand 
rij, brackets = \
    expand_in_omega_R_psi(l1max,l2max,ri,rj,omega1i_num,omega2j_num,omega1_num,omega2_num,R_num,psi_num)
print (rij)"
"# Testing against the geometrically obtained distances
R = matrix([0,0,R_num]).T

R_omega1i_num = euler_to_matrix(omega1i_num)
R_omega2j_num = euler_to_matrix(omega2j_num)
V1 = R_omega1i_num*matrix([0,0,ri]).T; print (V1)
V2 = R_omega2j_num*matrix([0,0,rj]).T; print (V2)
Rij = linalg.norm(V1-(V2+R)); print (""Actual distance case 1 is"", Rij,""\n"")

R_omega1_num = euler_to_matrix(omega1_num)
R_omega2_num = euler_to_matrix(omega2_num)
V11 = R_omega1_num*V1; print (V11)
V22 = R_omega2_num*V2; print (V22)
Rij = linalg.norm(V11-(V22+R)); print (""Actual distance case 2 is"", Rij,""\n"")

R_psi_num = euler_to_matrix([0,psi_num,0])
V111 = R_psi_num*V11; print (V111)
V222 = R_psi_num*V22; print (V222)
Rij = linalg.norm(V111-(V222+R)); print (""Actual distance case 3 is"", Rij,""\n"")"
"help("""".isdigit)"
"help("""".isnumeric)"
"bob = Person(""Bob"", ""bob@bob.com"", 89104)
bob"
print(json_dict)
print(*json_dict.values())
person
"with open(""students.csv"") as file:
    reader = csv.DictReader(file) # DictReader creates a dictionary out of each row matched to the header
    for row in reader:
        print(row)"
"my_print(""Hello"", ""World"", ""I"", ""am"", ""here"")"
"my_kw_print(foo=""bob"", bar=""fred"")"
[random.random() for _ in range(5)]
"[
    random.random()  #collection
    for _ in range(5) #iteration
]"
"sentence = ""Hello there pardner""
{
    letter: sentence.count(letter)
    for letter in sentence
    if letter is not "" ""
}"
help(frozenset)
"students = [""Blake"", ""Justice"", ""Kai"", ""Rowan"", ""Marion"", ""Hunter""]
possible_pairings = {
    frozenset([s1, s2])
    for s1 in students
    for s2 in students
    if s1 is not s2}
possible_pairings"
help(enumerate)
"# Map students to their grades.

students = [""Marion"", ""Sawyer"", ""Hayden""]
test_scores = [[87, 91, 79], [92, 90, 85], [90, 93, 82]]

{student: [test[idx] for test in test_scores] 
 for (idx, student) in enumerate(students)}"
"import warnings
warnings.filterwarnings('ignore')
import re
help(re.search)"
"re.search(r'e', ""hello"")"
"re.search(r'll', 'hello')"
"re.search(r""pattern"", sentence)"
"re.search(r""oo."", sentence)"
"re.search(r""a."", sentence)"
"re.search(r'.oo.', sentence)"
"re.search(r'.oo..', sentence)"
"re.search(r'\.', sentence)"
"match = re.search(""pattern"", sentence)
help(match)"
"re.search(r""^A "", sentence)"
"re.search(r"".\.$"", sentence)"
"re.search(r""How are you\?"", my_hello)"
"re.search(r""How are you\?"", how)"
"re.search(r'^How are you\?$', how)"
"re.search(r'.oo....', sentence)"
"print(re.search(r""a*b"", no_a))
print(re.search(r""a*b"", one_a))
print(re.search(r""a*b"", lots_of_a))"
"print(re.search(r""a+b"", no_a))
print(re.search(r""a+b"", one_a))
print(re.search(r""a+b"", lots_of_a))"
"print(re.search(r""a{2}b"", no_a))
print(re.search(r""a{2}b"", one_a))
print(re.search(r""a{2}b"", lots_of_a))"
"print(re.search(r""a{1,2}b"", no_a))
print(re.search(r""a{1,2}b"", one_a))
print(re.search(r""a{1,2}b"", lots_of_a))"
"print(re.search(r""a{1,}b"", no_a))
print(re.search(r""a{1,}b"", one_a))
print(re.search(r""a{1,}b"", lots_of_a))"
"print(re.search(r""a{,2}b"", no_a))
print(re.search(r""a{,2}b"", one_a))
print(re.search(r""a{,2}b"", lots_of_a))"
"re.search(r""(a+b){2}"", ""abaaaabaab"")"
"re.search(r""[\.,;\?!]"", sentence)"
"re.search(r""[0-9]{3}-[0-9]{3}-[0-9]{4}"", '111-222-3333')"
"re.search(r""\d{3}-\d{3}-\d{4}"", '111-222-3333')"
"re.search(r""\D*"", ""My phone number is 111-222-3333"")"
"bar = ""hello.  How are you?""
match = re.search(""(?:hello.\s*)?(.*)$"", bar)
match.groups()"
response.content
simple_info
print(serialized_info)
response.json()
vader['films']
"for film in vader['films']:
    print(requests.get(film).json()['title'])"
response.content
response2.content
"import warnings
warnings.filterwarnings('ignore')
%matplotlib inline
import matplotlib.pyplot as plt

import numpy
from scipy.integrate import odeint

# reaktor półokresowy !

# A --> C, katalizatorem jest dostaczany B, 
# wyrażenie na szybkość wyznaczone doświadczalnie: v=-k*ca*cb, 
# k=0.25 mol/L*min
# V0 = 2700 L
# A0 = 20 mol/L
# B0 = 0
# [B] = 0.05 mol/L
# FlowB = 12.5 L/min
# t = 0 ... 200 min

B = 0.05 #mol/L
FlowB = 12.5 #L/min
k = 0.25 #mol/L*min
FB = FlowB * B #mol/min

def model(y, t):  

    nA = y[0]              # ponieważ V(t), trzeba stosować n(t) zamiast jak zwykle c(t) 
    nB = y[1]
    nC = y[2]
    V = y[3]               # czwarta zmienna zależna od czasu

    cA = nA/V
    cB = nB/V
    
    dnAdt = -k*cA*cB*V     # substrat
    dnBdt = FB             # katalizator
    dnCdt = k*cA*cB*V      # produkt
    dVdt = FlowB           # objętość układu (zmienna w czasie!)

    return [dnAdt, dnBdt, dnCdt, dVdt]

A0 = 20 #mol/L
B0 = 0
C0 = 0
V0 = 2700 #L

nA0 = A0 * V0
nB0 = B0 * V0
nC0 = C0 * V0

initial = [nA0, nB0, nC0, V0]
t = numpy.linspace(0, 200,50)
results = odeint( model, initial, t )

# stężenie c = n/V
cA = results[:,0]/results[:,3]
cB = results[:,1]/results[:,3]
cC = results[:,2]/results[:,3]
V = results[:,3]

#n
nA = results[:,0]
nB = results[:,1]
nC = results[:,2]


plt.plot(t,cA,'bo', label='[a]')
plt.plot(t,cB,'ro', label='[b]')
plt.plot(t,cC,'co', label='[b]')
#plt.plot(t,V,'co', label='V')
plt.xlabel('Czas reakcji')
plt.ylabel('Stezenie reagentow')
"
"plt.plot(t,V,'go', label='V')
plt.xlabel('Czas reakcji')
plt.ylabel('V')

print(V)"
"# szybkość reakcji w funkcji czasu

plt.plot(t,k*cA*cB*V,'ko', label='V')
plt.xlabel('Czas reakcji, min')
plt.ylabel('Szybkosc reakcji, mol/L*min')"
"qht_min = -0.05
qht_max = 0.05
ght_min = -0.05
ght_max = 0.05
e1_min = -0.05
e1_max = 0.05
ap_min = 0.0
ap_max = 2.0
def displayISline(rr,qht,ght,e1,ap):
    yht_0 = ISline(rr,a,b,c,0,0,0)
    yht = ISline(rr,ap,b,c,qht,ght,e1)
    plt.plot(yht_0,rr,'k-')
    plt.plot(yht,rr)
    xmin = min(ISline(rr,ap_max,b,c,qht_min,ght_min,e1_min))
    xmax = max(ISline(rr,ap_max,b,c,qht_max,ght_max,e1_max))
    xl = xmin*1.15 if xmin<=0 else xmin*0.85
    xu = xmax*0.85 if xmax<=0 else xmax*1.15
    
    if min(rr)<0:
        yl = min(rr)*1.15
    elif min(rr)>0:
        yl = min(rr)*0.85
    else: 
        yl = -0.01    
    
    if max(rr)<0:
        yu = max(rr)*0.85
    elif max(rr)>0:
        yu = max(rr)*1.15
    else: 
        yu = 0.01   
    
    plt.xlim([xl,xu])
    plt.ylim([yl,yu])
    plt.hlines(0, xl, xu)
    plt.vlines(0, yl, yu)
    plt.xlabel('$\hat{y}_t$',fontsize=20)
    plt.ylabel('$r_t$',fontsize=20)
    plt.text(yht_0[0],rr[0]+0.002,""$IS_0$"",fontsize=15)
    plt.text(yht[-1],rr[-1],""$IS_1$"",fontsize=15,color=""blue"")
    
wIS = interactive(displayISline,
                rr = fixed(r_vec),
                qht = widgets.FloatSlider(min=qht_min,max=qht_max,step=0.01,description=""$\hat{q}_t$"",continuous_update=False), 
                ght = widgets.FloatSlider(min=ght_min,max=ght_max,step=0.01,description=""$\hat{g}_t$"",continuous_update=False), 
                e1 = widgets.FloatSlider(min=e1_min,max=e1_max,step=0.01,description=""$\epsilon_{1,t}$"",continuous_update=False),
                ap = widgets.FloatSlider(min=ap_min,max=ap_max,step=0.1,value=a,description=""$a$"",continuous_update=False))
display(wIS)"
"qht_min = -0.05
qht_max = 0.05
e2_min = -0.05
e2_max = 0.05
dp_min = 0.0
dp_max = 2.0
def displayPCline(yht,qht,e2,dp):
    infl_0 = PCline(d,e,yht,0,0)
    infl = PCline(dp,e,yht,qht,e2)
    plt.plot(yht,infl_0,'k-')
    plt.plot(yht,infl)
    
    if min(yht)<0:
        xl = min(yht)*1.15
    elif min(yht)>0:
        xl = min(yht)*0.85 
    else: 
        xl = -0.01    
    
    if max(yht)<0:
        xu = max(yht)*0.85
    elif max(yht)>0:
        xu = max(yht)*1.15
    else: 
        xu = 0.01   
    
    ymin = min(PCline(dp_max,e,yht,qht_min,e2_min))
    ymax = max(PCline(dp_max,e,yht,qht_max,e2_max))
    yl = ymin*1.15 if ymin<=0 else ymin*0.85
    yu = ymax*0.85 if ymax<=0 else ymax*1.15
    plt.xlim([xl,xu])
    plt.ylim([yl,yu])
    plt.hlines(0, xl, xu)
    plt.vlines(0, yl, yu)
    plt.xlabel('$\hat{y}_t$',fontsize=20)
    plt.ylabel('$\pi_t$',fontsize=20)
    plt.text(yht[0],infl_0[0]-0.02,""$PC_0$"",fontsize=15)
    plt.text(yht[-1]-0.005,infl[-1]+0.01,""$PC_1$"",fontsize=15,color=""blue"")

wPC = interactive(displayPCline, 
                yht = fixed(yhat_vec),
                qht = widgets.FloatSlider(min=qht_min,max=qht_max,step=0.01,description=""$\hat{q}_t$"",continuous_update=False),
                e2 = widgets.FloatSlider(min=e2_min,max=e2_max,step=0.01,description=""$\epsilon_{2,t}$"",continuous_update=False),
                dp = widgets.FloatSlider(min=dp_min,max=dp_max,step=0.1,value=d, description=""$d$"",continuous_update=False))
display(wPC)"
"plt.rcdefaults()
plt.rcParams['font.family'] = 'fantasy'
plt.rcParams['font.fantasy'] = 'Times New Roman', 'Ubuntu','Arial','Tahoma','Calibri'

rstar_min = -0.05
rstar_max = 0.10
pistar_min = -0.05
pistar_max = 0.10
ghat_min = -0.05
ghat_max = 0.05
eps1_min = -0.05
eps1_max = 0.05
eps2_min = -0.05
eps2_max = 0.05
Ap_min = 0.0
Ap_max = 2.0
ap_min = 0.0
ap_max = 2.0
bp_min = 0.0
bp_max = 2.0
cp_min = 0.0
cp_max = 2.0
dp_min = 0.0
dp_max = 2.0
ep_min = 0.0
ep_max = 2.0
fp_min = 0.0
fp_max = 2.0
phip_min = 0.05
phip_max = 0.3
taup_min = 0.30
taup_max = 0.45
gammabp_min = 0.0
gammabp_max = 0.04

def printmainvars(Apar=A, apar=a, bpar=b, cpar=c, dpar=d, epar=e, fpar=f, gammabarpar=gammabar, 
                  rstar_in=rstar, pistar_in=pistarbar, ghat_in=0, eps1_in=0, eps2_in=0,
                  phipar=phi, taupar=tau, Mshpar=Mshare):
    yhatsol = yhatfinal(rstar_in, Apar, apar, bpar, cpar, dpar, epar, fpar, pistar_in, ghat_in, eps1_in, eps2_in)
    pisol = pifinal(rstar_in, Apar, apar, bpar, cpar, dpar, epar, fpar, pistar_in, ghat_in, eps1_in, eps2_in)
    pisol = round(pisol,3)
    yhatsol = round(yhatsol,3)
    Mchsol = Mchange(pisol,yhatsol,gammabarpar)
    Gammachsol = Gammachange(pisol,yhatsol,ghat_in,gammabarpar,phipar,taupar,pibar) 
    Fchsol = Fchange(Mchsol,Gammachsol,Mshpar)
    print(""При зададените стойности на параметрите, шоковете и екзогенните променливи инфлацията е {:0.1f}%."".format(pisol*100))
    print(""Отклонението от потенциалното производство е {:0.1f}%."".format(yhatsol*100))
    print(""Изменението на резервните пари е {:0.1f}%."".format(Mchsol*100))
    print(""Изменението на депозита на правителството в централната банка е {:0.1f}%."".format(Gammachsol*100))
    print(""Изменението на валутните резерви е {:0.1f}%."".format(Fchsol*100))
    
    pct_multiplier = 100
    
    yhatsol*=pct_multiplier
    pisol*=pct_multiplier
    Mchsol*=pct_multiplier
    Gammachsol*=pct_multiplier
    Fchsol*=pct_multiplier
    
    f, (ax1, ax2) = plt.subplots(1, 2)
    ax1.scatter(yhatsol,pisol,s=100,c='r')
    ax1.set_xlabel('$\hat{y}_t$, %',fontsize=20)
    ax1.set_ylabel('$\pi_t$, %',fontsize=20)
    ax1.set_xlim([-0.2*pct_multiplier,0.2*pct_multiplier])
    ax1.set_ylim([-0.2*pct_multiplier,0.2*pct_multiplier])
    ax1.hlines(0,-.2*pct_multiplier,.2*pct_multiplier)
    ax1.hlines(pibar*pct_multiplier,-.2*pct_multiplier,.2*pct_multiplier,linestyles='dashed')
    ax1.vlines(0,-.2*pct_multiplier,.2*pct_multiplier)
    
    if yhatsol<0:
        ytext = ""Рецесия""
    elif yhatsol>0:
        ytext=""Прегряване""
    else:
        ytext=""Равновесие""
    
    if pisol==pibar*pct_multiplier:
        pitext=""Равновесна\nинфлация""
    elif pisol>pibar*pct_multiplier:
        pitext=""Инфлация\nнад\nравновесната""
    elif 0<pisol<pibar*pct_multiplier:
        pitext=""Инфлация\nпод\nравновесната""
    elif pisol<0:
        pitext= ""Дефлация""
    else:
        pitext=""???""
    
    txt = ytext + "",\n"" + pitext
    ax1.text(0.05*pct_multiplier,0.1*pct_multiplier, txt, size=""medium"")
    
    ax2.bar([1,2,3],[Mchsol,Fchsol,Gammachsol],color=['g','r','g'])
    ax2.set_xlim([0.75,4.0])
    ax2.set_ylabel('изменение, %',fontsize=20)
    ax2.set_xticks([1.5,2.5,3.5]) 
    ax2.set_xticklabels([""$M_t$"", ""$F_t$"", ""$\Gamma_t$""],fontsize=20)
    f.tight_layout() # to prevent label overlap
    
wsol = interactive(printmainvars,                      
                  Apar = widgets.BoundedFloatText(min=Ap_min,max=Ap_max,step=0.01,value=A,description=""$A$""),
                  apar = widgets.BoundedFloatText(min=ap_min,max=ap_max,step=0.01,value=a,description=""$a$""),
                  bpar = widgets.BoundedFloatText(min=bp_min,max=bp_max,step=0.01,value=b,description=""$b$""),
                  cpar = widgets.BoundedFloatText(min=cp_min,max=cp_max,step=0.01,value=c,description=""$c$""),
                  dpar = widgets.BoundedFloatText(min=dp_min,max=dp_max,step=0.01,value=d,description=""$d$""),
                  epar = widgets.BoundedFloatText(min=ep_min,max=ep_max,step=0.01,value=e,description=""$e$""),
                  fpar = widgets.BoundedFloatText(min=fp_min,max=fp_max,step=0.01,value=f,description=""$f$""),
                  gammabarpar = widgets.BoundedFloatText(min=gammabp_min,max=gammabp_max,step=0.01,value=gammabar,description=r""$ \bar{\gamma}  $""),
                  rstar_in = widgets.FloatSlider(min=rstar_min,max=rstar_max,step=0.01,value=rstar,description=""$r^*_t$""),
                  pistar_in = widgets.FloatSlider(min=pistar_min,max=pistar_max,step=0.01,value=pistar,description=""$\pi^*_t$""),
                  ghat_in = widgets.FloatSlider(min=ghat_min,max=ghat_max,step=0.01,value=0,description=""$\hat{g}_t$""),
                  eps1_in = widgets.FloatSlider(min=eps1_min,max=eps1_max,step=0.01,value=0,description=""$\epsilon_{1,t}$""),
                  eps2_in = widgets.FloatSlider(min=eps2_min,max=eps2_max,step=0.01,value=0,description=""$\epsilon_{2,t}$""),
                  phipar = widgets.FloatSlider(min=phip_min,max=phip_max,step=0.01,value=phi,description=""$\phi$""),
                  taupar = widgets.FloatSlider(min=taup_min,max=taup_max,step=0.01,value=tau,description= r'$ \tau $'),
                  Mshpar = widgets.FloatSlider(min=0,max=1,step=0.01,value=Mshare,description=r'$\frac{M_{t-1}}{F_{t-1}}$'),
                  __manual=True)
wsol.children[-1].description = ""Покажи""
display(wsol)"
"IFrame(""http://origins.sese.asu.edu/ses405/Reference%20Documents/DAU_Wallchart.pdf"",
       width=""1380"", height=""900"")"
"IFrame(""https://losc.ligo.org/s/events/GW150914/GW150914_tutorial.html"",
       width=""1380"", height=""900"")"
"IFrame(
    ""https://nbviewer.jupyter.org/format/slides/github/lmarti/evolutionary-computation-course/blob/master/AEC.06%20-%20Evolutionary%20Multi-Objective%20Optimization.ipynb#/1"",
    width=""1380"", height=""900"")"
"IFrame(""http://homepages.wmich.edu/~mcgrew/Siderius.pdf"", width=""1380"", height=""900"")"
"IFrame(""https://gitter.im/Anaconda-Platform/nbpresent/~embed"", 400, 700)"
"import warnings
warnings.filterwarnings('ignore')
!nbpresent --help"
"# The size of our dataset
size = 50

# x = [0,1..8,9]
x = np.arange(size)

# Generate noise
noise = np.random.randn(size)

# Make y = x + noise
y = x + noise

# Plot it and see what it looks like:
figure, ax = plt.subplots(figsize=(10,10))
ax.scatter(x, y)"
"# Run regression.  Note the call to `fit()` after the call to `ols`.
mod = smf.ols(formula='y ~ x', data = d).fit()

# Print the regression results
print(mod.summary())"
"figure, ax = plt.subplots(figsize=(10,10))

# Scatter the actual points
ax.scatter(x, y)

# The regression line is the intercept (`mod.params[0]`) + the coefficent for x (`mod.params[1]`) times x.
r = mod.params[0] + mod.params[1] * x

ax.plot(x,r, 'r--')"
"# Create 3 random variables: a, b and c
a = np.random.randn(size)
b = np.random.randn(size)
c = np.random.randn(size)

# Make y = a + 2b + 3c + noise
y = a + 2*b + 3*c + np.random.randn(size)/2

# Create dictionary
d = {'a': a, 'b': b, 'c': c, 'y': y}

# Run Regression with all three independent variables
mod = smf.ols(formula='y ~ a + b + c', data = d).fit()
print(mod.summary())"
"figure, ax = plt.subplots(figsize=(10,10))

# Scatter the values of y vs. the predicted values from the model
ax.scatter(y, mod.predict())

# Plot the line X=Y to help see the error
ax.plot(ax.get_xlim(), ax.get_ylim(), 'r--')

ax.set_xlabel(""True Values"")
ax.set_ylabel(""Predicted Values"")"
"# Create x
x = np.arange(size)

# Create x^2
x2 = x**2

# Make y = 2x + 3x^2 + noise
y = x*2 + 3*x2 + np.random.randn(size)*25

# Store in a dict
d = {'x': x, 'x2': x2, 'y': y}

# When running the regression, include x^2 as one of the independent variables
mod = smf.ols(formula='y ~ x + x2', data = d).fit()

# Visualize the regression
figure, ax = plt.subplots(figsize=(10,10))
ax.scatter(x, y, s=100)
r = mod.params[0] + mod.params[1]*x + mod.params[2]*x2
ax.plot(x,r, 'r--')"
"# Use a panda's DataFrame instead of a regular dictionary to pass the independent variables
import pandas as pd
X = pd.DataFrame({'x': x, 'x2': x2})

# Here we pass the dependent variable as the first parameter
# and the second parameter will contain all the independent variables
# you want to include in the regression.
mod = sm.OLS(y, X).fit()

# Visualize the regression
figure, ax = plt.subplots(figsize=(10,10))
ax.scatter(x, y, s=100)
r = mod.predict()
ax.plot(x, r, 'r--')"
"mod = smf.ols(formula='value ~ invest + capital + year + C(firm)', data = d).fit()
print(mod.summary())"
"figure, ax = plt.subplots(figsize=(10,10))

# Scatter the values of y vs. the predicted values from the model
ax.scatter(d['value'], mod.predict())

# Plot the line X=Y to help see the error
ax.plot(ax.get_xlim(), ax.get_ylim(), 'r--')

ax.set_xlabel(""True Values"")
ax.set_ylabel(""Predicted Values"")"
"mod = smf.ols(formula='value ~ invest + capital + year', data = d).fit()

print(mod.summary())

figure, ax = plt.subplots(figsize=(10,10))

# Scatter the values of y vs. the predicted values from the model
ax.scatter(d['value'], mod.predict())

# Plot the line X=Y to help see the error
ax.plot(ax.get_xlim(), ax.get_ylim(), 'r--')

ax.set_xlabel(""True Values"")
ax.set_ylabel(""Predicted Values"")"
"# Create a figure and axes
figure, ax = plt.subplots()

# Line of x vs y
ax.plot(x, y)"
"# Create a figure of size 8x8
figure, ax = plt.subplots(figsize=(8,8))
ax.plot(x, y)"
"figure, ax = plt.subplots()

# `r` means read, `--` means dashed
ax.plot(x, y, 'r--')"
"figure, ax = plt.subplots()
ax.plot(x, y)

ax.set_xlabel('X')
ax.set_ylabel('Y')
ax.set_title('Y v X')"
"figure, ax = plt.subplots()
ax.scatter(x, y)"
"figure, ax = plt.subplots()

# Make each point size 100
ax.scatter(x, y, s=100)"
"figure, ax = plt.subplots()

# Make each point size y value*20
ax.scatter(x, y, s=y*20)"
"figure, ax = plt.subplots()

ax.plot(x, y)
ax.scatter(x, y, s=100)"
"figure, ax = plt.subplots()

ax.plot(x, y, label='line')
ax.scatter(x, y, s=100, label='points')

ax.legend()"
"figure, ax = plt.subplots()

ax.plot(x, y, label='line')
ax.scatter(x, y, s=100, label='points')

ax.legend(loc='upper left')"
"nrows = 1 # Just one row
ncols = 2 # of two plots
figsize = (12,6) # We want each plot to be 6x6, so the width should be 6*2=12

# The axes are returned as a 1-tuple with a 2-tuple of the axes
figure, ((ax0, ax1)) = plt.subplots(figsize=figsize, nrows=nrows, ncols=ncols)

ax0.plot(x, y)
ax1.scatter(x, y, s=100)"
"nrows = 2 # This time we want 2 rows
ncols = 1 # A just one column
figsize = (6,12) # We need to switch the figsize to be height 12

# Notice we also need to modify the tuple to be a 2-tuple of 1-tuples
figure, ((ax0), (ax1)) = plt.subplots(figsize=figsize, nrows=nrows, ncols=ncols)

ax0.plot(x, y)
ax1.scatter(x, y, s=100)"
"# Let's craete some normally distributed data:
n = np.random.randn(1000)

figure, ax = plt.subplots()
ax.hist(n)"
"figure, ax = plt.subplots()

# Use 20 bins instead
ax.hist(n, bins=20)"
plt.style.available
"plt.style.use('ggplot')

figure, ax = plt.subplots()
ax.plot(x, y)

ax.set_xlabel('X')
ax.set_ylabel('Y')
ax.set_title('Y v X')"
!pwd
!ls
fc
ccs(ws[0])
[ccs(w) for w in ws]
"[[(x[1],x[0]) for x in ccs(w)] for w in ws]"
a = %ls
"fh = open(""divina_commedia.txt"",encoding=""latin1"").read()"
"""ecco,un\npo'di puntegg..iatura"".split()"
"s = [""ciao"",""a"",""tutti""]"
""""".join([c for c in s])"
[c for c in fh[1000:1200]]
"s = ""ecco,un\npo'di puntegg..iatura"""
"'.' in "".;:"""
""""".join([c for c in s if not c in "".:,;-""])"
from string import punctuation
punctuation
"fh = """".join([(c if not c in punctuation else "" "") for c in fh])"
fh[1000:1200]
words = fh.split()
len(words)
len(set(words))
words = list(set(words))
words[1500]
""
choice(words)
[choice(words) for _ in range(10)]
ccs(words[10])
Counter(words)
"import pandas as pd
pd.read_csv('input/value_translation_list.csv')"
"interact(f, x=(-100,100,5), continuous_update=False)"
"interact(f, x=(-100,100,5), continuous_update=True)"
"interact(f, x=(-100,100,5), __manual=True)"
"dd1 = widgets.Dropdown(
    options={'One': 1, 'Two': 2, 'Three': 3},
    value=2,
    description='Number:',
)
dd2 = widgets.Dropdown(
    options={'One': 1, 'Two': 2, 'Three': 3},
    value=2,
    description='Number:',
)
display(dd1, dd2)

def on_dd1_change(change):
    dd2.options = {'new': change['new'], 'old': change['old']}
    dd2.value = change['new']
    #print(change['new'])

dd1.observe(on_dd1_change, names='value')

int_range = widgets.IntSlider()
display(int_range)

def on_value_change(change):
    print(change['new'])

int_range.observe(on_value_change, names='value')"
"N = range(1, 20)
M = 7
plt.plot(N, [posion_dist(M, i) for i in N])"
"[(i, binomial_dist(4, 10, 0.5)) for i in range(3, 15)]"
"fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(16,6))
axes[0].plot(hours, visits, color='k', label='Visits')
axes[0].plot(hours, no_noise_visits, color='r', label='Visits w/o noise')
axes[0].set_xlabel('Hours', fontsize=20)
axes[0].set_ylabel('Visits', fontsize=20)
axes[0].set_title('Whole dataset', fontsize=20)
axes[0].tick_params(labelsize=14)
axes[0].legend(fontsize=14, loc='lower right')
sns.despine()

axes[1].plot(hours[10:25], visits[10:25], color='k', label='Visits')
axes[1].plot(hours[10:25], no_noise_visits[10:25], color='r', label='Visits w/o noise')
axes[1].set_xlabel('Hours', fontsize=20)
axes[1].set_ylabel('Visits', fontsize=20)
axes[1].set_title('Zoomed in', fontsize=20)
axes[1].tick_params(labelsize=14)
axes[1].legend(fontsize=14, loc='lower right')"
"fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(16,6))
axes[0].plot(hours, visits, color='k', alpha=0.6, linewidth=2, label='Visits')
axes[0].plot(hours, no_noise_visits, color='k', linestyle=':', linewidth=2, label='No noise')
axes[0].plot(hours[1:-1], visits_3hr, color='r', linewidth=2, label='3hr avg')
axes[0].set_title('3hr avg', fontsize=20)
axes[0].tick_params(labelsize=14)
axes[0].legend(fontsize=14, loc='lower right')

axes[1].plot(hours, visits, color='k', alpha=0.6, linewidth=2, label='Visits')
axes[1].plot(hours, no_noise_visits, color='k', linestyle=':', linewidth=2, label='No noise')
axes[1].plot(hours[2:-2], visits_5hr, color='b', linewidth=2, label='5hr avg')
axes[1].set_title('5hr avg', fontsize=20)
axes[1].tick_params(labelsize=14)
axes[1].legend(fontsize=14, loc='lower right')

axes[2].plot(hours, visits, color='k', alpha=0.6, linewidth=2, label='Visits')
axes[2].plot(hours, no_noise_visits, color='k', linestyle=':', linewidth=2, label='No noise')
axes[2].plot(hours[4:-5], visits_10hr, color='g', linewidth=2, label='10hr avg')
axes[2].set_title('10hr avg', fontsize=20)
axes[2].tick_params(labelsize=14)
axes[2].legend(fontsize=14, loc='lower right')

# plt.plot(hours, visits, color='k', label='data')
# plt.plot(hours[1:-1], visits_3hr, color='r', label='3hr avg')
# plt.plot(hours[2:-2], visits_5hr, color='b', label='5hr avg')
# plt.plot(hours[4:-5], visits_10hr, color='g', label='10hr avg')
# plt.xlabel('Hours', fontsize=20)
# plt.ylabel('Visits', fontsize=20)
# plt.tick_params(labelsize=14)
# plt.legend(fontsize=14, loc='lower right')
sns.despine()
"
"dx_gaussian = 0.03
x_gaussian = np.arange(-1, 1+dx_gaussian, dx_gaussian)
y_gaussian = sp.stats.norm.pdf(x_gaussian, loc=0.3, scale=0.1)

### 11 point moving average
x_gaussian11 = x_gaussian[5:-5]
y_gaussian11 = (y_gaussian[:-10] + y_gaussian[1:-9] + y_gaussian[2:-8] + y_gaussian[3:-7] +
                y_gaussian[4:-6] + y_gaussian[5:-5] + y_gaussian[6:-4] + y_gaussian[7:-3] +
                y_gaussian[8:-2] + y_gaussian[9:-1] + y_gaussian[10:])/11.0

plt.figure(figsize=(8,8))
plt.plot(x_gaussian, y_gaussian, color='k', label='original')
plt.plot(x_gaussian11, y_gaussian11, color='r', label='11 pt avg')
plt.legend(fontsize=14)
plt.axis([-1, 1, 0, 4])
sns.despine()"
"SG_11_coefs = savgol_coeffs(11, 2)
print(SG_11_coefs)
y_SG = savgol_filter(y_gaussian, 11, 2, delta=dx_gaussian)

plt.figure(figsize=(8,8))
plt.plot(x_gaussian, y_gaussian, color='k', label='original')
plt.plot(x_gaussian, y_SG, color='r', label='SG filter N = 2, L = 11')
plt.legend(fontsize=14, loc='upper left')
plt.axis([-1, 1, 0, 4])
sns.despine()"
"SGvisits = savgol_filter(visits, 13, 6, delta=1, mode='mirror')

plt.figure(figsize=(8,8))
plt.plot(hours, visits, color='k', alpha=0.6, label='Visits')
plt.plot(hours, SGvisits, color='r', label='SG')
plt.plot(hours, no_noise_visits, color='k', linestyle=':', linewidth=2, label='No noise')
plt.legend(loc='lower right', fontsize=14)
plt.tick_params(labelsize=14)
plt.xlabel('Hours', fontsize=20)
plt.ylabel('Visits', fontsize=20)
sns.despine()"
"dumb_deriv = savgol_filter(visits, 3, 1, deriv=1, delta=1, mode='mirror')
# ^^^ this is the -1, 0, 1 centered difference derivative
deriv_on_nonoise = savgol_filter(no_noise_visits, 3, 1, deriv=1, delta=1, mode='mirror')
SGvisits_deriv = savgol_filter(visits, 15, 4, deriv=1, delta=1, mode='mirror')
deriv_on_5hr = savgol_filter(visits_5hr, 3, 1, deriv=1, delta=1, mode='mirror')
#hours[2:-2], visits_5hr


fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(16,6))

axes[0].plot(hours, dumb_deriv, color='k', alpha=0.6, label='Simple derivative')
axes[0].plot(hours, SGvisits_deriv, color='r', label='SG derivative')
axes[0].plot(hours[2:-2], deriv_on_5hr, color='b', alpha=0.6, label='Derivative on 5hr avg')
axes[0].legend(loc='lower right', fontsize=14)
axes[0].set_title('Data', fontsize=20)
axes[0].tick_params(labelsize=14)

axes[1].plot(hours, deriv_on_nonoise, color='k', alpha=0.6, label='True derivative')
axes[1].plot(hours, SGvisits_deriv, color='r', label='SG derivative')
axes[1].plot(hours[2:-2], deriv_on_5hr, color='b', alpha=0.6, label='Derivative on 5hr avg')
axes[1].legend(loc='upper right', fontsize=14)
axes[1].set_title('Versus True result', fontsize=20)
axes[1].tick_params(labelsize=14)"
"from scipy.fftpack import fft, fftfreq, fftshift
from scipy.signal import freqz
from ipywidgets import interact

def Fourier_SG_coefs(M, N):
    # Just so we don't get crazy N's
    if N > 2*M:
        N = 2*M
    SG_coefs = savgol_coeffs(2*M+1, N)
    w, h = freqz(SG_coefs)
    plt.figure(figsize=(8,8))
    plt.plot(w, 20*np.log10(abs(h)), color='k')
    plt.ylabel('Amplitude [dB]', fontsize=20)
    plt.xlabel('Freq [rad/sample]', fontsize=20)
    plt.tick_params(labelsize=14)
    plt.title('SG M = %d, N = %d' % (M, N), fontsize=20)

interact(Fourier_SG_coefs, M=(1, 25, 1), N=(1, 25, 1))"
"from IPython.display import Image
Image('SG_Fourier.png')
# Image from http://www-inst.eecs.berkeley.edu/~ee123/fa11/docs/SGFilter.pdf"
decade_cat
decade_cat.categories
"pd.cut(years,2,precision=-1)"
"pd.cut(years,2,precision=2)"
dframe.index.map(str.lower)
dframe
group1
group1.mean()
"dframe['dataset1'].groupby([cities,month]).mean()"
dframe
dframe.groupby('k1').mean()
"dframe.groupby(['k1','k2']).mean()"
"for name,group in dframe.groupby('k1'):
    print (""This is the %s group"" %name)
    print (group)
    print ('\n')"
"for (k1,k2), group in dframe.groupby(['k1','k2']):
    print (""key1 = %s key2 %s"" %(k1,k2))
    print (group)
    print ('\n')"
group_dict['x']
dframe.groupby('k1')
type(dframe.groupby('k1'))
group_dict_axis1
"dataset2_group=dframe.groupby(['k1','k2'])[['dataset2']]
dataset2_group"
dataset2_group.mean()
dframe1
dframe2
"pd.concat([dframe1, dframe2])"
"pd.concat([dframe1, dframe2], ignore_index=True)"
"pd.concat([dframe1, dframe2], axis = 1)"
blender
dframe.take(blender)
shaker
"hand_grabs=box.take(shaker)
hand_grabs"
"dframe=DataFrame({'city':['Alma', 'Brian Head', 'Fox Park'],
                 'altitude':[3153,3000,2762]})
dframe"
dframe
"! ../scripts/count_fasta.pl \
-i 1000 \
../data-results/Geoduck-transcriptome-v2.transdecoder.pep"
"import warnings
warnings.filterwarnings('ignore')
from IPython.html import widgets
from IPython.display import display"
"radio1 = widgets.RadioButtons(
    description='Color',
    options=['red', 'blue', 'green'])
display(radio1)"
help(widgets.RadioButtons)
"radio2 = widgets.RadioButtons(
    options = {""red"":'r', ""blue"":'b', ""green"":'g'})
display(radio2)"
print(radio2.value)
"# set up some data
import numpy as np

x = np.linspace(0, 4*np.pi, 100)
y = np.sin(x)

#plot a line
from bokeh.plotting import figure
plot = figure()
plot.line(x, y)
show(plot)"
"import numpy as np
from numpy import ma
import matplotlib.pyplot as plt

x = np.arange(1, 7, 0.4)
y0 = np.sin(x)
y = y0.copy() + 2.5

fig = plt.figure(figsize=(10, 10))

plt.step(x, y, label='pre (default)')

y -= 0.5
plt.step(x, y, where='mid', label='mid')

y -= 0.5
plt.step(x, y, where='post', label='post')

y = ma.masked_where((y0 > -0.15) & (y0 < 0.15), y - 0.5)
plt.step(x, y, label='masked (pre)')

plt.legend()

plt.xlim(0, 7)
plt.ylim(-0.5, 4)
"
"text = widgets.Text()
display(text)
def handle_submit(sender):
    print(text.value)
    
text.on_submit(handle_submit)"
"interact(add, x=[0, 10], y=[0, 10])"
"def visualize_binary_classifier():
    while True:
        w = np.random.rand(2)
        w = w/np.linalg.norm(w)/2.
        b = np.random.rand(1)[0]/2.

        X = np.array([-0.5, 2])
        # from solving w.T.dot(x) - b == 0 for x1 (which is Y)
        Y = (-w[0]*X + b)/w[1]
        
        # get a base for the arrow
        x = b/w[1] / 2.
        y = (-w[0]*x + b)/w[1]
        
        # if the axes intercepts aren't within plotting range, generate everything anew
        if b/w[0] >= 2 or b/w[1] >= 2:
            continue
        
        print(""w = (%.2f, %.2f)"" % (w[0], w[1]))
        ax = pp.figure(figsize=(8,8)).add_subplot(1,1,1)
        pp.arrow(x, y, w[0], w[1], head_width=0.05, head_length=0.1, fc='k', ec='k')
        pp.xlim(-0.5, 2)
        pp.ylim(-0.5, 2)
        ax.set_xticks([0], minor=True)
        ax.set_yticks([0], minor=True)
        pp.grid(which=""minor"")

        # plot analytically found decision boundary
        pp.plot(X, Y, color=""black"", lw=1)

        # plot empirical decision boundary
        n = 500
        X = np.array(np.mgrid[-0.5:2:2.5/n, -0.5:2:2.5/n]).reshape(2,n**2)
        Z = np.sign(w.T.dot(X) - b)
        pp.contourf(X[0,:].reshape(n,n), X[1,:].reshape(n,n), Z.reshape(n,n), 1)

        # axes intercepts
        pp.scatter([0, b/w[0]], [b/w[1], 0])

        break
    return

visualize_binary_classifier()"
"def test():
    w = np.random.rand(3)[:,np.newaxis]
    x = np.random.rand(3)[:,np.newaxis]
    print(w.T.dot(w))
    print(np.linalg.norm(w)**2)
    
    for i in range(20):
        p, w = np.random.rand(2)*2. - 1.
        w = np.abs(w)
        print(p, w, p-(w*(p-1))/(p))
test()"
"def plot_point(x):
    pp.scatter(x[0], x[1])
def plot_arrow(x, s=[0, 0]):
    pp.arrow(s[0], s[1], x[0], x[1], head_width=0.05, head_length=0.1, fc='k', ec='k')

def visualize_binary_classifier():
    while True:
        w = np.random.rand(2)
        w = w/np.linalg.norm(w)/2.
        b = np.random.rand(1)[0]/2.

        X = np.array([-0.5, 2])
        # from solving w.T.dot(x) - b == 0 for x1 (which is Y)
        Y = (-w[0]*X + b)/w[1]
        
        # get a base for the arrow
        x = b/w[1] / 2.
        y = (-w[0]*x + b)/w[1]
        
        # if the axes intercepts aren't within plotting range, generate everything anew
        if b/w[0] >= 2 or b/w[1] >= 2:
            continue
        
        print(""w = (%.2f, %.2f)"" % (w[0], w[1]))
        ax = pp.figure(figsize=(8,8)).add_subplot(1,1,1)
#         pp.arrow(x, y, w[0], w[1], head_width=0.05, head_length=0.1, fc='k', ec='k')
        plot_arrow(w, [x,y])
        pp.xlim(-0.5, 2)
        pp.ylim(-0.5, 2)
        ax.set_xticks([0], minor=True)
        ax.set_yticks([0], minor=True)
        pp.grid(which=""minor"")

        # plot analytically found decision boundary
        pp.plot(X, Y, color=""black"", lw=1)

        # plot empirical decision boundary
        n = 500
        X = np.array(np.mgrid[-0.5:2:2.5/n, -0.5:2:2.5/n]).reshape(2,n**2)
        Z = np.sign(w.T.dot(X) - b)
        pp.contourf(X[0,:].reshape(n,n), X[1,:].reshape(n,n), Z.reshape(n,n), 1)

        # axes intercepts
        pp.scatter([0, b/w[0]], [b/w[1], 0])

        break
    
    basepoint = np.array([x,y])
    # choose closest datapoint x
    xc = np.array([1.5, 1.0])
    plot_point(xc)
    # project on w
    xTw = xc.T.dot(w)
    proj = (xTw * w)/(np.linalg.norm(w)**2)
    plot_point(proj)
    plot_arrow(proj)
    # calc distance
    a = b/xTw
    plot_arrow(proj*a)
    
    print(xTw/np.linalg.norm(w))
    print(np.linalg.norm(proj))
    
    return

visualize_binary_classifier()"
"import warnings
warnings.filterwarnings('ignore')
import numpy as np
import matplotlib.pyplot as plt
import scipy


w0 = np.linspace(-1,1,20)
w1 = np.linspace(1,-1,20)
b = np.linspace(0,3,10)

x1 = np.linspace(-6,6,3)
x2 = np.linspace(-6,6,3)

def classify(w0,w1,b,x1,x2):
    x01 = np.divide(-1.0*(np.dot(w1,x2)+b),(w0))
    x02 = np.divide(-1.0*(np.dot(w0,x1)+b),(w1))
    return x01, x02

# w0 in [-1,1], w1 in {0.5,1}, b = 0
plt.figure(figsize=(6,6))
for i in range(len(w0)):
    x01t, x02t = classify(w0[i],0.5,0,x1,x2)
    plt.plot(x01t,x02t,color = 'g',label='w005',linewidth=(0.5+2*i/len(w1)))
    if i==1:
        print(x01t)
        print(x02t)
    x01t, x02t = classify(w0[i],1,0,x1,x2)
    plt.plot(x01t,x02t,color = 'r',label='w01',linewidth=(0.5+2*i/len(w1)))
plt.grid()
plt.xlim((-6,6))
plt.ylim((-6,6))
plt.xlabel('$x_0$')
plt.ylabel('$x_1$')
plt.legend(['$w_1=0.5$','$w_1=1$'])
plt.title('$w_0 \in [-1,1], w_1 \in \{0.5,1\}, b = 0$')
plt.show()


# w0 in {-1,1}, w1 in [-1,1], b = 1
plt.figure(figsize=(6,6))
for i in range(len(w0)):
    x01t, x02t = classify(-1,w1[i],1,x1,x2)
    plt.plot(x01t,x02t,color = 'g',label='w1neg',linewidth=(0.5+2*i/len(w1)))
    x01t, x02t = classify(1,w1[i],1,x1,x2)
    plt.plot(x01t,x02t,color = 'r',label='w1pos',linewidth=(0.5+2*i/len(w1)))
plt.grid()
plt.xlim((-6,6))
plt.ylim((-6,6))
plt.xlabel('$x_0$')
plt.ylabel('$x_1$')
plt.legend(['$w_0=-1$','$w_0=1$'])
plt.title('$w_0 \in \{-1,1\}, w_1 \in [-1,1], b = 0$')
plt.show()


# w0 = 0.5, w1 = 1, b in [0,3]
plt.figure(figsize=(6,6))
for i in range(len(b)):
    x01t, x02t = classify(0.5,2,b[i],x1,x2)
    plt.plot(x01t,x02t,color = 'g',linewidth=(0.5+2*i/len(b)),label='w11')
    x01t, x02t = classify(1,2,b[i],x1,x2)
    plt.plot(x01t,x02t,color = 'r',linewidth=(0.5+2*i/len(b)),label='w01')
plt.grid()
plt.xlim((-6,6))
plt.ylim((-6,6))
plt.xlabel('$x_0$')
plt.ylabel('$x_1$')
plt.legend(['$w_0=0.5$','$w_0=1$'])
plt.title('$w_0 \in \{0.5,1\}, w_1 = 2, b \in [0,3]$')
plt.show()

"
"def scatter(data, color=""dodgerblue"", markers = None):
    pp.scatter(data[:,0], data[:,1], s=50, c=color, edgecolor=""white"")
    return

def newplot(i=1, title=""""):
    if (i == 1):
        pp.figure(figsize=(16,8))
    pp.subplot(1,2,i)
    if title != """":
        pp.title(title)
    pp.axis('equal')
    pp.xticks([0, 1])
    pp.yticks([0, 1])
    pp.xlim([-1, 2])
    pp.ylim([-1, 2])
    pp.grid()
    return

def grid(min_x, max_x, n=100):
    line = np.linspace(min_x, max_x, n)
    X = np.array(np.meshgrid(line, line))  # (dims, n, n)
    Xpoints = np.vstack([X[0,:,:].reshape(n**2), X[1,:,:].reshape(n**2)]).T  # (n*n, dims)
    return X, Xpoints

def gen_data(n=80, std=np.sqrt(0.1)):
    n = int(n/2)
    X1 = np.vstack([np.random.normal(0, std, n), np.random.normal(1, std, n)])
    X2 = np.vstack([np.random.normal(1, std, n), np.random.normal(0, std, n)])

    X3 = np.vstack([np.random.normal(0, std, n), np.random.normal(0, std, n)])
    X4 = np.vstack([np.random.normal(1, std, n), np.random.normal(1, std, n)])

    newplot(1, str(n) + "" seperate points from N1 to N4 each"")
    for X in [X1, X2]:
        scatter(X.T)
    for X in [X3, X4]:
        scatter(X.T, ""red"")

    newplot(2, str(n) + "" points from p1, p2 each"")
    Xp1 = np.hstack([X1, X2]).T
    Xp1 = Xp1[np.random.choice(n*2, n), :]
    # scatter(Xp1)
    Xp2 = np.hstack([X3, X4]).T
    Xp2 = Xp2[np.random.choice(n*2, n), :]
    # scatter(Xp2, ""red"")

    Y = np.hstack([-np.ones(n), np.ones(n)])
    data = np.vstack([np.hstack([Xp1.T, Xp2.T]), Y]).T
    scatter(data[data[:,2]==-1])
    scatter(data[data[:,2]==1], ""red"")
    print(data.shape, ""data"")
    return data

dataset = gen_data(80)
testset = gen_data(80)"
"def ex93a(dataset=dataset, testset=testset):
    Ytrain = dataset[:,2]
    Xtrain = dataset[:,:2]
    Xtest = testset[:,:2]
    Ytest = testset[:,2]
    
    # (a) TRAIN MODEL
    # 1/2 = (default gamma) != (datagen gamma) = 1/(2(std**2)) = 1/0.2 = 5
    # i.e. you should get better results with '-g 5'
    m = svm_train(Ytrain.tolist(), Xtrain.tolist(), '-t 2')
    
    # (b) PREDICT LABELS
    print(""predicting Xtest labels"")
    p_labs, p_acc, p_vals = svm_predict(Ytest.tolist(), Xtest.tolist(), m)
    print(""accuracy: %f \nmean square error: %f \nsquared correlation coefficient: %f"" % p_acc)
    labels = np.array(p_labs)

    # (c) PLOT RESULTS
    # calc decision boundary
    n = 300
    X, X_ = grid(-1.2, 2.2, n)
    # predicting labels for decision boundary
    p_labs, p_acc, p_vals = svm_predict([0]*(n*n), X_.tolist(), m, ""-q"")
    grid_labels = np.array(p_labs).reshape((n,n))
    
    newplot(1, ""decision boundary and training data"")
    scatter(Xtrain[Ytrain==-1.0,:], ""dodgerblue"")
    scatter(Xtrain[Ytrain==1.0,:], ""red"")
    pp.contourf(X[0,:,:], X[1,:,:], grid_labels, alpha=0.3, levels=[-1, 0, 1.])
    
    newplot(2, ""decision boundary and test data"")
    scatter(Xtest[Ytest==-1.0,:2], ""dodgerblue"")
    scatter(Xtest[Ytest==1.0,:2], ""red"")
    pp.contourf(X[0,:,:], X[1,:,:], grid_labels, alpha=0.3, levels=[-1, 0, 1.])
    return

ex93a()"
"def newplotCG(i=1, title=""""):
    if (i == 1):
        pp.figure(figsize=(16,8))
    pp.subplot(1,2,i)
    if title != """":
        pp.title(title)
    pp.axis('equal')
    cgrid = np.arange(-5,17,2)
    ggrid = np.arange(-15,5,2)
    pp.yticks(cgrid)
    pp.xticks(ggrid)
    pp.ylabel(""lg(c)"")
    pp.xlabel(""lg(gamma)"")
    return

def gridsearch(dataset=dataset, testset=testset, simple=False):
    # param simple: False -> use mean square error, True -> use simple accuracy
    
    # used to test final parameters determined by crossvalidation
    Xtest = testset[:,:2].tolist()
    Ytest = testset[:,2].tolist()
    
    Cs = 2**np.arange(-5,17,2).astype(float)  # complexity punishment
    Gs = 2**np.arange(-15,5,2).astype(float)  # tries to find gamma* = 1/(2*std**2)
    c_best = 0
    g_best = 0
    measure_best = 0 if simple else np.inf
    
    k = 5  # number of partitions in k-fold-crossvalidation
    n = int(len(dataset)/k)  # number of elements per partition
    data_parts = [dataset[i*n:i*n+n] for i in range(0, k)]
    
    m = None  # model
    Z = np.zeros((len(Cs), len(Gs), 2)).astype(float)  # for contour plot

    # (a) GRIDSEARCH (= try all combinations)
    for ci,c in enumerate(Cs):
        for gi,g in enumerate(Gs):
            
            # crossvalidation
            # curiously, if you use libsvms builtin crossvalidation (""-v n""), you get much worse results
            measure = []
            for i,current in enumerate(data_parts):
            
                # train on parts\current
                data = np.vstack([part for j,part in enumerate(data_parts) if j != i])
                X = data[:,:2].tolist()
                Y = data[:,2].tolist()
                m = svm_train(Y, X, '-t 2 -c %f -g %f -q' % (c, g))
                
                # validate on current
                X = current[:,:2].tolist()
                Y = current[:,2].tolist()
                p_labs, p_acc, p_vals = svm_predict(Y, X, m, ""-q"")
                
                if simple: measure.append(p_acc[0])  # accuracy (classified x/N correctly)
                else: measure.append(p_acc[1])  # mean square error
            measure_mean = np.array(measure).mean()
            
            # test on testset
            m = svm_train(dataset[:,2].tolist(), dataset[:,:2].tolist(), '-t 2 -c %f -g %f -q' % (c, g))
            p_labs, p_acc, p_vals = svm_predict(Ytest, Xtest, m, ""-q"")
            if simple: Z[ci,gi,:] = (measure_mean, p_acc[0])
            else: Z[ci,gi,:] = (measure_mean, p_acc[1])

            if simple and (measure_mean > measure_best):
                c_best = c
                g_best = g
                measure_best = measure_mean
                print(""CV accuracy: "" + str(measure_mean))
            elif not simple and (measure_mean < measure_best):
                c_best = c
                g_best = g
                measure_best = measure_mean
                print(""CV square error mean: "" + str(measure_mean))
                
    print(""best parameters: C = %f, g = %f"" % (c_best, g_best))
        
    # (b) PLOT ACCURACY
    cgrid = np.arange(-5,17,2)
    ggrid = np.arange(-15,5,2)
    newplotCG(1, ""crossvalidation %s"" % (""square error mean"" if not simple else ""accuracy""))
    pp.contour(ggrid, cgrid, Z[:,:,0], origin=""upper"")
    newplotCG(2, ""testset %s""  % (""square error mean"" if not simple else ""accuracy""))
    pp.contour(ggrid, cgrid, Z[:,:,1], origin=""upper"")
    
    # (c) TRAIN ON OPTIMAL VALUES, USING COMPLETE TRAINING SET
    m = svm_train(dataset[:,2].tolist(), dataset[:,:2].tolist(), '-t 2 -c %f -g %f' % (c_best, g_best))
    p_labs, p_acc, p_vals = svm_predict(Ytest, Xtest, m, ""-q"")
    print(""(c) Testset results:\n  accuracy: %f \n  mean square error: %f \n  square correlation coeff: %f"" % p_acc)
    
    # (d) PLOT RESULTS AS 93c
    Xtrain = dataset[:,:2]
    Ytrain = dataset[:,2]
    
    n = 300
    X, X_ = grid(-1.2, 2.2, n)
    # predicting labels for decision boundary
    p_labs, p_acc, p_vals = svm_predict([0]*(n*n), X_.tolist(), m, ""-q"")
    grid_labels = np.array(p_labs).reshape((n,n))
    
    newplot(1, ""decision boundary and training data"")
    scatter(Xtrain[Ytrain==-1.0,:], ""dodgerblue"")
    scatter(Xtrain[Ytrain==1.0,:], ""red"")
    pp.contourf(X[0,:,:], X[1,:,:], grid_labels, alpha=0.3, levels=[-1, 0, 1.])
    
    newplot(2, ""decision boundary and test data"")
    Xtest = np.array(Xtest)
    Ytest = np.array(Ytest)
    scatter(Xtest[Ytest==-1.0,:2], ""dodgerblue"")
    scatter(Xtest[Ytest==1.0,:2], ""red"")
    pp.contourf(X[0,:,:], X[1,:,:], grid_labels, alpha=0.3, levels=[-1, 0, 1.])
    return
    
    return c_best, g_best, acc_best
gridsearch(simple=False)
print("""")
print(""repeating 9.3 for comparison"")
ex93a()
pp.suptitle(""results from 9.3(c)"", fontsize=24)"
"import warnings
warnings.filterwarnings('ignore')
%matplotlib inline
import numpy as np
import math
import matplotlib.pyplot as plt
import random as rd

######### CHOOSE TASKS ##########
solve_a = 1
solve_b = 1
solve_c = 0
#################################

xa = np.matrix([-1.0,0.3,2]).T
ta = np.matrix([-0.1,0.5,0.5]).T

w_initial = np.matrix([rd.random()-0.5,rd.random()-0.5]).T

def output(w,xa):
    X = np.concatenate((np.matrix(np.ones(len(xa))*1.0),xa.T),axis=0)
    y = np.dot(w.T,X)
    return y[0]

def sumsquerror(w,xa,ta):
    arg = output(w,xa)-ta.T
    err = 0.5*np.dot(arg,arg.T)
    return err[0]

def gradient(w,xa,ta):
    X = np.concatenate((np.matrix(np.ones(len(xa))*1.0),xa.T),axis=0)
    H = np.dot(X,np.transpose(X))
    b = -1.0*np.dot(X,ta)
    g = np.dot(H,w)+b
    return g

def alpha_line_search(grad,xa):
    X = np.concatenate((np.matrix(np.ones(len(xa))*1.0),xa.T),axis=0)
    H = np.dot(X,np.transpose(X))
    anum = np.dot(np.transpose(grad),grad)
    Hgrad = np.dot(H,grad)
    aden = np.dot(np.transpose(grad),Hgrad)
    alpha = -1.0 * np.divide( anum , aden )
    return alpha

def alpha_conjugate_gradient(d,grad,xa):
    X = np.concatenate((np.matrix(np.ones(len(xa))*1.0),xa.T),axis=0)
    H = np.dot(X,np.transpose(X))
    anum = np.dot(np.transpose(d),grad)
    Hgrad = np.dot(H,grad)
    aden = np.dot(np.transpose(d),Hgrad)
    alpha = -1.0 * np.divide( anum , aden )
    return alpha

def beta_conjugate_gradient(grad,grad_old):
    bnum = np.dot(np.transpose(grad),grad)
    bden = np.dot(np.transpose(grad_old),grad_old)
    beta = -1.0 * np.divide( bnum , bden )
    return beta

def grad_update_conjugate_gradient(xa,w):
    X = np.concatenate((np.matrix(np.ones(len(xa))*1.0),xa.T),axis=0)
    H = np.dot(X,np.transpose(X))
    grad = np.dot(H,w)
    return grad

def d_update_conjugate_gradient(grad,beta,d):
    d_new = grad + np.multiply(beta,d)
    return d_new

################################################################################
##### a) gradient descent
if solve_a:

    w = w_initial
    n_iter = 0
    iter_max = 200
    Error1 = np.array(np.zeros(iter_max))
    y1 = np.array(np.zeros((iter_max,3)))
    w01 = np.array(np.zeros(iter_max))
    w11 = np.array(np.zeros(iter_max))
    eta = 0.02

    while n_iter < iter_max:
        w01[n_iter] = w[0]
        w11[n_iter] = w[1]
        y1[n_iter] = output(w,xa)
        Error1[n_iter] = sumsquerror(w,xa,ta)
        grad = gradient(w,xa,ta)
        w = w-eta*grad
        n_iter = n_iter+1


    Error1 = np.delete(Error1,0)
    y1 = np.delete(y1,0)

    plt.plot(Error1)
    plt.xlabel('iterations')
    plt.ylabel('sum-of-squares error')
    plt.grid()
    plt.show()

    plt.scatter(w01,w11)
    plt.xlabel('w0')
    plt.ylabel('w1')
    plt.title('a) gradient descent')
    plt.grid()
    plt.show()
    print ('a - final error value:')
    print (Error1[len(Error1)-1])


################################################################################
##### b) line search
if solve_b:

    w = w_initial
    n_iter = 0
    iter_max = 200
    Error2 = np.array(np.zeros(iter_max))
    y2 = np.array(np.zeros((iter_max,3)))
    w02 = np.array(np.zeros(iter_max))
    w12 = np.array(np.zeros(iter_max))

    while n_iter < iter_max:
        w02[n_iter] = w[0]
        w12[n_iter] = w[1]
        y2[n_iter] = output(w,xa)
        Error2[n_iter] = sumsquerror(w,xa,ta)
        grad = gradient(w,xa,ta)
        alpha = alpha_line_search(grad,xa)
        w = w + np.multiply(alpha,grad)
        n_iter = n_iter+1


    Error2 = np.delete(Error2,0)
    y2 = np.delete(y2,0)

    plt.plot(Error2)
    plt.xlabel('iterations')
    plt.ylabel('sum-of-squares error')
    plt.grid()
    plt.show()

    plt.scatter(w02,w12)
    plt.xlabel('w0')
    plt.ylabel('w1')
    plt.title('b) line search')
    plt.grid()
    plt.show()
    print ('b - final error value:')
    #print (Error2[len(Error1)-1])
    print (Error2[11])

    
################################################################################
##### c) conjugate gradient
if solve_c:

    grad = gradient(w_initial,xa,ta)
    w = -1*grad
    d = w
    n_iter = 0
    iter_max = 2000
    Error3 = np.array(np.zeros(iter_max))
    y3 = np.array(np.zeros((iter_max,3)))
    w03 = np.array(np.zeros(iter_max))
    w13 = np.array(np.zeros(iter_max))
    #w03all = w03
    #w13all = w13
    E_threshold = 0.3

    while n_iter < iter_max:
        w03[n_iter] = w[0]
        w13[n_iter] = w[1]
        #w03all[n_iter] = w[0]
        #w13all[n_iter] = w[1]

        n_itit = 0
        while n_itit < 2:
            y = output(w,xa)
            E = sumsquerror(w,xa,ta)
            alpha = alpha_conjugate_gradient(d,grad,xa)
            w = w + np.multiply(alpha,d)
            n_itit = n_itit+1

        y3[n_iter] = y
        Error3[n_iter] = E
        grad_old = grad
        grad = gradient(w,xa,ta)
        beta = beta_conjugate_gradient(grad,grad_old)
        d = d_update_conjugate_gradient(grad,beta,d)
        n_iter = n_iter+1


    Error3 = np.delete(Error3,0)
    y3 = np.delete(y3,0)

    plt.plot(Error3)
    plt.xlabel('iterations')
    plt.ylabel('sum-of-squares error')
    plt.grid()
    plt.show()

    plt.scatter(w03,w13)
    plt.xlabel('w0')
    plt.ylabel('w1')
    plt.title('c) conjugate gradient')
    plt.grid()
    plt.show()
"
"def sample(mu1, mu2, sigma, N):
    n = int(N/2)
    X1 = random.multivariate_normal(mu1, sigma, n)
    X2 = random.multivariate_normal(mu2, sigma, n)
    Y1 = -ones(n)
    Y2 = ones(n)
    X = vstack([X1, X2])
    Y = hstack([Y1, Y2])
    return X, Y

def ex72():
    mu1 = (0,1)
    mu2 = (1,0)
    sigma = 2*eye(2)

    T = [] # result table
    for N in [2, 4, 6, 8, 10, 20, 40, 100]:
        trials = []
        for i in range(50):
            # 1. generate (N,d) samples, (N) labels
            X, Y = sample(mu1, mu2, sigma, N)

            # 2. find weights
            # w = linalg.pinv(X).dot(Y)
            w = linalg.inv(X.T.dot(X)).dot(X.T).dot(Y)

            # 3. predict 1000 new data points
            X_test, Y_test = sample(mu1, mu2, sigma, 1000)
            Y_predicted = sign(w.dot(X.T))
            Y_test_predicted = sign(w.dot(X_test.T))

            # 4. calc %(correct classifications) for X and X_test
            r = sum(Y_predicted == Y)/len(Y)
            r_test = sum(Y_test_predicted == Y_test)/len(Y_test)

            trials.append([N, w[0], w[1], r, r_test])
        T.append(trials)
    T = array(T) # (#Ns=8, #trials=50, #values=5)-array

    pp.figure(figsize=(18,6))
    pp.subplot(2,2,1)
    pp.plot(T[:,0,0], T[:,:,1].mean(1), label=""w0"")
    pp.plot(T[:,0,0], T[:,:,2].mean(1), label=""w1"")
    pp.title(""w mean"")
    pp.legend(loc=""best"")
    pp.xticks(T[:,0,0])
    pp.grid()
    
    pp.subplot(2,2,2)
    pp.plot(T[:,0,0], T[:,:,3].mean(1), label=""r_train"")
    pp.plot(T[:,0,0], T[:,:,4].mean(1), label=""r_test"")
    pp.title(""r mean"")
    pp.legend(loc=""best"")
    pp.xticks(T[:,0,0])
    pp.grid()

    pp.subplot(2,2,3)
    pp.plot(T[:,0,0], T[:,:,1].std(1), label=""w0"")
    pp.plot(T[:,0,0], T[:,:,2].std(1), label=""w1"")
    pp.title(""w std"")
    pp.legend(loc=""best"")
    pp.xticks(T[:,0,0])
    pp.grid()

    pp.subplot(2,2,4)
    pp.plot(T[:,0,0], T[:,:,3].std(1), label=""r_train"")
    pp.plot(T[:,0,0], T[:,:,4].std(1), label=""r_test"")
    pp.title(""r std"")
    pp.legend(loc=""best"")
    pp.xticks(T[:,0,0])
    pp.grid()
    
    return

ex72()"
"from scipy.misc import comb as nCk

def f_binomial(k,n,p): # k successes in n tries, each with probability p
    return nCk(n,k) * (p**k) * ((1-p)**(n-k))

def ex73a():
    pp.figure(figsize=(16,8))
    pp.suptitle(""binomial distribution"", fontsize=24)
    index = 1
    ps = 3
    ns = array([2,4,16])
    for n in ns:
        for p in linspace(0, 1, ps+2)[1:-1]:
            Y = []
            
            for k in range(n+1):
                Y.append(f_binomial(k,n,p))
            
            pp.subplot(len(ns), ps, index)
            pp.bar(arange(n+1)-0.25, Y, 0.5)
            pp.title(""n=%i, p=%f"" % (n, p))
            pp.xticks(range(n+1))
            pp.xlabel(""k"")
            index += 1
    return
ex73a()
"
"def f_normal(x,mu,sigma):
    return (exp(-((x-mu)**2)/(2*sigma**2))) / (sigma*sqrt(2*pi))

def ex73b():
    pp.figure(figsize=(8,4))
    pp.suptitle(""normal distribution"", fontsize=24)
    index = 1
    
    mu = 0
    for sigma in [2,4,8]:
        X = linspace(-40, 40, 100)
        Y = f_normal(X,mu,sigma)

        pp.plot(X, Y, label=""sigma=%s"" % str(sigma))
        index += 1
    pp.legend()
    return
# ex73b()

def ex73bb(): # draw the normal over the binomial distribution
    pp.figure(figsize=(16,16))
    
    i = 0
    ns = hstack([array([2,4]), (arange(2,6)**3)])
    
    for n in ns:
        for p in linspace(0, 1, 7)[1:-1]:
            pp.subplot(len(ns),5,i+1)
            X = arange(n+1)

            Y_binomial = [f_binomial(x,n,p) for x in X]
            Y_normal = f_normal(X, n*p, sqrt(n*p*(1-p)))

            pp.plot(X, Y_binomial, label=""binomial"")
            pp.plot(X, Y_normal, label=""normal"")
            pp.title(""n=%i, p=%f"" % (n,p))
            pp.yticks([])
            pp.xticks([])
            if (i == 2):
                pp.legend(loc=""lower center"")
            
            i += 1
ex73bb()"
"from scipy.misc import factorial

def f_poisson(k, ld):
    return ((e**(-ld)) * (ld**k)) / (factorial(k))

def ex73bb(): # draw the poisson over the binomial distribution
    pp.figure(figsize=(16,16))
    
    i = 0
    ns = hstack([array([2,4]), (arange(2,6)**3)])
    
    for n in ns:
        for p in linspace(0, 1, 7)[1:-1]:
            pp.subplot(len(ns),5,i+1)
            X = arange(n+1)

            Y_binomial = [f_binomial(x,n,p) for x in X]
            Y_normal = f_poisson(X, n*p)

            pp.plot(X, Y_binomial, label=""binomial"")
            pp.plot(X, Y_normal, label=""normal"")
            pp.title(""n=%i, p=%f"" % (n,p))
            pp.yticks([])
            pp.xticks([])
            if (i == 2):
                pp.legend(loc=""lower center"")
            
            i += 1
ex73bb()"
"print(f_binomial(64,125,0.5), ""binomial"")
print(f_poisson(64,125*0.5), ""poisson"")"
"print(f_binomial(1, 2, 0.5), ""binomial"")
print(f_normal(1, 2*0.5, 2*0.5*(1-0.5)), ""normal"")"
np.degrees(output)
(np.degrees(output) * 2 ) + 90
np.degrees(output)
output
"import warnings
warnings.filterwarnings('ignore')
# we need a system to match two people up for secret santa. 

from random import shuffle

def random_assign(data:list):
    output = {}
    shuffle(data)
    count = 1
    for item in data:            
        if count < len(data):
            output[item] = data[count]
            count += 1
        else:
            output[item] = data[0]
        
    return output
            
def pretty_print(data:dict):
    for key, value in data.items():
        print(""{} gets {} a gift"".format(key, value))
        
# def test():
#     from collections import Counter
#     cnt = Counter()
#     for i in range(0, 2000):
#         data = random_assign(['addi', 'chelsea/quin', 'madeleine', 'hayley/andrew', 'alex'])
#         for key, value in data.items():
#             str_val = ""{} gets {} a gift"".format(key, value)
#             cnt[str_val] += 1
#     print(cnt)
# test()

# for i in range(0, 150):    
#     data = random_assign(['addi', 'chelsea/quin', 'madeleine', 'hayley/andrew', 'alex'])    
#     pretty_print(data)
#     print('\n')"
"corners = np.array([[0,0],[0,3],[1,3],[3,1],[3,3],[4,3],[4,0],[3,0],[1,2],[1,0],[0,0]])
pyplot.plot(corners[:,0],corners[:,1],linewidth=2)
pyplot.xlabel(r""$x$"")
pyplot.ylabel(r""$z$"")
pyplot.axis('equal')
pyplot.ylim(-0.1,3.1);"
"corners = np.array([[0.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 0.0]])
pyplot.plot(corners[:,0],corners[:,1],linewidth=2)
pyplot.xlabel(r""$\xi$"")
pyplot.ylabel(r""$\eta$"")
pyplot.axis('equal')
pyplot.ylim(-0.1,1.1);"
"fig=pyplot.figure()
ax=fig.add_subplot(111)
for e in IEN_1m:
    coords = np.vstack((nodes_1m[e], nodes_1m[e[0]]))
    ax.plot(coords[:,0], coords[:,1],'kx-')
pyplot.xlim(-0.1,4.1)
pyplot.ylim(-0.1,3.1)
pyplot.xlabel(r""$x$"")
pyplot.ylabel(r""$y$"")
pyplot.show();"
"fig=pyplot.figure()
ax=fig.add_subplot(111)
for e in IEN_0_5m:
    coords = np.vstack((nodes_0_5m[e], nodes_0_5m[e[0]]))
    ax.plot(coords[:,0], coords[:,1],'kx-')
pyplot.xlim(-0.1,4.1)
pyplot.ylim(-0.1,3.1)
pyplot.xlabel(r""$x$"")
pyplot.ylabel(r""$y$"")
pyplot.show();"
"fig=pyplot.figure(figsize=(16,8))
ax=fig.add_subplot(111)
eps = d_1m.reshape(len(nodes_1m), 2) *1e5
cm = pyplot.cm.RdBu
for e in IEN_1m[:-1]:
    coords = np.vstack((nodes_1m[e], nodes_1m[e[0]]))
    coords_disp = np.vstack((nodes_1m[e] + eps[e], nodes_1m[e[0]] + eps[e[0]]))
    ax.plot(coords[:, 0], coords[:, 1], 'k--')
    ax.plot(coords_disp[:, 0], coords_disp[:, 1], 'b:')
e = IEN_1m[-1]
coords = np.vstack((nodes_1m[e], nodes_1m[e[0]]))
coords_disp = np.vstack((nodes_1m[e] + eps[e], nodes_1m[e[0]] + eps[e[0]]))
ax.plot(coords[:, 0], coords[:, 1], 'k--', label=""Original geometry"")
ax.plot(coords_disp[:, 0], coords_disp[:, 1], 'b:', label=r""Deformation scaled by $\times 10^5$"")
ax.quiver(nodes_1m[:,0], nodes_1m[:,1], eps[:,0], eps[:, 1], label=""Deformation direction$"")
ax.set_xlim(-0.5, 4.1)
ax.set_ylim(-0.1, 4.1)
ax.set_xlabel(r""$x (m)$"")
ax.set_ylabel(r""$y (m)$"")
ax.set_title(r""Deformation of a coarse ($1m$) mesh, scaled by $10^5$"")
ax.legend()"
"fig=pyplot.figure(figsize=(16,8))
ax=fig.add_subplot(111)
eps = d_0_5m.reshape(len(nodes_0_5m), 2) *1e5
for e in IEN_0_5m[:-1]:
    coords = np.vstack((nodes_0_5m[e], nodes_0_5m[e[0]]))
    coords_disp = np.vstack((nodes_0_5m[e] + eps[e], nodes_0_5m[e[0]] + eps[e[0]]))
    ax.plot(coords[:, 0], coords[:, 1], 'k--')
    ax.plot(coords_disp[:, 0], coords_disp[:, 1], 'b:')
e = IEN_0_5m[-1]
coords = np.vstack((nodes_0_5m[e], nodes_0_5m[e[0]]))
coords_disp = np.vstack((nodes_0_5m[e] + eps[e], nodes_0_5m[e[0]] + eps[e[0]]))
ax.plot(coords[:, 0], coords[:, 1], 'k--', label=""Original geometry"")
ax.plot(coords_disp[:, 0], coords_disp[:, 1], 'b:', label=r""Deformation scaled by $\times 10^5$"")
ax.quiver(nodes_0_5m[:,0], nodes_0_5m[:,1], eps[:,0], eps[:, 1])
ax.set_xlim(-0.5, 4.1)
ax.set_ylim(-0.1, 4.1)
ax.set_xlabel(r""$x (m)$"")
ax.set_ylabel(r""$y (m)$"")
ax.set_title(r""Deformation of a fine ($0.5 m$) mesh"")
ax.legend()"
"pyplot.scatter(nodes[:,0], nodes[:, 1])
E = 4e10
nu = 0.15
K = global_stiffness(E, nu, nodes, IEN, fixed_nodes)"
count_ingredients(df.iloc[0:1])
"[(key, greek_counts[key]) for key in greek_counts.keys()][:10]"
"# create a new figure of size 8x6 points
# try to change those numbers and observe what happens to the aspect ratio and size
plt.figure(figsize=(8,6))

# plot the points in the array of factorial and fibonacci numbers and give them a label
plt.plot(facts, label=""Factorial"")
plt.plot(fibs,  label=""Fibonacci"")

# give a title to the plot
plt.title(""Factorial vs Fibonacci"")

# label the x axis
plt.xlabel(""n"")

# show the legend
plt.legend()

# show the plot
plt.show()"
"plt.figure(figsize=(8,6))

# plot the points in the array of factorial and fibonacci numbers 
# using log-scale on the y-axis (means that we are plotting y=log(f(n)) instead of the f)
plt.semilogy(facts, label=""Factorial"")
plt.semilogy(fibs,  label=""Fibonacci"")

plt.title(""Factorial vs Fibonacci - Log Scale"", fontsize=24)
plt.xlabel(""n"")
plt.legend()
plt.show()"
"plt.figure(figsize=(8,6))

# we create an array of 256 points going from -pi to pi
xs = np.linspace(-np.pi, np.pi, 256, endpoint=True)

# Plot cosine using blue color with a continuous line of width 1 (pixels)
plt.plot(xs, np.cos(xs), color=""blue"", linewidth=1.0, linestyle=""-"")

# Plot sine using green color with a dotted line of width 2 (pixels)
# try the following linestyles: '', ' ', 'None', '--', '-.', '-', ':'
plt.plot(xs, np.sin(xs), color=""green"", linewidth=2.0, linestyle="":"")

# Set x-axis plot limits
plt.xlim(-4.0, 4.0)

# Set position of ticks on the x-axis
plt.xticks(
    np.linspace(-4, 4, 9, endpoint=True)
)

# Set y limits
plt.ylim(-1.1, 1.1)

# Set y ticks
plt.yticks(
    np.linspace(-1, 1, 5, endpoint=True)
)

# Save figure using 72 dots per inch - not needed now
# savefig(""example-1.png"", dpi=72)

plt.show()"
"xs = np.linspace(-5,5,20)
ys = xs**2

# subplots: 1 row, 2 columns, 1st plot
plt.subplot(1,2,1)
plt.plot(xs, ys, 'r--')

# subplots: 1 row, 2 columns, 2nd plot
plt.subplot(1,2,2)
plt.plot(ys, xs, 'g*-')"
"image = np.random.rand(30, 30)
plt.imshow(image, cmap=plt.cm.hot)    
plt.colorbar()
plt.show()"
"plt.matshow(image)
plt.colorbar()
plt.show()"
"plt.figure(figsize=(8,6))
for i in range(8):
    x = np.arange(1000)
    y = np.random.randn(1000).cumsum() # cumulative sums, thanks numpy!
    plt.plot(x, y, label=str(i))
    plt.legend()"
_  = imageplot(image)
"coord = (50, 110)
size  = (60, 60)
subimage = image[coord[0]:coord[0]+size[0], coord[1]:coord[1]+size[1]]

_ = imageplot(subimage)"
"redchannel = image[:, :, 0]

plot = imageplot(redchannel)
plot.set_cmap('gray')"
"xmirrored = image[:, -1:0:-1, :]

_ = imageplot(xmirrored)"
"ymirrored = image[-1:0:-1, :, :]

_ = imageplot(ymirrored)"
"grayimage = np.sum(image, axis = 2) / 3.

plot = imageplot(grayimage)
plot.set_cmap('gray')"
"print(""ave_r_x:"", ave_r_x)
print(""ave_r_y:"", ave_r_y)
print(""V:"", V)
print(""la:"", la)
print(""U:"", U)"
"print(np.average(xi))
print(np.average(eta))"
"V_ = np.zeros((2,2))
V_[0][0] = np.dot(xi, xi.T) / 30
V_[0][1] = np.dot(xi, eta.T) / 30
V_[1][0] = np.dot(eta, xi.T) / 30
V_[1][1] = np.dot(eta, eta.T) / 30
print(V_)"
"from sklearn.decomposition import PCA
pca = PCA(n_components=2)
pca.fit(data)"
"U = pca.components_.T*-1
print(U)"
"plt.figure(figsize=(8,8)) # グラフの縦横比を8inchで固定
# plt.plot(r_x,r_y,""bo"") # 実データを青でプロット
plt.plot(u1_x,u1_y,""-r"") # u1を赤でプロット
plt.plot(u2_x,u2_y,""-r"") # u2を赤でプロット
plt.plot(x_x, x_y, ""go"") # 原点移動したデータを緑でプロット"
"plt.figure(figsize=(12, 6))
plt.plot(xi, eta, ""go"")
plt.plot(np.zeros(60), np.arange(-30,30,1) , ""-r"")
plt.plot(np.arange(-30,30,1), np.zeros(60) , ""-r"")"
"print(np.average(xi))
print(np.average(eta))"
"V_ = np.zeros((2,2))
V_[0][0] = np.dot(xi, xi.T) / 30
V_[0][1] = np.dot(xi, eta.T) / 30
V_[1][0] = np.dot(eta, xi.T) / 30
V_[1][1] = np.dot(eta, eta.T) / 30
print(V_)"
"plt.figure(figsize=(8,8)) # グラフの縦横比を8inchで固定
# plt.plot(r_x,r_y,""bo"") # 実データを青でプロット
plt.plot(u1_x,u1_y,""-r"") # u1を赤でプロット
plt.plot(u2_x,u2_y,""-r"") # u2を赤でプロット
plt.plot(x_x, x_y, ""go"") # 原点移動したデータを緑でプロット"
"plt.figure(figsize=(12, 6))
plt.plot(xi, eta, ""b+"")
plt.plot(np.zeros(60), np.arange(-30,30,1) , ""-r"")
plt.plot(np.arange(-30,30,1), np.zeros(60) , ""-r"")"
"# Feature Importance
from sklearn import datasets
from sklearn import metrics
from sklearn.ensemble import ExtraTreesClassifier

# load the iris datasets
dataset = datasets.load_iris()

# fit an Extra Trees model to the data
model = ExtraTreesClassifier()
model.fit(dataset.data, dataset.target)

# display the relative importance of each attribute
print(model.feature_importances_)"
"# Linear Regression
import numpy as np
from sklearn import datasets
from sklearn.linear_model import LinearRegression

dataset = datasets.load_iris()

model = LinearRegression()
model.fit(dataset.data, dataset.target)
expected = dataset.target
predicted = model.predict(dataset.data)

# summarize the fit of the model
mse = np.mean((predicted-expected)**2)
print(mse)
print(model.score(dataset.data, dataset.target))"
"# Linear Regression
import numpy as np
from sklearn import datasets
from sklearn.linear_model import LinearRegression

dataset = datasets.load_iris()

X_new = SelectKBest(chi2, k=3).fit_transform(X, y)

model = LinearRegression()
model.fit(X_new, dataset.target)
expected = dataset.target
predicted = model.predict(X_new)

# summarize the fit of the model
mse = np.mean((predicted-expected)**2)
print(mse)
print(model.score(X_new, dataset.target))"
iris.data
iris.target
iris.target_names
print(iris.DESCR)
"%matplotlib inline

X = iris.data[:, :2]  # we only take the first two features.
Y = iris.target

# Determine the range of axis
x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5
y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5

plt.figure(2, figsize=(8, 6))
plt.clf()

# Plot the training points
plt.scatter(X[:, 0], X[:, 1], c=Y, cmap=plt.cm.Paired)
plt.xlabel('Sepal length')
plt.ylabel('Sepal width')

plt.xlim(x_min, x_max)
plt.ylim(y_min, y_max)
plt.xticks(())
plt.yticks(())

# To getter a better understanding of interaction of the dimensions
# plot the first three PCA dimensions
fig = plt.figure(1, figsize=(8, 6))
ax = Axes3D(fig, elev=-150, azim=110)
X_reduced = PCA(n_components=3).fit_transform(iris.data)
ax.scatter(X_reduced[:, 0], X_reduced[:, 1], X_reduced[:, 2], c=Y,
           cmap=plt.cm.Paired)
ax.set_title(""First three PCA directions"")
ax.set_xlabel(""1st eigenvector"")
ax.w_xaxis.set_ticklabels([])
ax.set_ylabel(""2nd eigenvector"")
ax.w_yaxis.set_ticklabels([])
ax.set_zlabel(""3rd eigenvector"")
ax.w_zaxis.set_ticklabels([])

plt.show()"
"import warnings
warnings.filterwarnings('ignore')
%matplotlib inline
import numpy as np
import matplotlib.pyplot as plt
plt.rcParams['figure.figsize'] = (20.0, 10.0)
from sklearn import svm, datasets

# import some data to play with
iris = datasets.load_iris()
X = iris.data[:, :2]  # we only take the first two features. We could
                      # avoid this ugly slicing by using a two-dim dataset
y = iris.target

h = .02  # step size in the mesh

# we create an instance of SVM and fit out data. We do not scale our
# data since we want to plot the support vectors
C = 1.0  # SVM regularization parameter
svc = svm.SVC(kernel='linear', C=C).fit(X, y)
rbf_svc = svm.SVC(kernel='rbf', gamma=0.7, C=C).fit(X, y)
poly_svc = svm.SVC(kernel='poly', degree=3, C=C).fit(X, y)
lin_svc = svm.LinearSVC(C=C).fit(X, y)

# create a mesh to plot in
x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1
xx, yy = np.meshgrid(np.arange(x_min, x_max, h),
                     np.arange(y_min, y_max, h))

# title for the plots
titles = ['SVC with linear kernel',
          'LinearSVC (linear kernel)',
          'SVC with RBF kernel',
          'SVC with polynomial (degree 3) kernel']


for i, clf in enumerate((svc, lin_svc, rbf_svc, poly_svc)):
    # Plot the decision boundary. For that, we will assign a color to each
    # point in the mesh [x_min, x_max]x[y_min, y_max].
    plt.subplot(2, 2, i + 1)
    plt.subplots_adjust(wspace=0.4, hspace=0.4)

    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])

    # Put the result into a color plot
    Z = Z.reshape(xx.shape)
    plt.contourf(xx, yy, Z, cmap=plt.cm.Paired, alpha=0.8)

    # Plot also the training points
    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.Paired)
    plt.xlabel('Sepal length')
    plt.ylabel('Sepal width')
    plt.xlim(xx.min(), xx.max())
    plt.ylim(yy.min(), yy.max())
    plt.xticks(())
    plt.yticks(())
    plt.title(titles[i])

plt.show()"
"%matplotlib inline
from sklearn.svm import SVC
from sklearn.datasets import load_digits
from sklearn.feature_selection import RFE
import matplotlib.pyplot as plt
plt.rcParams['figure.figsize'] = (20.0, 10.0)

# Load the digits dataset
digits = load_digits()
X = digits.images.reshape((len(digits.images), -1))
y = digits.target

# Create the RFE object and rank each pixel
svc = SVC(kernel=""linear"", C=1)
rfe = RFE(estimator=svc, n_features_to_select=1, step=1)
rfe.fit(X, y)
ranking = rfe.ranking_.reshape(digits.images[0].shape)

# Plot pixel ranking
plt.matshow(ranking, cmap=plt.cm.Blues)
plt.colorbar()
plt.title(""Ranking of pixels with RFE"")
plt.show()"
"import warnings
warnings.filterwarnings('ignore')
%matplotlib inline

import numpy as np
import matplotlib.pyplot as plt
import scipy.stats"
"Xorig = np.empty(60)
Xorig[0] = 1
Yorig = np.empty(60)
Yorig[0] = observation(Xorig[0], 0)

for t in range(1, 60):
    Xorig[t] = transition(Xorig[t-1], t + 1)
    Yorig[t] = observation(Xorig[t], t + 1)

plt.plot(np.arange(60) + 1, Xorig, '-')
plt.xlabel('Time')
plt.ylabel('X')"
"particles = particle_filter(Yorig)
Xest = particles[0][1:].mean(axis=1)"
"plt.plot(np.arange(60) + 1, Xorig, 'ko')
plt.plot(np.arange(60) + 1, Xest, 'r-')
plt.xlabel('Time')
plt.ylabel('E[X]')"
"unscented_particles = unscented_particle_filter(Yorig)
unscented_Xest = unscented_particles[0][1:].mean(axis=1)"
"fig, ax = plt.subplots()
ax.plot(np.arange(60) + 1, Xorig, 'ko', label='True x')
ax.plot(np.arange(60) + 1, Xest, 'r-', label='PF')
ax.plot(np.arange(60) + 1, unscented_Xest, 'b-', label='UPF')
ax.set_xlabel('Time')
ax.set_ylabel('E[X]')
ax.legend(loc='best')"
"state = 0
history = tuple()
planner.search(0, history)"
"state = 1
history = ((0, 0, 1),)
planner.search(state, history)"
"import warnings
warnings.filterwarnings('ignore')
# Monte-Carlosimulatie - pi benaderen
import numpy as np

n = 1000  # aantal herhalingen experiment
aantal_in_cirkel = 0

for i in range(n):
    # experiment:
    x, y = np.random.uniform(0, 1), np.random.uniform(0, 1)
    in_cirkel = np.sqrt(x**2 + y**2) < 1
    if in_cirkel:
        aantal_in_cirkel += 1

# benadering pi:
print(""Pi ~= {:.8f}"".format(aantal_in_cirkel / n))"
"digits = ""0123456789ABCDEF""
print(digits[0], digits[3], digits[10], digits[15])
for i in range(0, len(digits), 4):
    print(digits[i])"
"
import matplotlib.pyplot as plt
import numpy as np
fig, ax = plt.subplots()
x = np.linspace(0, 10, 200)
y = np.sin(x)
ax.plot(x, y, 'r-', linewidth=2, label='sine function', alpha=0.6)
ax.legend()
plt.show()
"
%time answer_problem_II()
"for idx in zip(range(random.randint(1, 20)), multiples_gen(random.randint(1,100))):
    print(idx)"
%time nth_prime(n=5000)
%time nth_prime(n=1000000)
%timeit count_primes_lt(limit=5000)
%timeit count_primes_lt(limit=200000)
%time count_primes_lt(limit=1000000)
%timeit old_count_primes_lt(5000)
%time old_count_primes_lt(10000)
%time old_count_primes_lt(100000)
%time old_count_primes_lt(200000)
regressor.predict(test_2)
"import warnings
warnings.filterwarnings('ignore')
import pandas as pd

df = pd.read_csv('close_prices.csv')
df.shape"
df.head()
"from sklearn.decomposition import PCA

pca = PCA(n_components=10)
pca.fit(df.iloc[:,1:])"
"res, i = 0, 0
while res < 0.9:
    res += pca.explained_variance_ratio_[i]
    i += 1
(pca.explained_variance_ratio_, res, i)"
"# Напишите свой код
X = pca.transform(df.iloc[:,1:])
df2 = pd.DataFrame(X)
df2.head()"
df2[0]
"import numpy as np
np.corrcoef(df2[0], dj_df['^DJI'])[0][1]"
pca.components_[0]
"tup_comp = list(zip(df.columns[1:], pca.components_[0]))
tup_comp"
"sorted(tup_comp, key=lambda x: abs(x[1]), reverse=True)"
"np.random.seed(100) # Для репродукции

# Your code here
"
"import warnings
warnings.filterwarnings('ignore')
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

plt.style.use('ggplot')

%matplotlib inline"
"# Your code here
plt.scatter(df.y, df.a15)"
"fig, ax = plt.subplots(figsize=(7, 7))
ax.scatter(X[:, 0],
           X[:, 1],
           c=y,
           cmap=plt.cm.Paired)"
"np.random.seed(seed=2016)
df = pd.DataFrame({
    'incidence': np.random.randint(low=2, high=100, size=54),
    'age': np.random.randint(low=2, high=60, size=54),
    'state': [st for st in states for t in range(2)],
    'alert': np.random.randint(low=1, high=5, size=54),
    'w': [i for t in range(27) for i in range(1, 3)]
})

df"
"step = 3500000
x_bound = -9000000 # meters
y_bound = -1500000 # meters

fig = figure(
    tools='pan, wheel_zoom', 
    #x_range=(x_bound, bound+step), 
    #y_range=(y_bound, bound+step),
    height=450, width=900
)
#fig.axis.visible = False
#fig.add_tile(CARTODBPOSITRON)

for fn in glob('../data/estados_2010-*.json'):
    with open(fn) as f:
        geo_source = GeoJSONDataSource(geojson=f.read())
        
        fig.patches(xs='xs', ys='ys', source=geo_source)
        fig.multi_line(
            xs='xs', ys='ys', line_color='white', 
            line_width=2, source=geo_source
        )

show(fig)"
"# GENERAL INFO
π = np.pi
channels = 2
φs = [3, 95]  # phase shift of each channel

f = 25

ns = 680
fs = 680

t = np.linspace(0, ns//fs, ns)

df = pd.DataFrame(np.zeros((ns, channels)), index=t)

for i in range(channels):
    φ = φs[i]
    title = 'Phase of channel %s: %s (deg)' % (i, φ)
    df[i] = sine_signal(t, A=1, f=f, φ=φ)
    df[i].plot(title=title, figsize=(12, 2))
    plt.show()"
"_df = df.iloc[1:, :].copy()

phase_diff = np.arccos(
    np.dot(_df[0], _df[1])/(
        np.linalg.norm(_df[0])*np.linalg.norm(_df[1])
    )
)*180/π

if np.isnan(phase_diff):
    phase_diff = 0.0

print(phase_diff)"
"# GENERAL INFO
π = np.pi
channels = 2
φs = [3, 95]  # phase shift of each channel

f = 25

ns = 680
fs = 680

t = np.linspace(0, ns//fs, ns)

df = pd.DataFrame(np.zeros((ns, channels)), index=t)

for i in range(channels):
    φ = φs[i]
    title = 'Phase of channel %s: %s (deg)' % (i, φ)
    df[i] = sine_signal(t, A=1, f=f, φ=φ)
    df[i].plot(title=title, figsize=(12, 2))
    plt.show()"
"# need to remove the final term
# the 1st term here could be confused, but the 
# phase diff between the 2 signal will be right too
_df = df.iloc[:-1].copy()

xdft_1 = fft(_df[0])
xdft_2 = fft(_df[1])

phase_shift_1 = np.angle(xdft_1[f])+π/2
phase_shift_2 = np.angle(xdft_2[f])+π/2

phase_shift_1*180/π, phase_shift_2*180/π"
"x = np.array([0, 1, 2, 3, 4])
y = np.array([3, 6, 7, 8, 11])

w0, w1 = linear_regression(x, y)
print('w0:', w0, 'w1:', w1)"
"plot_base(df)
# plot_lines([0, 10, 50, 0], [0, 50, 10, 0])
plt.show()"
"routes = [[]]
n_routes = 1
n_points = df.shape[0] - 1

_gain = df_gain.copy()

i = 0
while True:
    i += 1
    print('=' * 80)
    print('ITERATION #%s' % i)
    print('=' * 80, end='\n\n')
    
    k1, k2 = get_next_pair(_gain, drop=True)
    print('pair:', k1, k2)
    routes = add_node(routes, k1, k2)
    
    plot_base(df)
    for j, route in enumerate(routes):
        print('route #%s' % j)
        print(route)
        connect_points(df, route)
    plt.grid(True)
    plt.show()
    
    if check_all_points_satisfied(routes, n_points):
        break"
"# initial basic solution with northwest corner
values_text = '''
0,1,2
0,,,
1,,,
2,,,
'''
df_initial = pd.read_csv(io.StringIO(values_text))

_capacity = list(capacity)
_absortion = list(absortion)

# iter by row
print(
    '#ID'.ljust(5, ' '), 
    'CAPACITY'.ljust(15, ' '), 
    'ABSORTION'.ljust(15, ' ')
)

ii = 1

print(
    ('#%s' % ii).ljust(5, ' '), 
    str(capacity).ljust(15, ' '), 
    str(absortion).ljust(15, ' ')
)

i = 0
while i < df_initial.shape[0]:
    # iter by column
    j = 0
    while j < df_initial.shape[1]:
        ii += 1
        r = _capacity[i] - _absortion[j]
        
        if r > 0:
            df_initial.iloc[i, j] = _absortion[j]
            _absortion[j] = 0
            _capacity[i] = r
        elif r < 0:
            df_initial.iloc[i+0, j] = _capacity[i]
            _capacity[i] = 0
            _absortion[j] = abs(r)
        else:
            df_initial.iloc[i+0, j] = _capacity[i]
            _capacity[i] = 0
            _absortion[j] = 0
            
        print(
            ('#%s' % ii).ljust(5, ' '), 
            str(_capacity).ljust(15, ' '), 
            str(_absortion).ljust(15, ' ')
        )
        
        if _capacity[i] == 0:
            break
        j += 1
    i += 1

df_initial = df_initial.replace(0, np.nan)
df_initial"
"df = df_initial.copy()
# calculate the cost
z = Z(df)
zs = [z]

# check the result
assert z == 105*7+ 50*12 + 120*10 + 130*11
print('Z =', z)

# check
print('\ninitial table')
df = df_initial.copy()
display(df)

it_n = 0
optimo = False

while not optimo:
    it_n += 1
    print('\n' + '#'*80)
    print('# Iteraction #:', it_n)
    print('#'*80 + '\n')
    df = fix_table(df)
    # df.iloc[1, 2] = 10**-10
    print('>>', 'table fixed')
    display(df)

    n, m = df.shape

    '''
    ## Calculation of multipliers

    ### basic variables

    $u_i + v_j = c_{ij}$

    where

    $u_0 = 0$

    '''

    basic_variables = calc_basic_variables(df, df_cost, verbose=True)

    multipliers = calc_multipliers(df, basic_variables, verbose=True)

    '''
    ### No basic variable

    $P_{ij} = c_{ij} - u_i - v_j$

    '''
    try:
        optimo, no_basic_variables = calc_no_basic_variables(
            df_cost, basic_variables, multipliers, verbose=True
        )
    except:
        break
    
    if optimo:
        break

    φ_id = get_minor_no_basic(no_basic_variables, verbose=True)

    ii, jj = map(int, φ_id.split('_')[1].split(','))
    print('>>', 'ii', 'jj')
    print(ii, jj)

    _r_used, _c_used = get_candidate_nodes(df, ii, jj, verbose=True)
    
    print('>>', '_c_used', '_r_used')
    print(_c_used)
    print(_r_used)

    df = rearange_table(df, _r_used, _c_used, ii, jj, verbose=True)
    zs.append(Z(df))
print('\n\n' + '#'*80)
print('# RESULTS')
print('#'*80)

for i, v in enumerate(zs):
    print('>> %s:' % i, v)
    
print('\n>> final answer:')
display(df.fillna(''))"
"initial_datetime = datetime.now()

df_chegada = pd.DataFrame({
    'entrada': sampling_generator(
        initial_datetime=initial_datetime
    )
}).sort_values(by='entrada').reset_index(drop=True)

df_saida = pd.DataFrame({
    'saída': sampling_generator(
        initial_datetime=initial_datetime
    )
}).sort_values(by='saída').reset_index(drop=True)

""""""
fig, ax = plt.subplots(1, 2)
df_chegada.hist(ax=ax[0])

df_saida.hist(ax=ax[1])
plt.show()
""""""
print('tables head')
display(df_chegada.head())
display(df_saida.head())"
initial_datetime.strftime('%d de %b de %Y - %l:%M%p')
"test = 100000
(1 - sum([sum(map(confronto, zip(mazzo(3),mazzo_casuale(10))))>0 for _ in range(test)])/test)*100"
"!{bbin}blastn \
-task blastn \
-evalue 1e-5 \
-outfmt 6 \
-num_threads 3 \
-query ../data/lncRNA-mytilus-CLC.fa \
-db analyses/transrate-Cgigas-pep/Mytilus-SXT.trimmed.trinity.v3/Mytilus-SXT.trimmed.trinity.v3 \
-out analyses/lncRNA-CLC-blastn-Trinity-v3-05.out"
!wc -l analyses/lncRNA-CLC-blastn-Trinity-v3-05.out
"!{bbin}blastn \
-task blastn \
-evalue 1e-60 \
-outfmt 6 \
-num_threads 3 \
-query ../data/lncRNA-mytilus-CLC.fa \
-db analyses/transrate-Cgigas-pep/Mytilus-SXT.trimmed.trinity.v3/Mytilus-SXT.trimmed.trinity.v3 \
-out analyses/lncRNA-CLC-blastn-Trinity-v3-60.out"
!wc -l analyses/lncRNA-CLC-blastn-Trinity-v3-60.out
"!{bbin}makeblastdb \
-in /Volumes/Monarch/trilo/Crassostrea_gigas.GCA_000297895.1.30.dna_sm.toplevel.fa \
-out /Volumes/Monarch/trilo/Crassostrea_gigas.GCA_000297895.1.30.dna_sm.toplevel \
-dbtype nucl"
"!{bbin}blastn \
-task blastn \
-evalue 1e-60 \
-outfmt 6 \
-num_threads 3 \
-query ../data/lncRNA-mytilus-CLC.fa \
-db /Volumes/Monarch/trilo/Crassostrea_gigas.GCA_000297895.1.30.dna_sm.toplevel \
-out analyses/lncRNA-CLC-blastn-Cgigas-genome-60.out"
"!{bbin}blastn \
-task blastn \
-evalue 1e-100 \
-outfmt 6 \
-num_threads 3 \
-query ../data/lncRNA-mytilus-CLC.fa \
-db analyses/transrate-Cgigas-pep/Mytilus-SXT.trimmed.trinity.v3/Mytilus-SXT.trimmed.trinity.v3 \
-out analyses/lncRNA-CLC-blastn-Trinity-v3-100.out"
"!{bbin}blastn -help
"
!wc -l analyses/lncRNA-CLC-blastn-Cgigas-genome-40-dc-mega.out
"!{bbin}blastn \
-task blastn \
-evalue 1e-20 \
-outfmt 6 \
-word_size 10 \
-gapopen 0 \
-gapextend 4 \
-num_threads 4 \
-query ../data/lncRNA-mytilus-CLC.fa \
-db /Volumes/Monarch/trilo/Crassostrea_gigas.GCA_000297895.1.30.dna_sm.toplevel \
-out analyses/lncRNA-CLC-blastn-Cgigas-genome-20-0-blastn.out"
!wc -l analyses/lncRNA-CLC-blastn-Cgigas-genome-20-0-blastn.out
"!{bbin}blastn \
-task blastn \
-evalue 1e-10 \
-outfmt 6 \
-word_size 10 \
-gapopen 0 \
-gapextend 4 \
-num_threads 4 \
-query ../data/lncRNA-mytilus-CLC.fa \
-db /Volumes/Monarch/trilo/Crassostrea_gigas.GCA_000297895.1.30.dna_sm.toplevel \
-out analyses/lncRNA-CLC-blastn-Cgigas-genome-10-0-blastn.out
!wc -l analyses/lncRNA-CLC-blastn-Cgigas-genome-10-0-blastn.out"
"!perl /Users/sr320/git-repos/LabDocs/code/script-box/2_Blast2Gff.pl \
-i analyses/lncRNA-CLC-blastn-Cgigas-genome-10-0-blastn.out \
-s ""something"" -o analyses/lncRNA-CLC-blastn-Cgigas.gff -p ""lncRNA"" -d ""Crassostrea_gigas.GCA_000297895.1.30.dna_sm.toplevel""\
"
!head analyses/lncRNA-CLC-blastn-Cgigas.gff
"!{bbin}blastn \
-task blastn \
-evalue 1e-20 \
-outfmt 6 \
-query ../data/lncRNA-mytilus-CLC.fa \
-db ../data/SRR1598943 \
-out analyses/lncRNA-CLC-blastn-SRR1598943-20-blastn.out
!wc -l analyses/lncRNA-CLC-blastn-SRR1598943-20-blastn.out"
"!{bbin}blastn \
-task dc-megablast \
-evalue 1e-20 \
-outfmt 6 \
-query ../data/lncRNA-mytilus-CLC.fa \
-db ../data/SRR1598943 \
-out analyses/lncRNA-CLC-blastn-SRR1598943-dc-megablast.out
!wc -l analyses/lncRNA-CLC-blastn-SRR1598943-dc-megablast.out"
"!perl /Users/sr320/git-repos/LabDocs/code/script-box/2_Blast2Gff.pl \
-i analyses/lncRNA-CLC-blastn-SRR1598943-20-blastn.out \
-s ""something"" -o analyses/lncRNA-CLC-blastn-SRR1598943.gff -p ""lncRNA"" -d ""SRR1598943""\

"
"import warnings
warnings.filterwarnings('ignore')
!fgrep -A 2 --color ""TR43257_c13_g2_i2"" ../data/Mytilus-SXT.trimmed.trinity.v3.fa"
"!fgrep --color ""TR43257_c13_g2_i2"" /Users/sr320/Google\ Drive/Mytilus\ chilensis\ transcriptome\ \(STX\)/Myt-sxt-trinity2_blastn_nt.out"
"!fgrep --color ""myostatin"" /Users/sr320/Google\ Drive/Mytilus\ chilensis\ transcriptome\ \(STX\)/Myt-sxt-trinity2_blastn_nt.out"
"!fgrep ""meth"" /Users/sr320/git-repos/paper-mchil-sxt/data/S0-Myt-SXT-blast-descriptions.xlsx"
"!fgrep --color ""methyltransferase"" /Users/sr320/git-repos/paper-mchil-sxt/data/S2-Myt-sxt-Diffexp888-Clusters.csv"
"import warnings
warnings.filterwarnings('ignore')
#from Gustavo
!head -40 ../data/Myt-sxt-contigs_CLC.fa"
"!/Applications/transrate-1.0.1-osx/transrate \
--assembly \
../data/Myt-sxt-contigs_CLC.fa \
--output analyses/Myt-sxt-CLC-transrate"
!head /Users/sr320/git-repos/paper-mchil-sxt/nbs/analyses/Myt-sxt-CLC-transrate/Myt-sxt-contigs_CLC/contigs.csv
"import warnings
warnings.filterwarnings('ignore')
%matplotlib inline

import matplotlib
import numpy as np
import matplotlib.pyplot as plt
import math

def pretti(v): return int(v) if v == math.floor(v) else v # print ints pretty
def join(*args): return np.concatenate(args)
def tos(v): return '(' + str(pretti(v[0])) + ', ' + str(pretti(v[1])) + ')'

z = np.array([0, 0])
b = np.array([5, 6])
a = np.array([8, 2])
p = a * (a.dot(b)/a.dot(a))
e = b - p

data = np.array([ join(z, a), join(z, b), join(p, e), join(z,p) ])

X,Y,U,V = zip(*data)
# X is an array that contains the x positions of all the vectors
# Y contains the y position
# U, V are arrays containing horizontal and vertical values of all vectors

plt.figure(figsize=(6,6)) # initialize the figure
ax = plt.gca() # get current axes

# Quiver Docs
# http://matplotlib.org/api/pyplot_api.html#matplotlib.pyplot.quiver
ax.quiver(X, Y, U, V, angles='xy', scale_units='xy', scale=1)
ax.set_xlim([-1, 10])
ax.set_ylim([-1, 10])
ax.set_aspect(1.)

plt.text(5.7, 4, r'$\vec{e} = \vec{b}-\vec{p} = \vec{b}-\hat{x}\vec{a}$', size=18)
plt.text(b[0], b[1] + 0.3 ,r'$\vec{b}=' + tos(b) + '$', size=18)
plt.text(a[0]-0.6, a[1] + 0.3 ,r'$\vec{a}=' + tos(a) + '$', size=18)
plt.text(p[0] - 0.3, p[1] - 0.8 ,r'$\vec{p}=\hat{x}\vec{a}$', size=18)
plt.show()"
"import warnings
warnings.filterwarnings('ignore')
import numpy as np

# This example matrix is taken from p. 233 of Gilbert Strang's textbook
Q = np.matrix([
        [-1,  2,  2],
        [ 2, -1,  2], 
        [ 2,  2, -1]]) * (1/3.)

print(Q * Q.transpose())
"
"L = ['A', 'B', 'C']
keylist = ['a', 'b', 'c']
print(list2dict(L, keylist))"
"import warnings
warnings.filterwarnings('ignore')
#Generamos respuestas para la opcion pregunta simple con Moodle

import numpy as np

a = np.array([144,280,183,2738,843,325])
b = np.array([9,13,9,153,42,23])

am = (a-b)//100
aM = (a+b)//100+2

for i0 in range(am[0],aM[0]):
    for i1 in range(am[1],aM[1]):
        for i2 in range(am[2],aM[2]):
            for i3 in range(am[3],aM[3]):
                for i4 in range(am[4],aM[4]):
                    for i5 in range(am[5],aM[5]):
                        print(""""""
                              <answer fraction=""100"" format=""moodle_auto_format"">
                              <text>({}*;{}*;{}*;{}*;{}*;{}*)</text>
                              <feedback format=""html"">
                              <text></text>
                              </feedback>
                              </answer>"""""".format(i0,i1,i2,i3,i4,i5))
                        print(""""""
                              <answer fraction=""100"" format=""moodle_auto_format"">
                              <text>[{}*;{}*;{}*;{}*;{}*;{}*]</text>
                              <feedback format=""html"">
                              <text></text>
                              </feedback>
                              </answer>"""""".format(i0,i1,i2,i3,i4,i5))
                        print(""""""
                              <answer fraction=""100"" format=""moodle_auto_format"">
                              <text>{}*;{}*;{}*;{}*;{}*;{}*</text>
                              <feedback format=""html"">
                              <text></text>
                              </feedback>
                              </answer>"""""".format(i0,i1,i2,i3,i4,i5))
                        print(""""""
                              <answer fraction=""100"" format=""moodle_auto_format"">
                              <text>({}*,{}*,{}*,{}*,{}*,{}*)</text>
                              <feedback format=""html"">
                              <text></text>
                              </feedback>
                              </answer>"""""".format(i0,i1,i2,i3,i4,i5))
                        print(""""""
                              <answer fraction=""100"" format=""moodle_auto_format"">
                              <text>[{}*,{}*,{}*,{}*,{}*,{}*]</text>
                              <feedback format=""html"">
                              <text></text>
                              </feedback>
                              </answer>"""""".format(i0,i1,i2,i3,i4,i5))
                        print(""""""
                              <answer fraction=""100"" format=""moodle_auto_format"">
                              <text>{}*,{}*,{}*,{}*,{}*,{}*</text>
                              <feedback format=""html"">
                              <text></text>
                              </feedback>
                              </answer>"""""".format(i0,i1,i2,i3,i4,i5))
                        print(""""""
                              <answer fraction=""100"" format=""moodle_auto_format"">
                              <text>({}* {}* {}* {}* {}* {}*)</text>
                              <feedback format=""html"">
                              <text></text>
                              </feedback>
                              </answer>"""""".format(i0,i1,i2,i3,i4,i5))
                        print(""""""
                              <answer fraction=""100"" format=""moodle_auto_format"">
                              <text>[{}* {}* {}* {}* {}* {}*]</text>
                              <feedback format=""html"">
                              <text></text>
                              </feedback>
                              </answer>"""""".format(i0,i1,i2,i3,i4,i5))
                        print(""""""
                              <answer fraction=""100"" format=""moodle_auto_format"">
                              <text>{}* {}* {}* {}* {}* {}*</text>
                              <feedback format=""html"">
                              <text></text>
                              </feedback>
                              </answer>"""""".format(i0,i1,i2,i3,i4,i5))"
"# how to use numpy polynomials
# required for interactive plotting
from __future__ import print_function
from ipywidgets import interact, interactive, fixed
import ipywidgets as widgets
import numpy.polynomial as np_poly

def poly_visualize(root1,root2,root3,root4,root5):
    poly1 = np_poly.Polynomial.fromroots((root1,root2,root3,root4,root5))
    x, y = poly1.linspace(domain=(-10,10))
    plt.plot(x, y)

interact(poly_visualize, root1=(-10,10), root2=(-10,10), root3=(-10,10), root4=(-10,10), root5=(-10,10))"
"# example 4.1
# target function: sine
# N = 2
# with and w/o weight decay
N, T = 2, 200
N_plot = 200
x_plot = np.linspace(-1,1,N_plot).reshape((N_plot,1))
xx_plot = np.matrix(np.hstack([np.ones_like(x_plot), x_plot])).reshape(N_plot, 2)
all_ys = np.matrix(np.zeros((N_plot, T))).reshape(N_plot, T)
for t in range(T):
    # sample the N points from sine
    x = np.matrix(np.random.rand(N)).reshape((N,1))*2-1
    sin_x = np.matrix([math.sin(math.pi*xx) for xx in x]).reshape((N,1))
    
    # form the homogenous coordinates
    X = np.hstack([np.ones_like(x),x])
    # could have done w/ the inverse instead, but what the heck
    w = (X.transpose()*X)**-1 * X.transpose() * sin_x
    
    # show the line
    w_plot = np.matrix([lm_inner(w,xx_plot[ix,:].transpose()) for ix in range(N_plot)]).reshape((N_plot,1))
    all_ys[:,t] = w_plot
    #plt.plot(xx_plot[:,1], w_plot, color=('#B5FA5E'))

w_mean = np.mean(all_ys, 1)
w_std  = np.std(all_ys,  1)
for ix in range(N_plot):
    x_val = xx_plot[ix, 1]
    y_mean, y_std = w_mean[ix,0], w_std[ix,0]
    # mpl.lines.Line2D
    plt.plot([x_val,x_val], [y_mean-y_std, y_mean+y_std], color='#B5FA5E', linewidth=2)

# show the sine man
sin_x_plot = np.matrix([math.sin(math.pi*xx) for xx in x_plot]).reshape(N_plot,1)
plt.plot(x_plot, sin_x_plot, color='magenta', linewidth=3)
plt.axis((-1,1,-3,3))

plt.show()"
"a = np.matrix(np.zeros((5,3)))
# print(a)
a[:,1] = np.matrix(np.ones((5,1)))
# print(a)
a = np.matrix(np.random.rand(5,3))
# print(a)
# print(np.mean(a,1))
# print(np.var(a,1))
np.std(a,1)"
"## legendre polynomials

# required for interactive plotting
from __future__ import print_function
from ipywidgets import interact, interactive, fixed
import ipywidgets as widgets
import numpy.polynomial as np_poly

def show_legendre(degree):
    leg_print(degree)
    b = np_poly.legendre.Legendre.basis(degree)
    x, y = b.linspace()
    plt.plot(x,y)
    plt.show()
    
interact(show_legendre, degree=(1,10))
"
"Q = np.zeros(A.shape,dtype=float)
for k in range(A.shape[1]):
    avec = A[:, k]
    q = avec
    for j in range(k):
        q = q - np.dot(avec, Q[:,j])*Q[:,j]
    qq = q*1./la.norm(q)
    qq = qq.reshape(1,-1)
    Q[:, k] = qq
print('Q:\n',Q)
R = Q.T @ A
print('R:\n', R)"
"import warnings
warnings.filterwarnings('ignore')
%load_ext autoreload
%autoreload 2
%matplotlib inline"
"# make some data:
R1 = random.Random()
R2 = random.Random()
#
N_points = 1000
b_slope =.5
dx_dy = 5.
#
datas = []
for j in range(N_points):
    x = dx_dy*R1.random()
    y = R2.random() + b_slope*x
    #
    datas += [[x,y]]
#
datas = numpy.array(datas)
XY = datas
plt.figure()
ax1=plt.gca()
plt.plot(*zip(*datas), ls='', marker='.')"
"# now, pca it:
#
cov_xy = numpy.cov(datas.T)
lambdas, eigvs = numpy.linalg.eigh(cov_xy)

print(lambdas, numpy.sqrt(lambdas[1]/lambdas[0]))
print(eigvs)"
"#print(pca_R)
pcas = sorted(zip(lambdas, eigvs), key=lambda rw: rw[0])
print('pcas: ', pcas)

e_vals = [rw[0] for rw in pcas]
A_pca = numpy.array([rw[1] for rw in pcas])
print('**', A_pca)
#
# reduced basis
A_pca_r = A_pca[-1:]
#
print('A_pca:   {}'.format(A_pca))
print('A_pca_r: {}'.format(A_pca_r))
#
# data transformed onto the reduced basis:
XY_prime = numpy.dot(XY, A_pca_r.T)
#print('**', numpy.dot(XY, A_pca_r.T))
#
# now, rotate reduced data back to original frame.
XY_prime2 = numpy.dot(XY_prime, A_pca_r)
#"
"plt.figure()
#
# plot basis vectors:
for lamb, vec in reversed(sorted(zip(lambdas, eigvs), key=lambda rw: rw[0])):
    print('vec: ', lamb, vec)
    #plt.arrow(x=0.,y=0., dx=10*lamb*vec[0], dy=10*lamb*vec[1], lw=2.5, zorder=11, ls='-', color='m')
    #plt.arrow(0., 0., .89, .45)
    #plt.arrow(.8,.8,-0.89188099,-0.45227015)
    #plt.plot(*zip(numpy.zeros(len(vec)), lamb*vec), ls='-', lw=2., color=None)
    d_center = numpy.array([numpy.mean(col) for col in zip(*XY)])
    plt.plot(*zip(d_center, numpy.sqrt(lamb)*vec + d_center), ls='-', lw=3., color='r')
#plt.figure()
plt.plot(*zip(*datas), ls='', marker='.', color='b', zorder=4, alpha=.5)
plt.plot(*zip(*XY_prime2), marker='.', ls='', color='r')
#print('dot: {}'.format(numpy.dot(A_pca, XY.T)))"
%pdef moodle_xml
"data = np.random.permutation(100)
plt.plot(data)
plt.show()
data = np.random.rand(100,100)
plt.imshow(data)"
"plt.plot(x, y, label='a(x)')
plt.plot(x, newy, label='g(a(x))')
plt.xlim(-2,2)
plt.ylim(-1,1)
plt.legend()
plt.show()"
"def softmax(x):
    return np.exp(x) / np.sum(np.exp(x), axis=0)

scores = [3.0, 1.0, 0.2]
print(softmax(scores))"
"def softmax(x):
    e_x = np.exp(x - np.max(x))
    return e_x / e_x.sum(axis=0) # 注意axis=0, 使得该方法可以扩展到2维

scores = [3.0, 1.0, 0.2]
softmax(scores)"
"scores2D = np.array([[1, 2, 3, 6],
                     [2, 4, 5, 6],
                     [3, 8, 7, 6]])
softmax(scores2D)"
"# linear

w = 2
y = w*x
plt.plot(x,y)
plt.show()"
"def sigmod(a):
    return 1/(1+np.exp(-a))

y = sigmod(x)
plt.plot(x,y)
plt.show()"
"def tanh(a):
    return (np.exp(2*a)-1)/(np.exp(2*a)+1)

y = tanh(x)
plt.plot(x, y)
plt.show()"
"def ReLU(a):
    return np.maximum(0,a)  

y = ReLU(x)
plt.plot(x,y)
plt.show()"
"def ReLU_fast(a):
    return a*(a > 0)

# note: list doesn't support this function, you have to use array
y = ReLU_fast(np.array(x))
plt.plot(x,y)
plt.show()"
"figure(1)
nbins = 30
hist(y, bins = x)
plot(x, px*N/sum(px), color='g', linewidth=2)
plot(x, q(x)*N/sum(px), color='r', linewidth=2)

figure(2)
plot(y)
show()"
"figure(1)
nbins = 30
hist(y2, bins = x)
plot(x, px*N/sum(px), color='g', linewidth=2)

show()"
"multilist = [[0 for col in range(5)] for row in range(3)]
print(multilist)"
E_loss
"import warnings
warnings.filterwarnings('ignore')
import json
import requests
from bs4 import BeautifulSoup
import xml
import pandas as pd
import time
import pprint

pd.set_option(""display.max_rows"",101)
pd.set_option('max_colwidth',5000)

report_name = ""BiologicalDetails""
code_list=""1stp,2jef,1cdg""


url =  (""http://www.rcsb.org/pdb/rest/customReport.csv?pdbids=""+
       code_list+
       ""&reportName=""+report_name+
       ""&service=wsfile&format=csv"")
print(url)


report_types = [
                # ""StructureSummary"",
                # ""Sequence"",
                # ""Ligands"",
                # ""BindingAffinity"",
                # ""BiologicalDetails"",
                # ""ClusterEntity"",
                # ""Domains"",
                # ""Crystallization"",
                # ""UnitCellDimensions"",
                # ""DataCollectionDetails"",
                # ""RefinementDetails""
                # ""refinementParameters""
                ""Citation"",
                # ""OtherCitations"",
                # ""SGProject""
                ]

code_list=""1stp,2jef,1cdg""

import urllib

def get_RCSB_csv(report_name, code_list):
    print(report_name)
    url = (""http://www.rcsb.org/pdb/rest/customReport.csv?pdbids=""+
       code_list+
       ""&reportName=""+report_name+
       ""&service=wsfile&format=csv"")
    print(url)
    try:
        df = pd.read_csv(url)
        return df
    #     print(df.head())
    except urllib.error.HTTPError:
        print(""Failed to retreive."")
        return None

# for report_name in report_types[9:11]:
for report_name in report_types:
    df = get_RCSB_csv(report_name, code_list)
    #print(df)
    num_record = df.shape[0]
    data_json = df.to_json()
    d2 = json.loads(data_json)

    for i in range(num_record):
        pass

    print(d2['authors'])
"
"pd.concat([df1, df2])"
"c = pd.concat([df1, df2], keys=['df1', 'df2'])
c"
c.ix['df2']
"pd.concat([df1, df3], axis=1)"
df1.append(df2)
"df1.append(df2, ignore_index=True)"
"customers = {'CustomerID': [10, 11],
            'Name': ['Mike', 'Marcia'],
            'Address': ['Address for Mike',
                        'Address for Marcia']}
customers = pd.DataFrame(customers)
customers"
customers.merge(orders)
"left.merge(right, how='outer')"
"left.merge(right, how='left')"
"left.merge(right, how='right')"
"left.join(right, lsuffix='_left', rsuffix='_right')"
multi_user_sensor_data.unstack(level=0)
"unstacked = multi_user_sensor_data.unstack(['who', 'axis'])
unstacked"
"data = pd.DataFrame({'Name': ['Mike', 'Mikael'],
                     'Height': [6.1, 6.0],
                     'Weight': [220, 185]})
data"
"df['c4'] = np.nan
df.loc['f'] = np.arange(15, 19)
df.loc['g'] = np.nan
df['c5'] = np.nan
df['c4']['a'] = 20
df

"
df.c4[df.c4.notnull()]
df.c4.dropna()
df.c4
df.dropna(how='all') # drop if ALL values in ROW are NaN
"df.dropna(how='all', axis=1) # drop if ALL values in COLUMN are NaN"
"df2 = df.copy()
df2.ix['g'].c1 = 0
df2.ix['g'].c3 = 0
df2"
"df2.dropna(how='any', axis=1)"
"df.dropna(thresh=5, axis=1) # at least 5 NaN"
"df3 = df.copy()
df3.dropna(thresh=5, axis=1, inplace=True) # inplace
df3"
s.cumsum()
df.c4 + 1
df.c4
"filled = df.fillna(0)
filled"
"df.fillna(0, limit=2)"
"df.c4.fillna(method=""ffill"")"
df.c4.fillna(method='bfill')
df.bfill()
df.c4.fillna(fill_values) #only fills Nan values
df.fillna(df.mean())
"ts = pd.Series([1, np.nan, 2],
               index=[datetime.datetime(2104, 1, 1),
                      datetime.datetime(2104, 2, 1),
                      datetime.datetime(2104, 4, 1)])
ts"
"s = pd.Series([0, np.nan, 100], index=[0, 1, 10])
s"
s.interpolate()
"s.interpolate(method=""values"")"
"x = pd.Series({""one"": 1, ""two"": 2, ""three"": 3})
y = pd.Series({1: ""a"", 2: ""b"", 3: ""c""})
x"
x.map(y)
"x = pd.Series({""one"": 1, ""two"": 2, ""three"": 3})
y = pd.Series({1: ""a"", 2: ""b""})
x.map(y)"
"s = pd.Series([0., 1., 2., 3., 2., 4.])
s"
"s.replace(2, 5)"
"s.replace([0, 1, 2, 3, 4], [4, 3, 2, 1, 0])"
"s.replace({0: 10, 1: 100})"
"s[0] = 10
s"
"s.replace([1, 2, 3,], method='pad')"
"df = pd.DataFrame(np.arange(0, 15).reshape(3, 5))
df.loc[1, 2] = np.nan
df"
"df.dropna().apply(lambda x: x.sum(), axis=1)"
print(cars.head())
"plt.scatter(cars.weight, cars.mpg)
plt.xlabel('Weight')
plt.ylabel('MPG')"
"plt.scatter(cars.weight, cars.acceleration)
plt.xlabel('Weight')
plt.ylabel('Acceleration')"
"plt.scatter(cars.mpg, cars.displacement)
plt.xlabel('MPG')
plt.ylabel('Displacement')"
"print(comics_characters_dc.index)  # Q1: 6896, Q4
print(comics_characters_dc.columns)"
comics_characters_dc.head() #Q2  #Q5: NaN
comics_characters_dc.sort_values(by='YEAR')
comics_characters_dc[0:3] # Rows 1-3
"comics_characters_dc[[""EYE"", ""YEAR""]] # columns EYE, YEAR"
"comics_characters_dc[[""EYE"", ""YEAR""]] # columns EYE, YEAR
comics_characters_dc[10:21][[""EYE"", ""YEAR""]] # Rows 11-22, columns EYE, YEAR "
comics_characters_marvel.head() #Q2 Q5=NaN
"comics_characters_marvel.sort_values(by=""Year"")"
"print(comics_characters_marvel.index)  # Q1: 16376  Q4
print(comics_characters_marvel.columns)"
"# These are the ""Tableau 20"" colors as RGB.    
tableau20 = [(31, 119, 180), (174, 199, 232), (255, 127, 14), (255, 187, 120),    
             (44, 160, 44), (152, 223, 138), (214, 39, 40), (255, 152, 150),    
             (148, 103, 189), (197, 176, 213), (140, 86, 75), (196, 156, 148),    
             (227, 119, 194), (247, 182, 210), (127, 127, 127), (199, 199, 199),    
             (188, 189, 34), (219, 219, 141), (23, 190, 207), (158, 218, 229)]    
  
# Scale the RGB values to the [0, 1] range, which is the format matplotlib accepts.    
for i in range(len(tableau20)):    
    r, g, b = tableau20[i]    
    tableau20[i] = (r / 255., g / 255., b / 255.)    

# You typically want your plot to be ~1.33x wider than tall. This plot is a rare    
# exception because of the number of lines being plotted on it.    
# Common sizes: (10, 7.5) and (12, 9)    
plt.figure(figsize=(12, 14))    

# Limit the range of the plot to only where the data is.    
# Avoid unnecessary whitespace.    
plt.ylim(0, 90)    
plt.xlim(1930, 2014)    

  
# Make sure your axis ticks are large enough to be easily read.    
# You don't want your viewers squinting to read your plot.    
plt.yticks(range(0, 91, 10), [str(x) + ""%"" for x in range(0, 91, 10)], fontsize=14)    
plt.xticks(fontsize=14)    
  
# Provide tick lines across the plot to help your viewers trace along    
# the axis ticks. Make sure that the lines are light and small so they    
# don't obscure the primary data lines.    
for y in range(10, 91, 10):    
    plt.plot(range(1968, 2012), [y] * len(range(1968, 2012)), ""--"", lw=0.5, color=""black"", alpha=0.3)    
"
"g = nx.circular_ladder_graph(20)
nx.draw_spring(g)"
"plt.scatter(xs, ys)"
"plt.scatter(xs, ys, c=zs, alpha=0.2)"
"from functools import partial

import numpy
import scipy.optimize
import matplotlib.pyplot as pp

def z(x, y):
    return x ** 2 + x * y + y ** 2 - 10

x_window = 0, 5
y_window = 0, 5

xs = []
ys = []
for x in numpy.linspace(*x_window, num=200):
    try:
        # A more efficient technique would use the last-found-y-value as a 
        # starting point
        y = scipy.optimize.brentq(partial(z, x), *y_window)
    except ValueError:
        # Should we not be able to find a solution in this window.
        pass
    else:
        xs.append(x)
        ys.append(y)

pp.plot(xs, ys)
pp.xlim(*x_window)
pp.ylim(*y_window)
pp.show()"
"from sympy import plot_implicit, cos, sin, symbols, Eq, And
x, y = symbols('x y')
p1 = plot_implicit(Eq(x**2 + y**2, 5))"
"def goursat_tangle(x,y,z):
    a,b,c = 0.0,-5.0,11.8
    return x**4+y**4+z**4+a*(x**2+y**2+z**2)**2+b*(x**2+y**2+z**2)+c

plot_implicit(goursat_tangle)"
"def hyp_part1(x,y,z):
    return -(x**2) - (y**2) + (z**2) - 1

plot_implicit(hyp_part1, bbox=(-100.,100.))"
"def sphere(x,y,z):
    return x**2 + y**2 + z**2 - 2.0**2

def translate(fn,x,y,z):
    return lambda a,b,c: fn(x-a,y-b,z-c)

def union(*fns):
    return lambda x,y,z: np.min(
        [fn(x,y,z) for fn in fns], 0)

def intersect(*fns):
    return lambda x,y,z: np.max(
        [fn(x,y,z) for fn in fns], 0)

def subtract(fn1, fn2):
    return intersect(fn1, lambda *args:-fn2(*args))

plot_implicit(union(sphere,translate(sphere, 1.,1.,1.)), (-2.,3.))"
"def torus(x,y,z):
    """"""((II)I)""""""
    r1 = 10
    r2 = 2
    return ((x**2 + y**2)**(1/2) - r1)**2 + z**2 - r2
    
plot_implicit(torus, bbox=(-12,12))"
"plt.imshow(t[:,:,50]);"
"plt.plot(t[:,50,50])"
"plt.plot(abs(t[:,50,50]))"
"plt.plot(1/abs(t[:,50,50]))"
"plt.plot(1/abs(t[:,50,50]))"
"p = np.clip(1/abs(t[:,50,50]), 0, 50) 
# NOTE 50 happens to be a good choice of clipping for this set, what's a method of coming up with this automatically?
p /= p.sum()
plt.plot(p)"
"np.percentile(1/abs(t[:,50,50]), 95)"
"percentile = np.percentile(1/abs(t[:,50,50]), 95)
p = np.clip(1/abs(t[:,50,50]), 0, percentile) 
# NOTE 100 happens to be a good choice of clipping for this set, what's a method of coming up with this automatically?
p /= p.sum()
plt.plot(p)"
"percentile = np.percentile(1/abs(t), 99)
p = np.clip(1/abs(t), 0, percentile) 
p /= p.sum()

plt.plot(p[:,50,50])

# Doesn't look at good, why? because we're looking at percentile in all 3 dimensions, e.g. this may be a trait
# of the dimensionality curse. It's logical to assume that in 4 dimensions we'd want an even higher percientile...

# Also Note that these are incredibly small values 1e-8...
# we have to do this by dimension if it's going to work well..."
"percentile = np.percentile(1/abs(t), 99)
p = 1/abs(t)
#p = np.clip(p, 0, percentile) 
p /= p.sum()

slice_idx = 70
for axis in range(len(t.shape)):
    plt.subplot(1,3,axis+1)
    if axis == 0:
        plt.imshow(p[slice_idx,:,:], label='axis %s' % axis)
    elif axis == 1:
        plt.imshow(p[:,slice_idx,:], label='axis %s' % axis)
    elif axis == 2:
        plt.imshow(p[:,:,slice_idx], label='axis %s' % axis)"
p.flatten().sum()
"n_samples = 10000
flat_samples = np.random.choice(len(t.flatten()), size=n_samples, replace=True, p=p.flatten())
flat_samples"
"samples = np.unravel_index(flat_samples, t.shape)
samples"
A[samples[0]]
"plt.scatter(xs, ys, c=zs, alpha=0.2);"
"def makePuzzle(puzzleSize):
    numbers=range(1, puzzleSize**2+1)
    Bnums=np.array(numbers)
    Tnums=np.array(numbers)
    board=np.reshape(Bnums, (puzzleSize,puzzleSize))
    np.random.shuffle(Tnums)
    tiles=np.reshape(Tnums, (puzzleSize,puzzleSize))
    return board, tiles
    
board,tiles=makePuzzle(3)
print(board)
print(tiles)
"
"for size in range(2,4):
    runstart=time.time()
    board, tiles=makePuzzle(size)
    print('Starting configuration\n', tiles)
    steps=0; match=True
    while not np.array_equal(tiles,board):
        steps+=1
        ##Random update
        Xaxis=bool(random.getrandbits(1))
        moveUp=bool(random.getrandbits(1))
        tiles=moveTile(tiles,Xaxis,moveUp)
        if steps>1000:
            match=False
            break

    if match:
        print('Match\n',tiles,'\n',board,'\n')
    print('Count', steps)
    print('Runtime', (time.time()-runstart) )"
"data=[]
for i in range(100):
    board, tiles=makePuzzle(2)
    steps=0
    while not np.array_equal(tiles,board):
        steps+=1
        ##Random update
        Xaxis=bool(random.getrandbits(1))
        moveUp=bool(random.getrandbits(1))
        tiles=moveTile(tiles,Xaxis,moveUp)
        if steps>400:
            break
    data.append(steps)

plt.hist(data, bins=50)
plt.title(""Number of random steps to solution"")
plt.xlabel(""Step number"")
plt.ylabel(""Frequency"")
plt.show()"
"Mdata=[]
for i in range(100):
    board, tiles=makePuzzle(2)
    steps=0; match=True
    while not np.array_equal(tiles,board):
        steps+=1
        ##Random update
        Xaxis=bool(random.getrandbits(1))
        moveUp=bool(random.getrandbits(1))
        tiles=moveTile(tiles,Xaxis,moveUp)
        if steps>400:
            match=False
            break
    if match:
        Mdata.append(steps)
print('Median is', np.median(Mdata))
plt.hist(Mdata, bins=50)
plt.title(""Number of random steps to solution"")
plt.xlabel(""Step number"")
plt.ylabel(""Frequency"")
plt.show()"
"from sklearn.datasets import make_s_curve
X, y = make_s_curve(n_samples=1000)

from mpl_toolkits.mplot3d import Axes3D
ax = plt.axes(projection='3d')

ax.scatter3D(X[:, 0], X[:, 1], X[:, 2], c=y)
ax.view_init(10, -60)"
"from sklearn.decomposition import PCA
X_pca = PCA(n_components=2).fit_transform(X)
plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y)"
"from sklearn.manifold import Isomap

iso = Isomap(n_neighbors=15, n_components=2)
X_iso = iso.fit_transform(X)
plt.scatter(X_iso[:, 0], X_iso[:, 1], c=y)"
"from sklearn.datasets import load_digits
digits = load_digits()

fig, axes = plt.subplots(2, 5, figsize=(10, 5),
                         subplot_kw={'xticks':(), 'yticks': ()})
for ax, img in zip(axes.ravel(), digits.images):
    ax.imshow(img, interpolation=""none"", cmap=""gray"")"
"# build a PCA model
pca = PCA(n_components=2)
pca.fit(digits.data)
# transform the digits data onto the first two principal components
digits_pca = pca.transform(digits.data)
colors = [""#476A2A"", ""#7851B8"", ""#BD3430"", ""#4A2D4E"", ""#875525"",
          ""#A83683"", ""#4E655E"", ""#853541"", ""#3A3120"",""#535D8E""]
plt.figure(figsize=(10, 10))
plt.xlim(digits_pca[:, 0].min(), digits_pca[:, 0].max() + 1)
plt.ylim(digits_pca[:, 1].min(), digits_pca[:, 1].max() + 1)
for i in range(len(digits.data)):
    # actually plot the digits as text instead of using scatter
    plt.text(digits_pca[i, 0], digits_pca[i, 1], str(digits.target[i]),
             color = colors[digits.target[i]],
             fontdict={'weight': 'bold', 'size': 9})
plt.xlabel(""first principal component"")
plt.ylabel(""second principal component"")"
"plt.figure(figsize=(10, 10))
plt.xlim(digits_tsne[:, 0].min(), digits_tsne[:, 0].max() + 1)
plt.ylim(digits_tsne[:, 1].min(), digits_tsne[:, 1].max() + 1)
for i in range(len(digits.data)):
    # actually plot the digits as text instead of using scatter
    plt.text(digits_tsne[i, 0], digits_tsne[i, 1], str(digits.target[i]),
             color = colors[digits.target[i]],
             fontdict={'weight': 'bold', 'size': 9})"
"ser1 = Series([2,np.nan,4,np.nan,6,np.nan],
             index=['Q','R','S','T','U','V'])
ser1"
"ser2 = Series(np.arange(len(ser1)),dtype=np.float64, 
             index=['Q','R','S','T','U','V'])
ser2"
"#using numpy.where ser1 is null we will put in the ser2 value, 
#else we will put in the ser1 value. 
Series(np.where(pd.isnull(ser1),ser2,ser1), index=ser1.index)"
"#using series.combine. Same logic as the above methods. 

ser1.combine_first(ser2)"
"nan = np.nan
dframeOdds = DataFrame({'X':[1.,nan,3.,nan],
                        'Y':[nan,5.,nan,7.],
                        'Z':[nan,9,nan,11]})
dframeOdds"
"dframeEvens = DataFrame({'X':[2.,4,nan,6.,8.],
                        'Y':[nan,10.,12.,14.,16.]})
dframeEvens"
dframeOdds.combine_first(dframeEvens)
"#if we count from 0 to 4, the blender will create a random permutation of it
blender = np.random.permutation(4)
blender"
"#from this dataframe take index 2,3,1,0 and put them in that order
dframe.take(blender)"
"shaker = np.random.randint(0,len(box),size=10)
shaker"
"hand_grabs = box.take(shaker)
hand_grabs"
plt.hist(dataset1)
"dataset2 = randn(80)
plt.hist(dataset2, color='indianred')"
"plt.hist(dataset1,normed=True,color='indianred',alpha=0.5,bins=20)
plt.hist(dataset2,normed=True,alpha=0.5,bins=20)"
"sns.jointplot(data1,data2)"
"sns.jointplot(data1, data2, kind='hex')"
obj.values
obj.index
"#convert series into dict
ww2_dict = ww2_cas.to_dict()

ww2_dict"
"# take a dict and convert it to a series
ww2_series = Series(ww2_dict)

ww2_series"
"obj2 = Series(ww2_dict, index = countries)

obj2"
ww2_series
"
obj2"
"#add series to series
ww2_series + obj2 "
"obj2.name = 'World war 2 Casualties'

obj2"
"obj2.index.name = 'Countries'

obj2"
"#rug plot. Puts a tick wherever a value occurs

dataset = randn(25)
sns.rugplot(dataset)

plt.ylim(0,1)"
"plt.hist(dataset, alpha=.3)
sns.rugplot(dataset)"
"sns.rugplot(dataset)

x_min = dataset.min() - 2
x_max = dataset.max() + 2

x_axis = np.linspace(x_min, x_max, 100)

bandwidth = ((4*dataset.std()**5)/(3*len(dataset))) **0.2

kernel_list = []

for data_point in dataset:
    #create a kernel for each point and append it to the kernel_list
    kernel = stats.norm(data_point, bandwidth).pdf(x_axis)
    kernel_list.append(kernel)
    
    #scale for plotting
    kernel = kernel / kernel.max()
    kernel = kernel * .4
    
    plt.plot(x_axis, kernel, color='grey', alpha=0.5)
    
plt.ylim(0,1)"
"sum_of_kde = np.sum(kernel_list, axis=0)

fig = plt.plot(x_axis, sum_of_kde, color='indianred')

sns.rugplot(dataset)

plt.yticks([])

plt.suptitle('Sum of the basis functions')"
"#using seaborn
sns.kdeplot(dataset)"
"sns.rugplot(dataset, color='black')

for bw in np.arange(0.5,2,.25):
    sns.kdeplot(dataset, bw=bw, lw=1.8, label=bw)"
"kernel_options = ['biw','cos','epa','gau','tri','triw']

for kern in kernel_options:
    sns.kdeplot(dataset, kernel=kern, label=kern, shade = True)"
"sns.kdeplot(dataset, vertical=True)"
"#cumulative distribution function

sns.kdeplot(dataset, cumulative = True)"
"mean = [0,0]

cov = [[1,0],[0,100]]

dataset2 = np.random.multivariate_normal(mean,cov,1000)

dframe = pd.DataFrame(dataset2, columns=['X','Y'])

sns.kdeplot(dframe)"
"sns.kdeplot(dframe.X, dframe.Y, shade=True)"
"sns.kdeplot(dframe,bw=1)"
"sns.kdeplot(dframe,bw='silverman')"
"sns.jointplot('X','Y', dframe, kind='kde')"
"import warnings
warnings.filterwarnings('ignore')
#The normal imports
import pandas as pd
import numpy as np
from numpy.random import randn

#Import the stats library from numpy
from scipy import stats

#These are the plotting modules and libs we will use
import matplotlib as mpl
import matplotlib.pyplot as plt
import seaborn as sns

#Command so that plots appear in the iPython Notebook
%matplotlib inline"
sns.heatmap(flight_dframe)
"#annotate each cell
sns.heatmap(flight_dframe,annot=True,fmt='d')"
"sns.heatmap(flight_dframe,center=flight_dframe.loc['January',1955])"
"f,(axis1, axis2) = plt.subplots(2,1)

yearly_flights = flight_dframe.sum()

years = pd.Series(yearly_flights.index.values)
years = pd.DataFrame(years)

flights = pd.Series(yearly_flights.values)
flights = pd.DataFrame(flights)

year_dframe = pd.concat((years,flights), axis=1)
year_dframe.columns = ['Year','Flights']

sns.barplot('Year', y='Flights', data=year_dframe, ax=axis1)

sns.heatmap(flight_dframe, cmap='Blues', ax=axis2, cbar_kws={'orientation':'horizontal'})
"
"#cluster map
sns.clustermap(flight_dframe)"
"sns.clustermap(flight_dframe,col_cluster=False)"
"#standardize
sns.clustermap(flight_dframe,standard_scale=1)"
"sns.clustermap(flight_dframe,standard_scale=0)"
"sns.clustermap(flight_dframe,z_score=1)"
"dframe = DataFrame({'city':['Alma','Brian Head','Fox Park'],
                   'altitude':[3158,3000,2762]})
dframe"
dframe
"decadeCat = pd.cut(years,decadeBins)
decadeCat"
decadeCat.categories
"pd.cut(years,2,precision=1)"
dframeWine[:5]
"#Find the avg alchohol content
dframeWine['alcohol'].mean()"
dframeWine
wino.describe()
wino.agg(maxToMin)
dframeWine.head()
"#add a new column
dframeWine['qual/alc ratio'] = dframeWine['quality'] / dframeWine['alcohol']
dframeWine.head()"
"dframeWine.plot(kind='scatter',x='quality', y='alcohol')"
"dframe = DataFrame({'k1':['x','x','y','y','z'],
                   'k2':['alpha','beta','alpha','beta','alpha'],
                   'dataset1': np.random.randn(5),
                   'dataset2': np.random.randn(5)})
dframe"
"group1 = dframe['dataset1'].groupby(dframe['k1'])

group1"
group1.mean()
"dframe['dataset1'].groupby([cities,month]).mean()"
dframe
dframe.groupby('k1').mean()
"#get the mean on multiple cplumns
dframe.groupby(['k1','k2']).mean()"
dframe
"#iterate over groups
for name,group in dframe.groupby('k1'):
    print('This is the {} group name'.format(name))
    print(group)
    print('\n')"
"for (k1,k2),group in dframe.groupby(['k1','k2']):
    print('Key 1 = {} Key 2 = {}'.format(k1,k2))
    print(group)
    print('\n')"
groupDict['x']
groupDictAxis1
"#use group by with columns
dataSet2Group = dframe.groupby(['k1','k2'])[['dataset2']]

#average the values
dataSet2Group.mean()"
dframe
dframe
"dframe_piv = dframe.pivot('date','variable','value')
dframe_piv"
"dframeSt = dframe1.stack()
dframeSt"
dframeSt.unstack()
dframe.unstack()
dframe.unstack().stack()
"dframe = dframe.unstack()
dframe"
dframe.stack()
dframe.stack(dropna=False)
"#replace we can select a value and replace it wiht a new value
ser1.replace(1, np.nan)"
ser1.replace({4:np.nan})
dframe.index.map(str.lower)
"N = 40
x = range(1,N)
plt.plot(x, [m.log2(xx) for xx in x], label='log')
plt.plot(x, [m.floor(m.log2(xx)) for xx in x],label='floor(log)')
plt.title('Log2')
plt.xlabel('x')
plt.ylabel('log2(x)')
plt.legend()
plt.show()"
"x = list(cat_dl.keys())
y = list(cat_dl.values())

x1 = list(cat_mno.keys())
y1 = list(cat_mno.values())

norm_y = list(np.divide(y,y1))

width = 1

fig = plt.figure(figsize=(10,6.5))
ind = np.arange(len(y))
plt.bar(ind,y)
plt.xticks(ind + width / 2, x)
fig.autofmt_xdate()
fig.suptitle(""Total Download per Category"",fontsize=20)

fig = plt.figure(figsize=(10,6.5))
ind = np.arange(len(y1))
plt.bar(ind,y1)
plt.xticks(ind + width / 2, x1)
fig.autofmt_xdate()
fig.suptitle(""Total Mods per Category"",fontsize=20)

fig = plt.figure(figsize=(10,6.5))
ind = np.arange(len(norm_y))
plt.bar(ind,norm_y)
plt.xticks(ind + width / 2, x1)
fig.autofmt_xdate()
fig.suptitle(""Normalized Download/mod per Category"",fontsize=20)
"
"for k in cat_dict.keys():
    tags = cat_dict[k]
    x = []
    y = []
    y1 = []
    for tag in tags:
        x.append(tag)
        y.append(tag_dl[tag])
        y1.append(tag_mno[tag])
    norm_y = list(np.divide(y,y1))
    fig = plt.figure(figsize=(12,5))
    ind = np.arange(len(y))
    plt.bar(ind,y)
    plt.xticks(ind + width / 2, x)
    fig.autofmt_xdate()
    fig.suptitle(""Total Download per Tag for category: ""+k,fontsize=20)

    fig = plt.figure(figsize=(12,5))
    ind = np.arange(len(y1))
    plt.bar(ind,y1)
    plt.xticks(ind + width / 2, x)
    fig.autofmt_xdate()
    fig.suptitle(""Total Mods per Tag for category: ""+k,fontsize=20)

    fig = plt.figure(figsize=(12,5))
    ind = np.arange(len(norm_y))
    plt.bar(ind,norm_y)
    plt.xticks(ind + width / 2, x)
    fig.autofmt_xdate()
    fig.suptitle(""Normalized Download/mod per Tag for category: ""+k,fontsize=20)"
"x = list(tag_dl.keys())
y = list(tag_dl.values())

x1 = list(tag_mno.keys())
y1 = list(tag_mno.values())

norm_y = list(np.divide(y,y1))

width = 1

fig = plt.figure(figsize=(12,35))
ind = np.arange(len(y))
plt.barh(ind,y)
plt.yticks(ind + width / 2, x)
fig.autofmt_xdate()
fig.suptitle(""Total Download per Tag"",fontsize=20)

fig = plt.figure(figsize=(12,35))
ind = np.arange(len(y1))
plt.barh(ind,y1)
plt.yticks(ind + width / 2, x1)
fig.autofmt_xdate()
fig.suptitle(""Total Mods per Tag"",fontsize=20)

fig = plt.figure(figsize=(12,35))
ind = np.arange(len(norm_y))
plt.barh(ind,norm_y)
plt.yticks(ind + width / 2, x1)
fig.autofmt_xdate()
fig.suptitle(""Normalized Download/mod per Tag"",fontsize=20)"
"plt.plot(plot_data, label='My Data')
plt.xlabel('Index')
plt.ylabel('Plot Value')
plt.title('The Plot Value From surveys.csv')"
"# Sampling data
s_time = 100
s_rate = 1000
s_nr = s_rate * s_time 

# frequencies
a0 = 0.75
f0 = 49

a1 = 0.5
f1 = 51

x = np.linspace(0, s_time, s_nr)
y = a0  * np.sin(2 * np.pi * x * f0) + a1 * np.sin(2 * np.pi * x * f1)

fig, axs = plt.subplots(2,1,figsize=(12, 5))
axs[0].plot(x, y)
axs[0].plot(x, y, color='red', ls='', marker='.')
axs[0].set_ylim(-2, 2)

x = np.linspace(0, s_time, s_nr)
y = a0  * np.sin(2 * np.pi * x * f0) + a1 * np.sin(2 * np.pi * x * f1) +  10 * np.random.normal(size=s_nr)
axs[1].plot(x, y)
axs[1].plot(x, y, color='red', ls='', marker='.')
axs[1].set_ylim(-25, 25)

for ax in axs:
    ax.set_xlim(0, 1)
    ax.set_xlabel('time (s)')
    
plt.tight_layout()"
"y_fft = fft(y)
freqs = fftfreq(s_nr, 1 / s_rate)
mask = freqs > 0

fig, axs = plt.subplots(3, 1, figsize=(15, 10))
axs[0].plot(freqs[mask], 2 / s_nr * np.abs(y_fft[mask]))

axs[1].plot(freqs[mask], 2 / s_nr * np.abs(y_fft[mask]), marker='.')
axs[1].set_xlim(47.5, 52.5)

axs[2].plot(freqs[mask], 2 / s_nr * np.abs(y_fft[mask]), marker='.')
axs[2].set_xlim(48.9, 49.1)

axs[2].set_xlabel('frequency (Hz)')

for pos in np.arange(48.9, 49.1, 1 / s_time):
    axs[2].axvline(pos, ls='--', color='black')"
freqs > 0
"# Create simple background noise filter
freq_filter = 2 / s_nr * np.abs(y_fft) < 0.3
y_fft_filtered = y_fft.copy()
y_fft_filtered[freq_filter] = 0

# Reverse FFT on filtered data
y_filtered = ifft(y_fft_filtered)

mask = freqs > 0
fig, axs = plt.subplots(3, 1, figsize=(15, 10))

axs[0].plot(freqs[mask], 2 / s_nr * np.abs(y_fft[mask]))
axs[1].plot(freqs[mask], 2 / s_nr * np.abs(y_fft_filtered[mask]))

axs[2].plot(x, y_filtered)
axs[2].set_xlim(0, 1)"
"# Load filenames
filenames = sorted(glob.glob('data/quantum_hall/*'))

fig, axs = plt.subplots(1, 2, figsize=(12, 6))
         
for filename in filenames:
    data = np.genfromtxt(filename, dtype=""f8, f8 ,f8"", names=['b','vxx','vxy'], skip_header=7, delimiter=';')
        
    gate_voltage = filename.split('_')[-1][:-4]
        
    axs[0].plot(data['b'], data['vxx'] * 1e9, '-', label=gate_voltage)
    axs[1].plot(data['b'], data['vxy'] * 1e9, '-', label=gate_voltage)
    
for n in range(2, 7):
    axs[0].axhline(25812 / n, color='black', ls='--', lw=0.5)

axs[0].set_ylim(0, 25000)
axs[0].set_title('Hall resistance')
axs[0].set_xlabel('magnetic field B (T)')
axs[0].set_ylabel('Rxx (Ohm)')
axs[0].legend(loc=2)

axs[1].set_ylim(0, 7000)
axs[1].set_title('Longitudinal resistance') 
axs[1].set_xlabel('magnetic field B (T)')
axs[1].set_ylabel('Rxy (Ohm)')
axs[1].legend(loc=2);

plt.tight_layout()"
"string = ""abcdabcdabcy""
substr = ""abcdabcy""
firstOccurrence(string, substr)"
print(skip_list.head[1].head)
"import warnings
warnings.filterwarnings('ignore')
%matplotlib inline
import numpy as np
import matplotlib.pyplot as plt
import sympy as sp

N = 70 #define number of sample points
t=np.linspace(0, 2*np.pi, N)
t=t[0:-1]

x0=np.ones(N-1)
#x0=np.cos(2*t)

f = np.cos(2*t)*np.sin(5*t)
freq = np.fft.fftfreq(N-1, 1/(N-1))+.000000001 #define Fourier Transform sample frequencies

# Function resFun defines a residual term that will be used in the optimization operation
def resFun(x):
    X = np.fft.fft(x)
    xddot = np.fft.ifft(-freq**2*X)
    xdot = np.fft.ifft(1j*freq*X)
    res = xddot + xdot + x - f
    RES = np.sum(np.abs(res**2))
    return RES

# Goal of the function is to minimize error between solution guess x0 and response in order to
# converge to a solution
from scipy.optimize import minimize
sol = minimize(resFun, x0, method = 'BFGS', options={'maxiter':50000, 'disp':True})
#sol = minimize(resFun, x0)

print('Values of x after optimization:')
print(sol.x)

#Optional to plot Jacobian of the minimize function
#print(sol.jac)

A = (-3785/171769)
B = (-3274/171769)
C = (-11164/171769)
D = (7660/171769)

Analytical = (A)*np.cos(2*t)*np.cos(5*t) + (B)*np.sin(2*t)*np.sin(5*t) + (C)*np.cos(2*t)*np.sin(5*t)\
            + (D)*np.sin(2*t)*np.cos(5*t)

#Plot of numerical and analytical solutions
fig = plt.figure(figsize = (12,8))
plt.plot(t, sol.x, 'bo', label = 'MHB_Solution') #FFT solution plot
plt.plot(t, Analytical, 'r--', label = 'Analytical_Solution') #analytical steady state solution to the above posed problem
plt.legend(loc = 'upper right')
fig.suptitle('Plot of FFT and analytical solutions', fontsize = 14)
plt.xlabel('Time')
plt.ylabel('Displacement')

plt.grid()
plt.show()"
"%matplotlib inline
import numpy as np
import matplotlib.pyplot as plt
import sympy as sp

N = 60 #define number of sample points
t=np.linspace(0, 2*np.pi, N)
t = t[0:-1]


x0=1*np.ones(N-1)
#x0=np.cos(2*t)

f = np.cos(2*t)*np.sin(5*t)
freq = np.fft.fftfreq(N-1, 1/(N-1))+.000000001 #define Fourier Transform sample frequencies

# Function resFun defines a residual term that will be used in the optimization operation
def resFun(x):
    X = np.fft.fft(x)
    xddot = np.fft.ifft(-freq**2*X)
    xdot = np.fft.ifft(1j*freq*X)
    res = xddot + xdot + x + x**3 - f
    RES = np.sum(np.abs(res**2))
    return RES

# Goal of the function is to minimize error between solution guess x0 and response in order to
# converge to a solution
from scipy.optimize import minimize
sol = minimize(resFun, x0, method = 'BFGS', options={'maxiter':50000, 'disp':True})
#sol = minimize(resFun, x0)

print('Values of x after optimization:')
print(sol.x)

#Optional to plot Jacobian of the minimize function
#print(sol.jac)

#Numerical solution for comparison
from scipy.integrate import odeint

def deriv(x, t):
    return np.array([x[1], -0.25*x[1] - x[0] - x[0]**3 + np.cos(2*t)*np.sin(5*t)])

time = np.linspace(0.0, 200, 2000)
xinit=np.array([0,0])
x = odeint(deriv, xinit, time)

#Plot of numerical and analytical solutions
fig = plt.figure(figsize = (12,8))
plt.plot((60.02*np.pi)+t, sol.x, 'bo', label = 'MHB_Solution') #FFT solution plot
plt.plot(time, x[:, 0], 'r--', label = 'Numerical_Solution') #analytical steady state solution to the above posed problem
plt.legend(loc = 'upper right')
fig.suptitle('Plot of FFT and numerical solutions', fontsize = 14)
plt.xlabel('Time')
plt.ylabel('Displacement')
plt.axis([186, 198, -0.25, 0.25])
plt.grid()
plt.show()"
"%matplotlib inline
import numpy as np
import matplotlib.pyplot as plt
import sympy as sp

N = 70 #define number of sample points
t=np.linspace(0, 2*np.pi, N)
t=t[0:-1]

F = 1

#x0=1.5*np.ones(N-1)
#x0=1.48*np.cos(1*t)
x0=1.5*np.sin(4*t)*np.cos(1*t)

f = F*np.cos(1*t)*np.sin(4*t)
freq = np.fft.fftfreq(N-1, 1/(N-1))+.00000000001 #define Fourier Transform sample frequencies

# Function resFun defines a residual term that will be used in the optimization operation
def resFun(x):
    X = np.fft.fft(x)
    xddot = np.fft.ifft(-freq**2*X)
    xdot = np.fft.ifft(1j*freq*X)
    res = xddot + (x**2 - 1)*xdot + x - f
    RES = np.sum(np.abs(res**2))
    return RES

# Goal of the function is to minimize error between solution guess x0 and response in order to
# converge to a solution
from scipy.optimize import minimize
sol = minimize(resFun, x0, method = 'BFGS', options={'maxiter':50000, 'disp':True})
#sol = minimize(resFun, x0)

print('Values of x after optimization:')
print(sol.x)

#Optional to plot Jacobian of the minimize function
#print(sol.jac)

#Numerical solution
from scipy.integrate import odeint
def deriv(x,t):
    return np.array([x[1], -(x[0]**2 - 1)*x[1] - x[0] + F*np.cos(1*t)*np.sin(4*t)])

time=np.linspace(0.0,200,2000)
xinit=np.array([-2, 0])
x=odeint(deriv, xinit, time)

#Plot of numerical and analytical solutions
fig = plt.figure(figsize = (12,8))
plt.plot((57.325*np.pi)+t, sol.x, 'bo', label = 'MHB_Solution') #FFT solution plot
plt.plot(time, x[:,0] , 'r--', label = 'Numerical_Solution') #analytical steady state solution to the above posed problem
plt.legend(loc = 'upper right')
fig.suptitle('Plot of FFT and analytical solutions', fontsize = 14)
plt.xlabel('Time')
plt.ylabel('Displacement')
plt.axis([175, 192, -3, 3])
plt.grid()
plt.show()"
"%matplotlib inline
import numpy as np
import matplotlib.pyplot as plt
import sympy as sp

N = 50 #define number of sample points
t=np.linspace(0, 2*np.pi, N)
t=t[0:-1]

F = 2

#x0=1*np.ones(N-1)
x0=1*np.cos((2*np.pi*t)/1)
#x0=2.0*np.sin(1*t)*np.cos(1*t)

f = F*np.cos((2*np.pi*t))
freq = np.fft.fftfreq(N-1, 1/(N-1))+.00000000001 #define Fourier Transform sample frequencies

# Function resFun defines a residual term that will be used in the optimization operation
def resFun(x):
    X = np.fft.fft(x)
    xddot = np.fft.ifft(-freq**2*X)
    xdot = np.fft.ifft(1j*freq*X)
    res = xddot + (x**2 - 1)*xdot + x - f
    RES = np.sum(np.abs(res**2))
    return RES

# Goal of the function is to minimize error between solution guess x0 and response in order to
# converge to a solution
from scipy.optimize import minimize
sol = minimize(resFun, x0, method = 'BFGS', options={'maxiter':50000, 'disp':True})
#sol = minimize(resFun, x0)

print('Values of x after optimization:')
print(sol.x)

#Optional to plot Jacobian of the minimize function
#print(sol.jac)

#Numerical solution
from scipy.integrate import odeint
def deriv(x,t):
    return np.array([x[1], -(x[0]**2 - 1)*x[1] - x[0] + F*np.cos((2*np.pi*t))])

time=np.linspace(0.0,210,2000)
xinit=np.array([-2.0, 0])
x=odeint(deriv, xinit, time)

#Plot of numerical and analytical solutions
fig = plt.figure(figsize = (12,8))
plt.plot((61.75*np.pi)+t, sol.x, 'bo', label = 'MHB_Solution') #FFT solution plot
plt.plot(time, x[:,0] , 'r--', label = 'Numerical_Solution') #analytical steady state solution to the above posed problem
plt.legend(loc = 'upper right')
fig.suptitle('Plot of FFT and numerical solutions', fontsize = 14)
plt.xlabel('Time')
plt.ylabel('Displacement')
plt.axis([190, 205, -3, 3])
plt.grid()
plt.show()"
"#Define function to be called in the ""leastsq"" function
# sol.x is the method of harmonic balance solution
def errorFun(p, t):
    A0, A1, A2, A3, A4, A5, A6, A7, A8, omega, beta = p
    err = sol.x - (A0 + A1*np.cos(omega*t + beta) + A2*np.cos(2*omega*t + 2*beta)\
                   + A3*np.cos(3*omega*t + 3*beta) + A4*np.cos(4*omega*t + 4*beta)\
                   + A5*np.cos(5*omega*t + 5*beta) + A6*np.cos(6*omega*t + 6*beta)\
                   + A7*np.cos(7*omega*t + 7*beta) + A8*np.cos(8*omega*t + 8*beta))
    return err

#Initial guess for coefficients
p0 = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]
#print(np.array(p0))

#Least square calculation
from scipy.optimize import leastsq
plsq = leastsq(errorFun, p0, args=(t))

#Function used for plotting purposes
def peval(t, p):
    return p[0] + p[1]*np.cos(p[9]*t + p[10]) + p[2]*np.cos(2*p[9]*t + 2*p[10])\
                + p[3]*np.cos(3*p[9]*t + 3*p[10]) + p[4]*np.cos(4*p[9]*t + 4*p[10])\
                + p[5]*np.cos(5*p[9]*t + 5*p[10]) + p[6]*np.cos(6*p[9]*t + 6*p[10])\
                + p[7]*np.cos(7*p[9]*t + 7*p[10]) + p[8]*np.cos(8*p[9]*t + 8*p[10])

#Plot numerical and least square results
import matplotlib.pyplot as plt
fig = plt.figure(figsize=(12,8))
plt.plot(t, peval(t, plsq[0]), 'b--', linewidth = 4, label = 'Least_Square_Result')
plt.plot(t, sol.x, 'ro', label = 'Numerical_Result')
plt.legend(loc = 'upper right')
plt.axis([-1, 7, -3, 3])
plt.grid()
plt.show()

coeff = plsq[0]

#Plot calculated results the coefficients
print('Coefficient values listed below')
print(plsq[0])"
"import sympy as sp
sp.init_printing(use_latex='mathjax')

time=sp.Symbol('t', real = True)

A0=sp.nsimplify(round(coeff[0], 2))
A1=sp.nsimplify(round(coeff[1], 2))
A2=sp.nsimplify(round(coeff[2], 2))
A3=sp.nsimplify(round(coeff[3], 2))
A4=sp.nsimplify(round(coeff[4], 2))
A5=sp.nsimplify(round(coeff[5], 2))
A6=sp.nsimplify(round(coeff[6], 2))
A7=sp.nsimplify(round(coeff[7], 2))
A8=sp.nsimplify(round(coeff[8], 2))
omega=sp.nsimplify(round(coeff[9], 2))
beta=sp.nsimplify(round(coeff[10], 2))

ans = A0 + A1*sp.cos(omega*time + beta) + A2*sp.cos(2*omega*time + 2*beta)\
         + A3*sp.cos(3*omega*time + 3*beta) + A4*sp.cos(4*omega*time + 4*beta)\
         + A5*sp.cos(5*omega*time + 5*beta) + A6*sp.cos(6*omega*time + 6*beta)\
         + A7*sp.cos(7*omega*time + 7*beta) + A8*sp.cos(8*omega*time + 8*beta)
ans            "
"import warnings
warnings.filterwarnings('ignore')
%matplotlib inline
import numpy as np
import matplotlib.pyplot as plt
import sympy as sp

N = 49 #define number of sample points
t=np.linspace(0, 2*np.pi, N)
t=t[0:-1]

F = 1

#x0=1.5*np.ones(N-1)
#x0=1.48*np.cos(1*t)
x0=3.0*np.sin(4*t)*np.cos(1*t)

f = F*np.cos(1*t)*np.sin(4*t)
freq = np.fft.fftfreq(N-1, 1/(N-1))+.00000000001 #define Fourier Transform sample frequencies

# Function resFun defines a residual term that will be used in the optimization operation
def resFun(x):
    X = np.fft.fft(x)
    xddot = np.fft.ifft(-freq**2*X)
    xdot = np.fft.ifft(1j*freq*X)
    res = xddot + (x**2 - 1)*xdot + x - f
    RES = np.sum(np.abs(res**2))
    return RES

# Goal of the function is to minimize error between solution guess x0 and response in order to
# converge to a solution
from scipy.optimize import minimize
sol = minimize(resFun, x0, method = 'BFGS', options={'maxiter':50000, 'disp':True})
#sol = minimize(resFun, x0)

print('Values of x after optimization:')
print(sol.x)

#Optional to plot Jacobian of the minimize function
#print(sol.jac)

#Numerical solution
from scipy.integrate import odeint
def deriv(x,t):
    return np.array([x[1], -(x[0]**2 - 1)*x[1] - x[0] + F*np.cos(1*t)*np.sin(4*t)])

time=np.linspace(0.0,100,2000)
xinit=np.array([-2, 0])
x=odeint(deriv, xinit, time)

#Plot of numerical and analytical solutions
fig = plt.figure()
plt.plot((28.35*np.pi)+t, sol.x, 'bo') #FFT solution plot
plt.plot(time, x[:,0] , 'r--') #analytical steady state solution to the above posed problem
fig.suptitle('Plot of numerical and analytical solutions', fontsize = 14)
plt.xlabel('Time')
plt.ylabel('Displacement')
plt.axis([80, 100, -3, 3])
plt.grid()
plt.show()"
"#Define function to be called in the ""leastsq"" function
def errorFun(p, t):
    A0, A1, A2, A3, A4, A5, A6, A7, A8, omega, beta = p
    err = sol.x - (A0 + A1*np.cos(omega*t + beta) + A2*np.cos(2*omega*t + 2*beta)\
                   + A3*np.cos(3*omega*t + 3*beta) + A4*np.cos(4*omega*t + 4*beta)\
                   + A5*np.cos(5*omega*t + 5*beta) + A6*np.cos(6*omega*t + 6*beta)\
                   + A7*np.cos(7*omega*t + 7*beta) + A8*np.cos(8*omega*t + 8*beta))
    return err

#Initial guess for coefficients
p0 = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]
#print(np.array(p0))

#Least square calculation
from scipy.optimize import leastsq
plsq = leastsq(errorFun, p0, args=(t))

#Function used for plotting purposes
def peval(t, p):
    return p[0] + p[1]*np.cos(p[9]*t + p[10]) + p[2]*np.cos(2*p[9]*t + 2*p[10])\
                + p[3]*np.cos(3*p[9]*t + 3*p[10]) + p[4]*np.cos(4*p[9]*t + 4*p[10])\
                + p[5]*np.cos(5*p[9]*t + 5*p[10]) + p[6]*np.cos(6*p[9]*t + 6*p[10])\
                + p[7]*np.cos(7*p[9]*t + 7*p[10]) + p[8]*np.cos(8*p[9]*t + 8*p[10])

#Plot numerical and least square results
import matplotlib.pyplot as plt
fig = plt.figure()
plt.plot(t, peval(t, plsq[0]), 'b--', linewidth = 2)
plt.plot(t, sol.x, 'ro')
plt.axis([-1, 7, -3, 3])
plt.grid()
plt.show()

coeff = plsq[0]

#Plot calculated results the coefficients
print('Coefficient values listed below')
print(plsq[0])"
"import warnings
warnings.filterwarnings('ignore')
%matplotlib inline
import numpy as np
import matplotlib.pyplot as plt
import sympy as sp

N = 51 #define number of sample points
t=np.linspace(0, 2*np.pi, N)
t=t[0:-1]

x0=np.ones(N-1)
#x0=np.cos(2*t)

f = np.cos(2*t)
freq = np.fft.fftfreq(N-1, 1/(N-1))+.0000001 #define Fourier Transform sample frequencies

# Function resFun defines a residual term that will be used in the optimization operation
def resFun(x):
    X = np.fft.fft(x)
    xddot = np.fft.ifft(-freq**2*X)
    xdot = np.fft.ifft(1j*freq*X)
    res = xddot + xdot + x - f
    RES = np.sum(np.abs(res**2))
    return RES

# Goal of the function is to minimize error between solution guess x0 and response in order to
# converge to a solution
from scipy.optimize import minimize
sol = minimize(resFun, x0, method = 'BFGS', options={'maxiter':50000, 'disp':True})
#sol = minimize(resFun, x0)

print('Values of x after optimization:')
print(sol.x)

#Optional to plot Jacobian of the minimize function
#print(sol.jac)

#Plot of numerical and analytical solutions
fig = plt.figure()
plt.plot(t, sol.x, 'bo') #FFT solution plot
plt.plot(t, (2/13)*np.sin(2*t) - (3/13)*np.cos(2*t), 'r--') #analytical steady state solution to the above posed problem
fig.suptitle('Plot of numerical and analytical solutions', fontsize = 14)
plt.xlabel('Time')
plt.ylabel('Displacement')

plt.grid()
plt.show()"
"#Define function to be called in the ""leastsq"" function
def errorFun(p, t):
    A0, A1, A2, A3, omega, beta = p
    err = sol.x - (A0 + A1*np.cos(omega*t + beta) + A2*np.cos(2*omega*t + 2*beta) + A3*np.cos(3*omega*t + 3*beta))
    return err

#Initial guess for coefficients
p0 = [1, 1, 1, 1, 1, 1]
#print(np.array(p0))

#Least square calculation
from scipy.optimize import leastsq
plsq = leastsq(errorFun, p0, args=(t))

#Function used for plotting purposes
def peval(t, p):
    return p[0] + p[1]*np.cos(p[4]*t + p[5]) + p[2]*np.cos(2*p[4]*t + 2*p[5]) + p[3]*np.cos(3*p[4]*t + 3*p[5])

#Plot numerical and least square results
import matplotlib.pyplot as plt
fig = plt.figure()
plt.plot(t, peval(t, plsq[0]), 'b--')
plt.plot(t, sol.x, '-o')

plt.grid()
plt.show()

coeff = plsq[0]

#Plot calculated results the coefficients
print('Coefficient values listed below')
print(plsq[0])"
"import warnings
warnings.filterwarnings('ignore')
%matplotlib inline
import numpy as np
import matplotlib.pyplot as plt

N = 50 #define number of sample points
t=np.linspace(0, 2*np.pi, N)
t=t[0:-1]

x0=np.ones(N-1)
#x0=np.cos(2*t)

f = np.cos(2*t)
freq = np.fft.fftfreq(N-1, 1/(N-1))+.0000001 #define Fourier Transform sample frequencies

# Function resFun defines a residual term that will be used in the optimization operation
def resFun(x):
    X = np.fft.fft(x)
    xddot = np.fft.ifft(-freq**2*X)
    res = xddot + x - f
    RES = np.sum(np.abs(res**2))
    return RES

# Goal of the function is to minimize error between solution guess x0 and response in order to
# converge to a solution
from scipy.optimize import minimize
sol = minimize(resFun, x0, method = 'BFGS', options={'maxiter':50000, 'disp':True})
#sol = minimize(resFun, x0)

print('Values of x after optimization:')
print(sol.x)

#Optional to plot Jacobian of the minimize function
#print(sol.jac)

#Plot of numerical and analytical solutions
fig = plt.figure()
plt.plot(t, sol.x, 'bo') #FFT solution plot
plt.plot(t, -(1/3)*np.cos(2*t), 'r--') #analytical steady state solution to the above posed problem
fig.suptitle('Plot of numerical and analytical solutions', fontsize = 14)
plt.xlabel('Time')
plt.ylabel('Displacement')

plt.grid()
plt.show()"
"#Define function to be called in the ""leastsq"" function
def errorFun(p, t):
    A0, A1, A2, A3, omega, beta = p
    err = sol.x - (A0 + A1*np.cos(omega*t + beta) + A2*np.cos(2*omega*t + 2*beta) + A3*np.cos(3*omega*t + 3*beta))
    return err

#Initial guess for coefficients
p0 = [1, 1, 1, 1, 1, 1]
#print(np.array(p0))

#Least square calculation
from scipy.optimize import leastsq
plsq = leastsq(errorFun, p0, args=(t))

#Function used for plotting purposes
def peval(t, p):
    return p[0] + p[1]*np.cos(p[4]*t + p[5]) + p[2]*np.cos(2*p[4]*t + 2*p[5]) + p[3]*np.cos(3*p[4]*t + 3*p[5])

#Plot numerical and least square results
import matplotlib.pyplot as plt
fig = plt.figure()
plt.plot(t, peval(t, plsq[0]), 'b--', linewidth = 3)
plt.plot(t, sol.x, 'y-o')

plt.grid()
plt.show()

coeff = plsq[0]

#Plot calculated results the coefficients
print('Coefficient values listed below')
print(plsq[0])"
"plot(volts,ampers,'o')"
"popt, pcov = curve_fit(lin,volts,ampers,p0=(1,-0.3))
plot(volts,ampers,'o')
plot(volts,lin(volts,*popt))
print(popt)"
"# Given force and point coordinates
F = 5.1
A = np.array([0, 1.2, 1.9])
B = np.array([5.3, 4, 4.9])

# Vector AB
AB = B - A
AB"
np.linalg.norm(AB)
"# Unit vector along AB
uAB = AB / np.linalg.norm(AB)
uAB"
"# Vector F_ is the magnitude F times the unit vector uAB
F_ = F * uAB
F_"
"# theta_x is arccos(F_x/F)
theta_x = np.arccos(F_[0] / F) * (180/np.pi)
theta_x"
"# Magnitude of T_proj_AC
T_AC = np.dot(T_, uAC)
T_AC"
"R = F1 + F2 + F3 + F4
R"
"Mo = np.cross(r2, F2) + np.cross(r3, F3) + np.cross(r4, F4)
Mo"
"# Find the direction vectors for each tension
AB = B - A
AC = C - A
AD = D - A
print(""AB = {} \nAC = {} \nAD = {}"".format(AB, AC, AD))"
"# Find the unit vectors for each tension
uAB = AB / np.linalg.norm(AB)
uAC = AC / np.linalg.norm(AC)
uAD = AD / np.linalg.norm(AD)
print(""uAB = {} \nuAC = {} \nuAD = {}"".format(uAB, uAC, uAD))"
"# All tensions must equal the weight
b = np.array([0,0,mg])
print(b)"
"# Calculate distance from line CD to point M
x = 0.5 * L * np.tan(30*np.pi/180)

# Calculate height of base triangle
h = L * np.sin(60*np.pi/180)

# Position vectors of each point
A = np.array([0, 0, H])
B = np.array([-x, -L/2, 0])
C = np.array([h-x, 0, 0])
D = np.array([-x, L/2, 0])

# Truss section vectors
AB = B - A
AC = C - A
AD = D - A

# Truss section unit vectors
uAB = AB / np.linalg.norm(AB)
uAC = AC / np.linalg.norm(AC)
uAD = AD / np.linalg.norm(AD)

# Create a 3x3 matrix
M = np.zeros(shape=(3,3))

# Populate cols of matrix with unit vectors
M[:, 0] = uAB
M[:, 1] = uAC
M[:, 2] = uAD

# Force vector at A
b = F * uF

T = np.linalg.solve(M, -b)
T_AB = T[0]
T_AC = T[1]
T_AD = T[2]

print(""T_AB = {:6.1f} lb"".format(T_AB))
print(""T_AC = {:6.1f} lb"".format(T_AC))
print(""T_AD = {:6.1f} lb"".format(T_AD))"
"# Plot the analytical solution
x = np.linspace(0,10)
def plot(x,t):
    axes = plt.gca()
    axes.set_xlim([0,10.6]); axes.set_ylim([200,450])
    plt.plot(x,t, lw=6, label='Analytical Solution')
    plt.xlabel('$x$ (m)'); plt.ylabel('$T$ (K)')
plot(x,T(x))
plt.legend(loc=2)
plt.show()"
"# Plot finite difference
xi = np.linspace(1,9,9)
plot(x,T(x))
plt.plot(xi,Ti,'D', ms=11, label='Finite-Difference Approximation (10 segments)')
plt.legend(loc=2)
plt.show()"
"# Plot both methods (zoomed)
xj = np.linspace(2, 8, 4)
axes = plt.gca()
axes.set_xlim([7.99,8.01]); axes.set_ylim([335.4,336.4])
plt.plot(x,T(x), lw=6, label='Analytical Solution')
plt.plot(xi,Ti,'D', ms=11, label='Finite-Difference (10 segments)')
plt.plot(xj,Tj,'o', ms=13, label='Finite-Difference (5 segments)')
plt.xlabel('$x$ (m)'); plt.ylabel('$T$ (K)')
plt.legend(loc=2)
plt.show()"
"# Lecture 36 - Neumann boundary conditions
# Method (a) with forward difference

A = np.array([[ -1,    1,    0,    0,    0],
              [  1, -2.2,    1,    0,    0],
              [  0,    1, -2.2,    1,    0],
              [  0,    0,    1, -2.2,    1],
              [  0,    0,    0,    1, -2.2]])
b = np.array([0,-40,-40,-40,-440])
Ti = np.linalg.solve(A,b)
Ti"
"# Lecture 36 - Chapra text example 24.6
# Method (b) with central difference

A = np.array([[ 2.2,  -2,   0,   0,    0],
              [  -1, 2.2,  -1,   0,    0],
              [   0,  -1, 2.2,  -1,    0],
              [   0,   0,  -1, 2.2,   -1],
              [   0,   0,   0,  -1, 2.2]])
b = np.array([40,40,40,40,440])
Tj = np.linalg.solve(A,b)
Tj"
"# Plot both methods (zoomed)
x  = np.linspace(0,10)
xi = np.linspace(0,8,5)
axes = plt.gca()
axes.set_xlim([0,10.6]); axes.set_ylim([200,450])
plt.plot(x,T(x), lw=6, label='Analytical Solution')
plt.plot(xi,Ti,'D', ms=11, label='(a) Forward Difference')
plt.plot(xi,Tj,'o', ms=11, label='(b) Central Difference')
plt.xlabel('$x$ (m)'); plt.ylabel('$T$ (K)')
plt.legend(loc=2)
plt.show()"
"A = np.array([[-5, 1, 0],
              [1, -6, 1],
              [0, 1, -6]])

b = np.array([f(1), f(2), f(3)-1])
y = np.linalg.solve(A,b)
y"
"t = np.linspace(0,2)
plt.plot(t,y(t),lw=6,label=""Analytical Solution"")
plt.plot(T1,Y1,'D-',lw=2,ms=10,label=""Heun's Method"")
plt.plot(T2,Y2,'o-',lw=2,ms=10,label=""Heun's Method (with corrector)"")
plt.plot(T3,Y3,'^-',lw=2,ms=12,label=""Midpoint Method"")
plt.plot(T4,Y4,'s-',lw=2,ms=10,label=""Ralston's Method"")
plt.legend(loc=2)
plt.show()"
"plt.plot(x,y(x), lw=6, label=""Analytical Solution"")
plt.plot(X,Y*10**5, ""D-"", lw=2, label=""Finite Difference Method"")
plt.legend(bbox_to_anchor=[0.665, 0.81])
plt.xlabel(""distance, $x$ (m)"")
plt.ylabel(""deflection, $y$ (10$^{-5}$ m)"")
plt.show()"
"# The given interval
a = 0
b = (np.pi)/2

# The given parametric equation
t = np.linspace(a, b)
x = np.exp(-t) * np.cos(t)
y = np.exp(-t) * np.sin(t)

# Let's graph it
plt.title('The curve')
plt.plot(x, y)
plt.grid();"
"m_r = Q_H / (h1 - h2)
m_r"
"COP_max = 1 / (((Tw1)/(-34+273)-1))
COP_max"
"W_in_min = Q_L / COP_max
W_in_min"
"wins = 0
losses = 0
rounds = []
for x in range(0,100000):
    money = 1000
    rounds_till_win = 0
    counter = 0
    while money > 0 and money < 1001:
        counter += 1
        x = randint(1,38)
        if x <= 18:
            money += (2**(rounds_till_win))
            rounds_till_win = 0
        else:
            money -= 2**(rounds_till_win)
            rounds_till_win += 1
    if money > 1:
        wins += 1
    else:
        losses += 1
    rounds.append(counter)
    if rounds_till_win > 10:
        print(""HELL0"")
    

        
print(wins,""/"", losses)
print(""win perc:"",wins/(wins+losses))
print(""lose perc:"",losses/(wins+losses))
print(""avg rounds:"", sum(rounds)/len(rounds))"
(wins - (losses*(2**10)))/100000
"x,y = zip(*[(x,((1-((20/38)**x))*1) - (((20/38)**x)*((2**x)-1))) for x in range(1,100)])
plt.plot(x,y)"
"for sym, op in ops.items():
    for fnum in first_num:
        for snum in second_num:
            """"""
            if switch == 1:
                fnum = fnum
                snum = snum
            elif switch == 2 and fnum <5:
                fnum = factorial(fnum)
                snum = snum
            elif switch == 3 snum <5:
                fnum = fnum
                snum = factorial(snum)
            elif switch == 4 and fnum <5 and snum <5:
                fnum = factorial(fnum)
                snum = factorial(snum)
            """"""

            # Combination of operations with 2 numbers
            if sorted([str(fnum),str(snum)]) == sorted(str(op(fnum,snum))):
                print(fnum, sym, snum,"" = "",op(fnum,snum), ""FOUND IT"")

            # Combinations of exponents
            if sorted(str(op(fnum**snum,1))) == sorted([str(fnum),str(snum),'1']):
                print(""("",fnum,""^"",snum,"")"",sym,'1',"" = "",op(fnum**snum,1),""HOORAY"")
            if sorted(str(fnum**op(snum,1))) == sorted([str(fnum),str(snum),'1']):
                print(fnum,""("",snum,sym,1,"") = "", fnum**op(snum,1), ""Wooo"")
            if sorted(str(op(fnum,1)**snum)) == sorted([str(fnum),str(snum),'1']):
                print(fnum,""("",snum,sym,1,"") = "", op(fnum,1)**snum, ""Ummm"")

            # Factorials
            if sorted([str(fnum),str(snum)]) == sorted(str(op(factorial(fnum),snum))):
                print(fnum,sym,snum,""! = "", op(factorial(fnum),snum), ""Tada!"")
            
            for sym2, op2 in ops.items():
                
                if sorted([str(fnum),str(snum),'1']) == sorted(str(op2(op(factorial(fnum),snum),1))):
                    print(fnum,sym,snum,""! = "", op2(op(factorial(fnum),snum),1), ""Tada!"")
                if factorial(fnum) * factorial(snum) < 2000:
                    if sorted([str(fnum),str(snum),'1']) == sorted(str(op2(op(factorial(fnum),factorial(snum)),1))):
                        print(fnum,sym,snum,""! = "", op2(op(factorial(fnum),factorial(snum)),1), ""Tada blah!"")
                    if sorted([str(fnum),str(snum),'1']) == sorted(str(op2(op(factorial(fnum),1),factorial(snum)))):
                        print(fnum,snum,1)
            
            if sorted([str(fnum),str(snum)]) == sorted(str(op(factorial(fnum),snum**(-1)))):
                print(fnum,snum,1)
            
            #print(sorted([str(fnum),str(snum)]), sorted(str(op(fnum,snum))))

"
"color_svd_slider = FloatSlider(min=1, max=rgb_img.shape[0], step=1, value=1)

# i is the number of singular values used to make the image
@interact(i=color_svd_slider)
def plot_image_using_singular_values(i):
    i = int(i)
    total_bytes = 0
    
    red = rgb_img[:, :, 0]
    green = rgb_img[:, :, 1]
    blue = rgb_img[:, :, 2]
    
    rgbArray = np.zeros(rgb_img.shape, 'uint8')
    for ix, color_matrix in enumerate([red, green, blue]):
        U, S, V = sp.linalg.svd(color_matrix)
        total_bytes += np.matrix(U[:,:i]).size + S[:i].size + np.matrix(V[:i, :]).size
        rgbArray[:,:, ix] = np.dot(U[:,:i], np.dot(np.diag(S[:i]), V[:i, :]))
        
    print(""Bytes Used:"", total_bytes)
    print(""Compression Ratio"", total_bytes/np.prod(rgb_img.shape))
    
    plt.imshow(rgbArray, cmap=plt.cm.gray)"
"plt.imshow(optimal_compression(rgb_img, cost_function, 'rgb'), cmap=plt.cm.gray)"
"# Color frequencies (sorted)
plt.xlabel(""Grayscale Value"")
plt.ylabel(""Count"")
plt.hist(gray_img.flatten(), bins=256)
plt.show()"
"no_loss = np.dot(U, np.dot(sp.linalg.diagsvd(Sigma,gray_img.shape[0],gray_img.shape[1]), Vh))
plt.imshow(no_loss, cmap=plt.cm.gray)"
"# Percentage of pixels in original image as a function of the grayscale pixel value

img_values, img_counts = np.unique(gray_img.flatten(), return_counts=True)
img_count_percs = img_counts/gray_img.flatten().size
plt.xlabel(""Grayscale Value"")
plt.ylabel(""Percentage of Pixels in Original Image"")
plt.plot(img_values, img_count_percs)"
"# See how the number of singular values used effects the visual quality of an 
# image and the amount of storage neeeded
U, Sigma, Vh = sp.linalg.svd(gray_img)

gray_img_svd_slider = FloatSlider(min=1, max=gray_img.shape[0], step=1, value=1)

# i is the number of singular values used to make the image
@interact(i=gray_img_svd_slider)
def plot_image_using_singular_values(i):
    i = int(i)
    total_bytes = np.matrix(U[:,:i]).size + Sigma[:i].size + np.matrix(Vh[:i, :]).size
    print(""Bytes Used:"", total_bytes)
    print(""Compression Ratio"", total_bytes/np.prod(gray_img.shape))
    
    U_1 = np.matrix(U[:,:i])
    Sigma_1 = np.diag(Sigma[:i])
    V_t = np.matrix(Vh[:i, :])
    
    result = U_1* Sigma_1 * V_t
    histogram_data = result.flatten().T
    
    #plt.hist(histogram_data, bins=256)
    plt.imshow(result, cmap=plt.cm.gray)"
"gray_img_fft_slider = FloatSlider(min=0, max=.01, step=0.000001, value=800)

img_fft = fft.fft2(gray_img)
M = np.max(np.abs(img_fft))
# i is the number of singular values used to make the image
@interact(i=gray_img_fft_slider)
def plot_image_using_fft2(i):
    truncated_image = (np.abs(img_fft) > M*i) * img_fft    

    #total_bytes = np.sum(np.abs(truncated_image > 0))
    #print(""Bytes Used:"", total_bytes)
    #print(""Compression Ratio"", total_bytes/num_pixels)
    
    reconstructed_image = np.real(fft.ifft2(truncated_image))
    
    #plt.hist(histogram_data, bins=256)
    plt.imshow(reconstructed_image, cmap=plt.cm.gray)"
"# See how the number of singular values used effects distribution of grayscale values
# used in the reconstructed image
U, Sigma, Vh = sp.linalg.svd(gray_img)
pdf_slider = FloatSlider(min=1, max=gray_img.shape[0], step=1, value=800)

def set_max(a):
        return a if (a<256) else 255

# i is the number of singular values used to make the image
@interact(i=pdf_slider)
def plot_reconstructed_image_pdf(i):
    i = int(i)

    approx_A = np.dot(U[:,:i], np.dot(np.diag(Sigma[:i]), Vh[:i, :]))
    approx_A_list = approx_A.flatten()
    approx_A_list = approx_A_list.astype(int)

    vfunc = np.vectorize(set_max)
    rounded_approx_A_list = vfunc(approx_A_list)

    approx_A_values, approx_A_counts = np.unique(rounded_approx_A_list, return_counts=True)
    approx_A_count_percs = approx_A_counts/np.prod(gray_img.shape)
    
    plt.xlabel(""Grayscale Value"")
    plt.ylabel(""Percentage of Pixels in Reconstructed Image"")
    plt.plot(approx_A_values, approx_A_count_percs)"
"U, Sigma, Vh = sp.linalg.svd(gray_img)
compression_ratios = []
approximation_errors = []

for i in range(1,gray_img.shape[0]):
    compression_ratios.append((np.matrix(U[:,:i]).size + Sigma[:i].size + np.matrix(Vh[:i, :]).size)/np.prod(gray_img.shape))
    approx_A = np.dot(U[:,:i], np.dot(np.diag(Sigma[:i]), Vh[:i, :]))
    approximation_errors.append(((LA.norm(gray_img - approx_A)**2)/(LA.norm(gray_img)**2))*100)

plt.xlabel(""Compression Ratio"")
plt.ylabel(""Error"")
plt.title(""SVD Compression Efficiency"")
plt.plot(compression_ratios, approximation_errors)"
"print(""All permutations of standard tableau for shape:"") 
shape.displayAllSYTPermutations()
print(""==============================================="")"
"for x in [1,2,3,4,5,10,100]:
    print(""Fraction of collision-free permutations when n={0} and r=500: {1}"".format(x, f(x,500)))"
"for x in [1,2,3,4,5,10,100]:
    temp = [f(x,500) for _ in range(0,100)]
    print(""Median fraction of collision-free permutations after 100 trials when n={0} and r=500: {1}"".format(x,np.median(temp)))   "
"xn = np.linspace(-L/2, L/2, N + 1) #N+1 equally spaced sampling points

def f(x):
    return np.exp(-x**2 / (2*xw**2) ) * np.cos(kw * x)

fn = f(xn)
exact = plt.plot(xn, fn)
plt.title(""Plot of a gaussian wavepacket in the time domain"", fontsize = 16)
plt.xlabel(""t / s"", fontsize = 16)
plt.ylabel(""Amplitude"", fontsize = 16)
plt.xlim(-6,6)
plt.ylim(-1.5, 1.5)
plt.legend([""Exact function""])"
"def a(kr):
    ap = kw + kr
    am = kw - kr
    return 1/2 * (np.exp(-ap**2 * xw **2 / 2) + np.exp(-am**2 * xw **2 / 2))
                  
r = np.arange(29)
kr = 2*pi*r/L

plt.plot(kr, a(kr))
plt.plot([kw, kw], [0,0.5], ""--k"")
plt.plot([0, 20], [0.25,0.25], ""--k"")
dk = 1 / xw
plt.plot([kw+dk, kw+dk], [0,0.25], ""--k"")
plt.plot([kw-dk, kw-dk], [0,0.25], ""--k"")
plt.title(""Plot of a the fourier coefficient a_r against wavenumber k_r"", fontsize = 16)
plt.xlabel(""k_r / m^-1"", fontsize = 16)
plt.ylabel(""a_r"", fontsize = 16)
plt.xlim(0,20)
plt.ylim(0, 0.5)
kr[13], kw, a(kr[13]), 2/ xw"
"@np.vectorize
def f_from_fourier(x, a, n):
    kr = 2*pi/L * np.arange(1,n)
    return a(0)/2.0 + sum(a(kr)*np.cos(kr*x))


f_f = f_from_fourier(xn, a, 29)

f, (main, sub) = plt.subplots(2)
exact = main.plot(xn, fn)
fourier = main.plot(xn, f_f)
delta = sub.plot(xn, fn - f_f)

main.set_title(""Plot of a gaussian wavepacket in the time domain"", fontsize = 16)
sub.set_title(""Error between the two"", fontsize = 16)
plt.xlabel(""t / s"", fontsize = 16)
main.set_ylabel(""Amplitude"", fontsize = 16)
sub.set_ylabel(""Amplitude Difference"", fontsize = 16)
sub.plot((-6,6), (max(fn - f_f),max(fn - f_f)), ""--k"")
sub.plot((-6,6), (min(fn - f_f),min(fn - f_f)), ""--k"")

plt.xlim(-6,6)
plt.ylim(-1.5, 1.5)
main.legend([""Exact function"", ""Fourier series with 29 terms""])
sub.legend([""Error""])
print(""largest mag of delta f: "", max(abs(fn - f_f)))"
"import warnings
warnings.filterwarnings('ignore')
#use a high order polynomial ansatz starting from a flat function
x0 = [0, 0, 0, 1]
def E(f):
    gamma_glass = 1.0
    gamma_air = 1.0
    rho = 1000.0
    g = 9.81
    "
"
def logplot_Z(Z):
    f, (ax2, ax1) = plt.subplots(2,1, sharex =""col"")
    ws = np.logspace(0, 10, 200)
    ax1.loglog(ws, abs(Z(ws)))
    ax1.set_ylabel(""Ω"")
    
    ax2.set_xscale('log')
    ax2.set_ylabel(""phase"")
    plt.xticks(
        10**np.arange(0, 10) * 2 * pi,
            [""1Hz"", ""10Hz"", ""100Hz"", ""1kHz"", ""10kHz"", ""100kHz"", ""1MHz"", ""10MHz"", '100MHz', '1GHz'])
    ax2.plot(ws, np.angle(Z(ws)))
    
def logplot_output(O):
    V, Z = O
    f, (ax2, ax1) = plt.subplots(2,1, sharex =""col"")
    ws = np.logspace(0, 10, 200)
    ax1.loglog(ws, abs(V(ws)))
    ax1.set_ylabel(""V"")
    
    ax2.set_xscale('log')
    ax2.set_ylabel(""|Z|"")
    plt.xticks(
        10**np.arange(0, 10) * 2 * pi,
    [""1Hz"", ""10Hz"", ""100Hz"", ""1kHz"", ""10kHz"", ""100kHz"", ""1MHz"", ""10MHz"", '100MHz', '1GHz'])
    #ax2.set_yrange([1e2, 1e-2])
    ax2.plot(ws, abs(Z(ws)))
    

notch_filter = parra(Z(""10Ω""), parra(Z(""100uF""), Z(""100uH"")))
logplot_Z(notch_filter)
    "
"high_pass = series(Z(""1Ω""), Z('10uF'))
logplot_Z(high_pass)"
"#define a thevenin source as a (V(w), Z(w)) tuple

#define filters in terms of what effect they have on a thevenin source
def passive_filter(Z1, Z2):
    def apply_to(thevenin):
        Vin, Zin = thevenin
        def Vout(w): return Vin(w) * Z2(w) / (Z1(w) + Z2(w) + Zin(w))
        def Zout(w): return Z2(w) * (Z1(w) + Zin(w)) / (Z1(w) + Z2(w) + Zin(w)) 
        return Vout, Zout
        
    return apply_to

stiff_source = (Z(""1V""), Z(""0Ω""))

Res = Z(""1kΩ"")
f = w(""20kHz"")
Cap = C(1.0 / (Res(0) * f))
low_pass = passive_filter(Res, Cap)
output = low_pass(stiff_source)

logplot_output(output)
"
"#what happens if we chain the same two low pass filters together?
output = low_pass(low_pass(stiff_source))
logplot_output(output)"
"#That didn't work because the output impedance of the first filter was too high
#and the input impedance of the second too low compared to it.
#If we define a high impedance low pass first it should work better
Res = Z(""100kΩ"")
f = w(""20kHz"")
Cap = C(1.0 / (Res(0) * f))
highZ_low_pass = passive_filter(Res, Cap)

output = highZ_low_pass(low_pass(stiff_source))
logplot_output(output)"
"#That still didn't really work, what about a high pass and low pass together?
Res = Z(""1kΩ"")
f = w(""100Hz"")
Cap = C(1.0 / (Res(0) * f))
high_pass = passive_filter(Cap, Res)

output = highZ_low_pass(high_pass(stiff_source))
logplot_output(output)"
"def buffer(factor):
    def apply_to(thevenin):
        Vin, Zin = thevenin
        def Vout(w): return Vin(w)
        def Zout(w): return Zin(w) * factor
        return Vout, Zout
        
    return apply_to


buf = buffer(1e-3)
output = highZ_low_pass(buf(high_pass(stiff_source)))
logplot_output(output)"
"#high emphasis filter
Res = Z(""1kΩ"")
f = w(""1kHz"")
Cap = C(1.0 / (Res(0) * f))

emph = passive_filter(parra(Res, Cap), Res)
logplot_output(emph(stiff_source))"
"#ex 1.44 from Art of electronics
ground_path = parra(Z(""120pF""), Z(""1MΩ""))
signal_path = parra(C(120e-12 /9), R(1e6 * 9)) #should be 9 but that makes it underflow
overall = passive_filter(signal_path, ground_path)
out = overall(stiff_source)
logplot_output(out)
V, _ = out
abs(V(w(""1Hz"")))"
"R1 = series(Z(""680pF""), Z(""30kΩ""))
R2 = parra(Z(""220kΩ""), Z(""2.2pF""))
def gain(w): return -R2(w) / R1(w)
logplot_Z(gain)"
"def normal(x, mu, sigma):
    return exp(-(x - mu)**2/(2*sigma**2))/(sigma*sqrt(2*pi))
mu1, sigma1, x = symbols(""mu_1 sigma_1 x"", real = True, positive = True)
N1 = normal(x, mu1, sigma1)

#check its normalised
assert(integrate(N1, (x, -oo, oo) )  == 1)

N1"
"mu2, sigma2 = symbols(""mu_2 sigma_2"", real = True, positive = True)
N2 = normal(x, mu2, sigma2)
N2"
"#fun fun fun
P = integrate(N1 * N2, (x, -oo, oo) )
P"
"A0, C0, An, theta, r = symbols(""A_0 C_0 A_n theta r"")
a, b = symbols(""a b"", positive = True)
n = symbols(""n"", positive = True, integer = True)
An = integrate(sin(n*theta), (theta, 0, pi)) / integrate(sin(n*theta)**2, (theta, -pi, pi)) / a**n 
An.simplify()"
"f = phi.subs({r : 1.0, a : 1.0, b : 2.0}).doit()
plot(f, (theta, -pi, pi))"
"from sympy.plotting.plot import plot3d
from sympy.abc import x, y
f = phi.subs({r : sqrt(x**2 + y**2), theta: atan2(y,x) ,a : 1.0, b : 2.0}).doit()
plot3d(f, (x, -3, 3), (y, -3, 3))"
expr
"solve(expr, x)"
m
m.inv()
m.det()
plot(x**2)
"interest = topics.apply(pd.Series.value_counts)
interest = interest.reindex(['Very interested', 'Interested',
                             'Only slightly interested', 'Not interested'])

%matplotlib inline
from IPython.core.pylabtools import figsize
figsize(10, 8)
interest.T.sort_values('Very interested').plot(kind='bar', stacked=True)"
"import warnings
warnings.filterwarnings('ignore')
import matplotlib.pyplot as plt
import numpy as np
import math
%matplotlib inline

points = np.array([[0, 6], [0, 4], [1, 5], [2, 1], [2, 4], [3, 0], [3, 5],
                   [1, 3], [2, 6], [3, 3], [5, 2], [5, 4]])
partitions = np.array([0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1])

colors = partitions / partitions.max()
plt.axis([0, 7, 0, 7])
plt.scatter(points[:, 0], points[:, 1], c=colors, s=100)
plt.grid()
plt.show()"
"points = np.array([[3, 8], [4, 7], [3, 6], [3, 4],
                   [4, 5], [5, 5], [5, 2], [8, 4], [9, 4], [9, 1]])
partitions = np.array([0, 0, 0, 0, 1, 1, 1, 1, 1, 1])

colors = partitions / partitions.max()
plt.axis([0, 10, 0, 10])
plt.scatter(points[:, 0], points[:, 1], c=colors, s=100)
plt.show()"
"classes, centers = kmeans(points, k=2)
plot(points, classes, centers)"
"# Plot parameters
font = {'family' : 'serif','color'  : 'darkred', 'weight' : 'normal', 'size'   : 14};
for n in range(0,VectSize):
    plt.plot(waveLengthVect,RadianceMat[n][:], label = TemperatureVect[n],);
    plt.fill_between(waveLengthVect,RadianceMat[n][:], alpha=0.05*n, facecolor='k');
plt.axis('auto');
plt.title('Distribution at each temperature', fontdict=font); 
plt.xlabel('Wavelength [m]', fontdict=font);
plt.ylabel('Spectral Radiance [$W m^{-2}nm^{-1}sr^{-1}$]', fontdict=font);
plt.grid();
plt.legend();
plt.show();"
"# Data analysis
for n in range(0,VectSize):
    CurrentData = RadianceMat[n][:];
    MaxDataMat[n][0] = waveLengthVect[CurrentData.index(max(CurrentData))];
    MaxDataMat[n][1] = max(CurrentData);
    
MaxDataArry = np.asarray(MaxDataMat)
TempArry = np.asarray(TemperatureVect);

plt.plot(TemperatureVect,MaxDataArry[:,1], marker='o', linestyle='--', color='b');
plt.axis('auto');
plt.title('Radiance Maxima for Temperatures', fontdict=font); 
plt.xlabel('Temperature [K]', fontdict=font);
plt.ylabel('Max Radiance [$W m^{-2}nm^{-1}sr^{-1}$]', fontdict=font);
plt.grid();
plt.show();
  
Frame = pd.DataFrame(MaxDataMat,index=TemperatureVect, columns=['Max Radiance WL [m]','Max Radiance','Total Energy']);
Frame"
"for n in range(0,VectSize):
    plt.plot(waveLengthVect,DerivMat[n][:], label = TemperatureVect[n],);
    plt.fill_between(waveLengthVect,DerivMat[n][:], alpha=0.05*n, facecolor='k');
plt.axis('auto');
plt.title('First Derivative of distributions', fontdict=font); 
plt.xlabel('Wavelength [nm]', fontdict=font);
plt.ylabel('Function Derivative', fontdict=font);
plt.grid();
plt.legend();
plt.show();"
"# Find Derivative Zeros - all zeros are assumed to be between global maximum and minimum
MinDataMat = [[0 for x in range(2)] for x in range(VectSize)];

# Loop to find individual maximum at each temperature
CurrentMax = 0;
for n in range(0,VectSize):
    CurrentData = DerivMat[n][:];
    MaxIndex = CurrentData.index(max(CurrentData));
    MinIndex = CurrentData.index(min(CurrentData));
    TGTIntval = DerivMat[n][MaxIndex:MinIndex]
    IntvalMin = min(abs(i) for i in TGTIntval)
    try:
        a = CurrentData.index(IntvalMin);

    except: 
        a = CurrentData.index(-IntvalMin);
    MinDataMat[n][0] = waveLengthVect[a];
    MinDataMat[n][1] = RadianceMat[n][a]
    CurrentData = RadianceMat[n][a];
    
     
    if CurrentMax < CurrentData:
        MaxRadiance =  RadianceMat[n][a];
        MaxWlength = waveLengthVect[a];
        Temperature = TemperatureVect[n];
    else:
        CurrentMax = RadianceMat[n][a];
        
print(""The Black Body with Temperature: %s, has the Maximum radiance: %s which is reached at Wavelength: %s."" %(Temperature,MaxRadiance,MaxWlength))

FrameB = pd.DataFrame(MinDataMat,index=TemperatureVect, columns=['Max Radiance WL [m]','Max Radiance']);
FrameB"
"MinDataArry = np.asarray(MinDataMat);
plt.plot(TemperatureVect,MinDataArry[:,0], marker='o', linestyle='--', color='r',label='Actual');
plt.plot([TemperatureVect[0],TemperatureVect[VectSize-1]],[MinDataArry[0,0],MinDataArry[VectSize-1,0]], marker='o', linestyle='--', color='g',label='Idea');
plt.axis('auto');
plt.title('Wavelngth/Temperature ralationship', fontdict=font); 
plt.xlabel('Temperature [K]', fontdict=font);
plt.ylabel('Wavelength [m]', fontdict=font);
plt.grid();
plt.legend();
plt.show();"
"# Plot regression results
DataScale = 10e14;
plt.errorbar(TempArry,MinDataArry[:,0], xerr=SigmaX*DataScale, yerr=SigmaY*DataScale,fmt='o', color='r', capthick=1,label='Data STD');
plt.plot(TempArry,Yr(TempArry), marker='o', linestyle='--', color='b',label='R.Regression');
plt.plot(TempArry,Y(TempArry), marker='o', linestyle='--', color='g',label='Regression',alpha=0.6);
         
plt.title('Linear Fit Data analysis - STD', fontdict=font); 
plt.xlabel('Temperature [K]', fontdict=font);
plt.ylabel('Wavelength [m]', fontdict=font);
plt.grid();
plt.legend();
plt.show();"
"# Temperature array
TempData = np.linspace(min(TempArry),max(TempArry),20);

# Plot Data
plt.plot(TempData,Yr(TempData), marker='o', linestyle='-', color='b',label='R.Regression');
plt.plot(TempData,Y(TempData), marker='o', linestyle='--', color='g',label='Regression',alpha=0.7);
plt.plot(TempData,WLw(TempData), marker='o', linestyle='--', color='k',label=""Wien's Law"");
plt.axis('auto');
plt.title('Wavelngth/Temperature ralationship', fontdict=font); 
plt.xlabel('Temperature [K]', fontdict=font);
plt.ylabel('Wavelength [m]', fontdict=font);
plt.grid();
plt.legend();
plt.show();"
"# Fixed temperature
FxdTemp = 4000; #[K]
WLSet = [1.146939e-5,4.146939e-06,6.204082e-07,1e-18];
SetSize = len(WLSet);
FxdData = [[0 for x in range(2)] for x in range(SetSize)];

BF = lambda waveLength: (2*h*c**2)/waveLength**5*(1/(np.exp((h*c)/(waveLength*Kb*FxdTemp))-1));

for n in range(SetSize):
    FxdData[n][0] = WLSet[n];
    FxdData[n][1] = BF(WLSet[n]);
    
FxdDataArry = np.asarray(FxdData);"
"# Compute interpolant coefficients and polynomial
Coeff = coeffts(FxdDataArry[:,0],FxdDataArry[:,1]);
PolyData = [0]*dataSize;
print(""Polynomial coefficients: %s"" %(Coeff));

# Evaluate set with polynomial
WLSet = np.linspace(WLSet[0],WLSet[SetSize-1],dataSize)
for n in range(dataSize):
    PolyData[n] = evalPoly(Coeff,FxdDataArry[:,0],WLSet[n]);
    
# Rayleigh-Jeans Law
BRJ = lambda waveLength: (2*c*Kb*FxdTemp)/waveLength**4;"
"# Plot Data
plt.plot(WLSet,PolyData, marker='o', linestyle='-', color='b',label='Interpolation');
plt.plot(WLSet,BRJ(WLSet), marker='o', linestyle='--', color='g',label='Rayleigh-Jeans Law',alpha=0.6);
plt.axis('auto');
plt.title('Rayleigh-Jeans Law Comparison', fontdict=font); 
plt.xlabel('Wavelength [m]', fontdict=font);
plt.ylabel('Spectral Radiance [$W m^{-2}nm^{-1}sr^{-1}$]', fontdict=font);
plt.grid();
plt.legend();
plt.show();"
"# Original code: http://w3mentor.com/learn/python/scientific-computation/gauss-legendre-m-point-quadrature-in-python/

''' x,A = gaussNodes(m,tol=10e-9)
    Returns nodal abscissas {x} and weights {A} of
    Gauss-Legendre m-point quadrature.
'''
from math import cos,pi
from numpy import zeros
 
def gaussNodes(m,tol=10e-9):
    
    if m<2:
        raise Exception(""m parameter must be greater than 1."");
 
    def legendre(t,m):
        p0 = 1.0; p1 = t;
        for k in range(1,m):
            p = ((2.0*k + 1.0)*t*p1 - k*p0)/(1.0 + k )
            p0 = p1; p1 = p
        dp = m*(p0 - t*p1)/(1.0 - t**2)
        return p,dp;
 
    A = zeros(m);
    x = zeros(m);
    nRoots = (m + 1)//2          # Number of non-neg. roots
    
    for i in range(nRoots):
        t = cos(pi*(i + 0.75)/(m + 0.5))  # Approx. root
        for j in range(30): 
            p,dp = legendre(t,m);         # Newton-Raphson
            dt = -p/dp; t = t + dt        # method         
            if abs(dt) < tol:
                x[i] = t; x[m-i-1] = -t
                A[i] = 2.0/(1.0 - t**2)/(dp**2) # Eq.(6.25)
                A[m-i-1] = A[i];
                break;
    return x,A
nRoots = 2;
[roots,Coeff] = gaussNodes(nRoots);
print('Number of roots: ', nRoots);
print('Roots =',roots);
print('Coefficients = ',Coeff);"
"## Define parameters as global variables
N = 5000  # sample size
p = 5     # number of coefficients
sigma = 1

# make design matrix
X = np.column_stack((np.repeat(1, N), np.random.rand(N, p - 1)))
print(X.shape)

# assign true coefficients
true_theta = np.random.normal(0, 3, p)
print(true_theta)

# generate response vector
y = X.dot(true_theta) + np.random.normal(0, sigma, N)"
"for i in range(m.theta_out.shape[1]):
    plt.axhline(true_theta[i], linestyle = 'dashed', color = 'black')
    plt.plot(range(m.theta_out.shape[0]), m.theta_out[:, i], lw = 2)
    plt.xlabel('iteration')
    plt.ylabel(r'$\theta$')"
"import os
os.getcwd()"
"kinetic_energy(N, Pa)"
"kinetic_energy(N, Pa, A)"
"Pa.potential_energy = m * g * h
A.potential_energy = M * g * H
potential_energy(A, Pa)"
"Lagrangian(N, Pa, A)"
%whos
%history
"from sklearn import linear_model
lr = linear_model.LogisticRegression(C=1)
# This class implements regularized logistic regression. C is the Inverse of regularization strength.
# Large value => no regularization.

lr.fit(X, y)
y_pred_lr = lr.predict(X)

errors =  y_pred_lr != y
print(""Nb errors=%i, error rate=%.2f"" % (errors.sum(), errors.sum() / len(y)))
print(lr.coef_)"
"from sklearn import linear_model
lrl1 = linear_model.LogisticRegression(penalty='l1')
# This class implements regularized logistic regression. C is the Inverse of regularization strength.
# Large value => no regularization.

lrl1.fit(X, y)
y_pred_lrl1 = lrl1.predict(X)

errors =  y_pred_lrl1 != y
print(""Nb errors=%i, error rate=%.2f"" % (errors.sum(), errors.sum() / len(y_pred_lrl1)))
print(lrl1.coef_)"
"from sklearn import svm

svmlin = svm.LinearSVC()
# Remark: by default LinearSVC uses squared_hinge as loss
svmlin.fit(X, y)
y_pred_svmlin = svmlin.predict(X)

errors =  y_pred_svmlin != y
print(""Nb errors=%i, error rate=%.2f"" % (errors.sum(), errors.sum() / len(y_pred_svmlin)))
print(svmlin.coef_)"
"from sklearn import svm

svmlinl1 = svm.LinearSVC(penalty='l1', dual=False)
# Remark: by default LinearSVC uses squared_hinge as loss

svmlinl1.fit(X, y)
y_pred_svmlinl1 = svmlinl1.predict(X)

errors =  y_pred_svmlinl1 != y
print(""Nb errors=%i, error rate=%.2f"" % (errors.sum(), errors.sum() / len(y_pred_svmlinl1)))
print(svmlinl1.coef_)"
"from sklearn import metrics
score_pred = np.array([.1 ,.2, .3, .4, .5, .6, .7, .8])
y_true = np.array([0, 0, 0, 0, 1, 1, 1, 1])
thres = .9
y_pred = (score_pred > thres).astype(int)

print(""Predictions:"", y_pred)
metrics.accuracy_score(y_true, y_pred)

# The overall precision an recall on each individual class
p, r, f, s = metrics.precision_recall_fscore_support(y_true, y_pred)
print(""Recalls:"", r)
# 100% of specificity, 0% of sensitivity

# However AUC=1 indicating a perfect separation of the two classes
auc = metrics.roc_auc_score(y_true, score_pred)
print(""AUC:"", auc)"
"import warnings
warnings.filterwarnings('ignore')
import numpy as np
import scipy
import matplotlib.pyplot as plt
import seaborn as sns
%matplotlib inline

np.random.seed(42)
 
# dataset
n_samples = 100
experience = np.random.normal(size=n_samples)
salary = 1500 + experience + np.random.normal(size=n_samples, scale=.5)
X = np.column_stack([experience, salary])

# PCA using SVD
X -= X.mean(axis=0)  # Centering is required
U, s, Vh = scipy.linalg.svd(X, full_matrices=False)
explained_variance_ = (s ** 2) / n_samples
explained_variance_ratio_ = (explained_variance_ /
                             explained_variance_.sum())
Xproj = np.dot(X, Vh.T)

# U : Unitary matrix having left singular vectors as columns.
#     Of shape (n_samples,n_samples) or (n_samples,n_comps), depending on
#     full_matrices.
#
# s : The singular values, sorted in non-increasing order. Of shape (n_comps,), 
#     with n_comps = min(n_samples, n_features).
#
# Vh: Unitary matrix having right singular vectors as rows. 
#     Of shape (n_features, n_features) or (n_comps, n_features) depending 
# on full_matrices.


plt.figure(figsize=(9, 3)) 

plt.subplot(131)
plt.scatter(X[:, 0], X[:, 1], s=50)
for i in range(Vh.shape[0]):
    plt.arrow(x=0, y=0, dx=Vh[i, 0], dy=Vh[i, 1], head_width=0.2, 
              head_length=0.2, linewidth=2, fc='r', ec='r')
plt.axis('equal')
plt.ylim(-4, 4)

plt.title(""Original data"")
plt.xlabel(""experience"")
plt.ylabel(""salary"")

plt.subplot(132)
plt.scatter(Xproj[:, 0], Xproj[:, 1], s=50)
plt.axis('equal')
plt.title(""Rotated data"")

plt.subplot(133)
assert np.allclose(Xproj / s,  U)
plt.scatter(U[:, 0], U[:, 1], s=50)
plt.axis('equal')
plt.title(""Rotated / scaled data: U"")
plt.xlabel(""U1 (PC1)"")
plt.ylabel(""U2 (PC2)"")

plt.tight_layout()"
"import numpy as np
from sklearn.decomposition import PCA

X = np.column_stack([experience, salary])
pca = PCA(n_components=2)
pca.fit(X)
print(pca.explained_variance_ratio_)
assert np.all(Xproj == pca.transform(X))"
"import warnings
warnings.filterwarnings('ignore')
import numpy as np
from scipy.stats import f
import matplotlib.pyplot as plt
%matplotlib inline

fvalues = np.linspace(.1, 5, 100)

# pdf(x, df1, df2): Probability density function at x of F.
plt.plot(fvalues, f.pdf(fvalues, 1, 30), 'b-', label=""F(1, 30)"")
plt.plot(fvalues, f.pdf(fvalues, 5, 30), 'r-', label=""F(5, 30)"")
plt.legend()

# cdf(x, df1, df2): Cumulative distribution function of F.
# ie. 
proba_at_f_inf_3 = f.cdf(3, 1, 30) # P(F(1,30) < 3)

# ppf(q, df1, df2): Percent point function (inverse of cdf) at q of F.
f_at_proba_inf_95 = f.ppf(.95, 1, 30) # q such P(F(1,30) < .95)
assert f.cdf(f_at_proba_inf_95, 1, 30) == .95

# sf(x, df1, df2): Survival function (1 - cdf) at x of F.
proba_at_f_sup_3 = f.sf(3, 1, 30) # P(F(1,30) > 3)
assert  proba_at_f_inf_3 + proba_at_f_sup_3 == 1

# p-value: P(F(1, 30)) < 0.05
low_proba_fvalues = fvalues[fvalues > f_at_proba_inf_95]
plt.fill_between(low_proba_fvalues, 0, f.pdf(low_proba_fvalues, 1, 30),
                 alpha=.8, label=""P < 0.05"")
plt.show()"
"# compute with scipy
tval, pval = stats.ttest_1samp(x, 1.75)

#tval = 2.1598800019529265 # assume the t-value
tvalues = np.linspace(-10, 10, 100)
plt.plot(tvalues, stats.t.pdf(tvalues, n-1), 'b-', label=""T(n-1)"")
upper_tval_tvalues = tvalues[tvalues > tval]
plt.fill_between(upper_tval_tvalues, 0, stats.t.pdf(upper_tval_tvalues, n-1), alpha=.8, label=""p-value"")
plt.legend()"
"import numpy as np
import pandas as pd
import scipy.stats as stats

# Dataset:
# 15 samples:
# 10 first with canalar tumor, 5 last without
canalar_tumor = np.array([1] * 10 + [0] * 5)
# 8 first with metastasis, 6 without, the last with.
meta = np.array([1] * 8 + [0] * 6 + [1])

crosstab = pd.crosstab(canalar_tumor, meta, rownames=['canalar_tumor'], colnames=['meta'])
print(""Observed table:"")
print(""---------------"")
print(crosstab)

chi2, pval, dof, expected = stats.chi2_contingency(crosstab)
print(""Statistics:"")
print(""-----------"")
print(""Chi2=%f, pval=%f"" % (chi2, pval))
print(""Expected table:"")
print(""---------------"")
print(expected)"
"# Compute expected cross-table based on proportion
meta_marg = crosstab.sum(axis=0)
meta_freq = meta_marg / meta_marg.sum()
canalar_tumor_marg = crosstab.sum(axis=1)
canalar_tumor_freq = canalar_tumor_marg / canalar_tumor_marg.sum()

print('Canalar tumor frequency ? Yes:%.2f' % canalar_tumor_freq[0], 'No:%.2f' % canalar_tumor_freq[1])
print('Metastasis frequency ? Yes:%.2f' % meta_freq[0], 'No:%.2f' % meta_freq[1])

print('Expected frequecies:')
print(np.outer(canalar_tumor_freq, meta_freq))

print('Expected cross-table:')
print(np.outer(canalar_tumor_freq, meta_freq) * len(canalar_tumor))"
"import numpy as np
import scipy.stats as stats
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

np.random.seed(seed=42)  # make example reproducible

n = 50
noutliers = 10
x = np.random.normal(size=n)
y = 2 * x + np.random.normal(size=n)
y[:noutliers] = np.random.normal(loc=-10, size=noutliers)  # Add 40 outliers
outlier = np.array([""N""] * n)
outlier[:noutliers] = ""Y""

# Compute with scipy
cor, pval = stats.spearmanr(x, y)
print(""Non Parametric Spearman cor test, cor %4.f, pval%4.f"" % (cor, pval))


# Plot distribution + pairwise scatter plot
df = pd.DataFrame(dict(x=x, y=y, outlier=outlier))
g = sns.PairGrid(df, hue=""outlier"")
g.map_diag(plt.hist)
g.map_offdiag(plt.scatter)
g = g.add_legend()

# Compute the parametric Pearsonw cor test
print(""Parametric Pearson cor test:"", stats.pearsonr(x, y))"
"import scipy.stats as stats
n = 20
# Buisness Volume time 0
bv0 = np.random.normal(loc=3, scale=.1, size=n)
# Buisness Volume time 1
bv1 = bv0 + 0.1 + np.random.normal(loc=0, scale=.1, size=n)

# create an outlier
bv1[0] -= 10

# Paired t-test
print(stats.ttest_rel(bv0, bv1))

# Wilcoxon
print(stats.wilcoxon(bv0, bv1))"
"import scipy.stats as stats
n = 20
# Buismess Volume group 0
bv0 = np.random.normal(loc=1, scale=.1, size=n)

# Buismess Volume group 1
bv1 = np.random.normal(loc=1.2, scale=.1, size=n)

# create an outlier
bv1[0] -= 10

# Two-samples t-test
print(stats.ttest_ind(bv0, bv1))

# Wilcoxon
print(stats.mannwhitneyu(bv0, bv1))"
"from scipy import stats
import numpy as np
y, x = salary.salary, salary.experience
beta, beta0, r_value, p_value, std_err = stats.linregress(x,y)
print(""y=%f x + %f,  r:%f, r-squared:%f, \np-value:%f, std_err:%f"" 
      % (beta, beta0, r_value, r_value**2, p_value, std_err))
# plotting the line
yhat = beta * x  +  beta0 # regression line
plt.plot(x, yhat, 'r-', x, y,'o')
plt.xlabel('Experience (years)')
plt.ylabel('Salary')
plt.show()"
"import numpy as np
import scipy
np.random.seed(seed=42)  # make the example reproducible

# Dataset
N, P = 50, 4
X = np.random.normal(size= N * P).reshape((N, P))
## Our model needs an intercept so we add a column of 1s:
X[:, 0] = 1
print(X[:5, :])

betastar = np.array([10, 1., .5, 0.1])
e = np.random.normal(size=N)
y = np.dot(X, betastar) + e

# Estimate the parameters
Xpinv = scipy.linalg.pinv2(X)
betahat = np.dot(Xpinv, y)
print(""Estimated beta:\n"", betahat)"
"import statsmodels.api as sm

## Fit and summary:
model = sm.OLS(y, X).fit()
print(model.summary())

# prediction of new values
ypred = model.predict(X)

# residuals + prediction == true values
assert np.all(ypred + model.resid == y)"
"import statsmodels.formula.api as smfrmla

df = pd.DataFrame(np.column_stack([X, y]), columns=['inter', 'x1','x2', 'x3', 'y'])

# Build a model excluding the intercept, it is implicit
model = smfrmla.ols(""y ~ x1 + x2 + x3"", df).fit()
print(model.summary())"
"import statsmodels.formula.api as smfrmla

oneway = smfrmla.ols('salary ~ management + experience', salary).fit()
print(oneway.summary())
aov = sm.stats.anova_lm(oneway, typ=2) # Type 2 ANOVA DataFrame
print(aov)"
"import statsmodels.formula.api as smfrmla

twoway = smfrmla.ols('salary ~ education + management + experience', salary).fit()
print(twoway.summary())
aov = sm.stats.anova_lm(twoway, typ=2) # Type 2 ANOVA DataFrame
print(aov)"
"print(sm.stats.anova_lm(oneway, twoway))
# or
print(twoway.compare_f_test(oneway))  # return F, pval, df"
"print(twoway.model.data.param_names)
print(twoway.model.data.exog[:10, :])"
"# t-test of the specific contribution of experience:
ttest_exp = twoway.t_test([0, 0, 0, 0, 1])
ttest_exp.pvalue, ttest_exp.tvalue
print(ttest_exp)

# Alternatively, you can specify the hypothesis tests using a string
twoway.t_test('experience')

# Post-hoc is salary of Master different salary of Ph.D ? 
# ie. t-test salary of Master = salary of Ph.D
print(twoway.t_test('education[T.Master] = education[T.Ph.D]'))"
"import numpy as np
np.random.seed(seed=42)  # make example reproducible

# Dataset
import numpy as np
np.random.seed(seed=42)  # make example reproducible


# Dataset
n_samples, n_features = 100, 1000
n_info = int(n_features/10) # number of features with information
n1, n2 = int(n_samples/2), n_samples - int(n_samples/2)
snr = .5
Y = np.random.randn(n_samples, n_features)
grp = np.array([""g1""] * n1 + [""g2""] * n2)

# Add some group effect for Pinfo features
Y[grp==""g1"", :n_info] += snr

# 
import scipy.stats as stats
import matplotlib.pyplot as plt
tvals, pvals = np.full(n_features, np.NAN), np.full(n_features, np.NAN)
for j in range(n_features):
    tvals[j], pvals[j] = stats.ttest_ind(Y[grp==""g1"", j], Y[grp==""g2"", j],
                                         equal_var=True)

fig, axis = plt.subplots(3, 1)#, sharex='col')

axis[0].plot(range(n_features), tvals, 'o')
axis[0].set_ylabel(""t-value"")

axis[1].plot(range(n_features), pvals, 'o')
axis[1].axhline(y=0.05, color='red', linewidth=3, label=""p-value=0.05"")
#axis[1].axhline(y=0.05, label=""toto"", color='red')
axis[1].set_ylabel(""p-value"")
axis[1].legend()

axis[2].hist([pvals[n_info:], pvals[:n_info]], 
    stacked=True, bins=100, label=[""Negatives"", ""Positives""])
axis[2].set_xlabel(""p-value histogram"")
axis[2].set_ylabel(""density"")
axis[2].legend()

plt.tight_layout()"
"# здесь можно задать диапазон по х для вывода графика
x = np.linspace(-10, 10)
plt.grid()
plt.plot (x, a*x**2+b*x+c)
plt.plot (x, 0*x, 'r')
#plt.plot (x, x**2)"
"@interact
def showit (a=[-10,10,1], b=[-10,10,1], c=[-10,10,1]):
    x = np.linspace(-20, 20)
    plt.grid()
    plt.plot (x, a*x**2+b*x+c)
    plt.plot (x, 0*x, 'r')"
"# здесь считаем корни комплексные:

ca = complex(a); cb = complex(b); cc = complex(c)
cx1 = (-cb + np.sqrt(cb**2 - 4*ca*cc))/(2*ca)
cx2 = (-cb - np.sqrt(cb**2 - 4*ca*cc))/(2*ca)

print (""корни:"", cx1, cx2)"
"# здесь считаем корни действительные:

x1 = (-b + np.sqrt(b**2 - 4*a*c))/(2*a)
x2 = (-b - np.sqrt(b**2 - 4*a*c))/(2*a)

print (""корни:"", x1, x2)"
"# здесь можно задать диапазон по х для вывода графика
x = np.linspace(-10, 10)
plt.grid()
plt.plot (x, a*x**2+b*x+c)
plt.plot (x, 0*x, 'r')
#plt.plot (x, x**2)"
"# здесь считаем корни комплексные:

ca = complex(a); cb = complex(b); cc = complex(c)
cx1 = (-cb + np.sqrt(cb**2 - 4*ca*cc))/(2*ca)
cx2 = (-cb - np.sqrt(cb**2 - 4*ca*cc))/(2*ca)

print (""корни:"", cx1, cx2)"
"# здесь считаем корни действительные:

x1 = (-b + np.sqrt(b**2 - 4*a*c))/(2*a)
x2 = (-b - np.sqrt(b**2 - 4*a*c))/(2*a)

print (""корни:"", x1, x2)"
"# show data
print (head)
print (mul)
print (data)"
"# print info, pre-calculated in Google Calc
res1 = data[:,CODE:CODE+3]
res1"
"res['force'] = [np.sqrt(res['acc_x'][i] ** 2 +
                        res['acc_y'][i] ** 2 +
                        res['acc_z'][i] ** 2)
               for i in range(len(res['acc_x']))]
plt.plot(res['acc_x'], linewidth=0.3, label='Acceleration x-axis $m/s^2$')
plt.plot(res['acc_y'], linewidth=0.3, label='Acceleration y-axis $m/s^2$')
plt.plot(res['acc_z'], linewidth=0.3, label='Acceleration z-axis $m/s^2$')
plt.plot(res['force'], linewidth=0.7, label='Acceleration Force $m/s^2$')
plt.legend()
plt.show()"
"plt.plot(times, [s.speed for s in sensors], label='GNSS-Speed')
plt.xlabel('seconds')
plt.legend()
plt.show()"
"start, end = 0, 1000
acc = [s for s in sensors[start:end]]
t = times[start:end]
plt.plot(t, [s.x for s in acc], linewidth=0.5, label='Acc x')
plt.plot(t, [s.y for s in acc], linewidth=0.5, label='Acc y')
plt.plot(t, [s.z for s in acc], linewidth=0.5, label='Acc z')
plt.plot(t, [s.acc_force() for s in acc], linewidth=1.5, label='Force')
plt.legend()
plt.xlabel('seconds')
plt.show()"
"start, end = 300, 600
t = times[start:end]
accs = [s.acc for s in sensors[start:end]]
plt.plot(t, accs, label='IMU Acceleration', linewidth=0.5)
plt.plot(t, [est for est in g_h_filter(accs, 0, 0, 0.2, 0.5, 1.)], 'o', label='Filter Estimates', markersize=4.0)
plt.plot(t, [est for est in k_filter(accs, 0., 1.0, 1.0, 4.0)], label='K Filter Estimates')
plt.xlabel('seconds')
plt.legend()
plt.show()"
"# Interval in milliseconds
start, end = 0, 1000
plot_data(data1[start:end])
plot_data(data2[start:end])
plot_data(dataCA[start:end])"
"data = np.array([
    (2, 4), (2, 6), (3, 1), (3, 5),
    (3, 6), (3, 7), (4, 8), (5, 8),
    (5, 9), (6, 7), (7, 2), (7, 7),
    (8, 2), (8, 4), (9, 3), (9, 4),
])
plot_clusters(data, np.ones(len(data)))"
"class DBSCAN:
    
    def __init__(self, min_points, radius, distance):
        self.min_points = min_points
        self.radius = radius
        self.distance = distance
        self.points = None
        self.unvisited = None
        self.assignment = None

    def __call__(self, points):
        """"""
        Return the assignment as a vector of cluster numbers for each
        point. Cluster zero contains the noise points.
        """"""
        self.points = [tuple(x) for x in points]
        self.unvisited = [tuple(x) for x in points]
        self.assignment = {}
        cluster = 0
        while self.unvisited:
            point = self.unvisited.pop()
            neighbors = self.neighbors(point)
            assert point in neighbors
            # Points without enough neighbors are noise points, except if
            # they're take over later; see below.
            if len(neighbors) < self.min_points:
                self.assignment[point] = 0
            # Unvisited points with enough neighbors start a new cluster.
            else:
                cluster += 1
                self.expand(neighbors, cluster)
        return np.array([self.assignment[x] for x in self.points])
        
    def expand(self, points, cluster):
        while points:
            current = points.pop()
            # Consider points that are not yet part of a cluster
            if current in self.unvisited:
                self.unvisited.remove(current)
                self.assignment[current] = cluster
                neighbors = self.neighbors(current)
                # Base points help to expand further
                if len(neighbors) >= self.min_points:
                    points += neighbors
            # Noise points are taken over by real clusters
            elif current not in self.assignment or self.assignment[current] == 0:
                self.assignment[current] = cluster
        
    def neighbors(self, center):
        neighbors = []
        for point in self.points:
            if self.distance(np.array(center), np.array(point)) < self.radius:
                neighbors.append(point)
        return neighbors
    
    
assignment = DBSCAN(min_points=4, radius=2.2, distance=euclidean)(data)
plot_clusters(data, assignment)"
"import warnings
warnings.filterwarnings('ignore')
from sympy import *
init_printing()
from IPython.display import display
A=Matrix([[3,1], [5,2], [0,9]])
B=Matrix([[3,2], [5,2], [0,9]])
display(A)
display(B)
A==B"
eye(5)
zeros(5)
"A=Matrix([[1,2,3,4],[5,6,7,8],[9,10,11,12]])
display(A)
display(A.transpose())"
"A=Matrix([[12, 3, -4],[3, 6, 1]])
B=Matrix([[1,5,-2],[13,6,-1]])
A+B"
"A= Matrix([[1,2,3],[4,5,6],[7,8,9]])
10*A"
"A= Matrix([[2,3,0],[1,3,4],[3,7,2],[5,16,2]])
B= Matrix([[5,7,18],[0,8,1],[2,0,3]])
C= A*B
C"
"A=Matrix([[2,3,1],[5,2,0],[6,1,8]])
A**3"
"A = Matrix([[200,180,140,60],[80,40,120,120]])
B = A * 1.1
display(A, B)
2*30*A + 3*30*B"
"A = Matrix([[0.6,0.6,0.2],[1.0,0.9,0.3],[1.5,1.2,0.4]])
B = Matrix([[5,6],[6,7],[4,5]])
C = A*B
display(C)
print('Κόστος παραγωγής μιας καρέκλας στο εργοστάσιο Ε1=%.2f' % C[1,0])
print('Κόστος παραγωγής ενός πάγκου στο εργοστάσιο Ε2=%.2f' % C[0,1])"
"import warnings
warnings.filterwarnings('ignore')
from sympy import *
from IPython.display import display
init_printing()
I**1000"
"z = sqrt(3)+2*I
cz = conjugate(z)
display(z)
display(cz)
display(z*cz)
display(simplify(z*cz))"
"a = 2 + 5*I
b = 3 - 2*I
display(a+b)
display(a-b)
display(simplify(a*b))
display(expand(a**3))
display(simplify(a/b))"
"z= 5+5*I
r=  Abs(z)
phi = arg(z)
display(r)
display(phi)"
"import sys
print(sys.version)  
sys.version_info"
"from sympy import *
from IPython.display import display
init_printing()
x = Symbol('x')
f_x = x**2 +3*x +5
g_x = x**2-1
h_x = f_x*g_x
display(h_x)
display(expand(h_x))"
"n=Symbol('n') 
a_n = summation(n, [n,1,n])
display(a_n)
display(factor(a_n))
a_n.subs(n,100).evalf()"
"import warnings
warnings.filterwarnings('ignore')
from sympy import *
from IPython.display import display
init_printing()
A = Matrix([[12,1,7],[6,14,2],[7,31,12]])
display(A)
print('η ορίζουσα είναι', A.det())
display(A.inv())"
"A = Matrix([[5,-1,7],[7,0,6],[4,-1,8]])
B = 1/A.det() * A.adjugate()
display(A.inv())
display(B)"
"# 1. με τη χρήση αντίστροφου πίνακα
A= Matrix([[2,-3,1,1],[1,1,-4,-1],[3,2,0,3],[2,-2,7,-3]])
B= Matrix([8,2,6,0])
x = A.inv()*B
x"
"# 2. με τη μέθοδο του Cramer
A= Matrix([[2,-3,1,1],[1,1,-4,-1],[3,2,0,3],[2,-2,7,-3]])
Dx = Matrix([[8,-3,1,1],[2,1,-4,-1],[6,2,0,3],[0,-2,7,-3]])
Dy = Matrix([[2,8,1,1],[1,2,-4,-1],[3,6,0,3],[2,0,7,-3]])
Dz = Matrix([[2,-3,8,1],[1,1,2,-1],[3,2,6,3],[2,-2,0,-3]])
Dw = Matrix([[2,-3,1,8],[1,1,-4,2],[3,2,0,6],[2,-2,7,0]])
x = Dx.det()/A.det()
y = Dy.det()/A.det()
z = Dz.det()/A.det()
w = Dw.det()/A.det()
x,y,z,w"
"# 3. με τη συνάρτηση solve_linear_system του sympy
A_B= Matrix([[2,-3,1,1,8],[1,1,-4,-1,2],[3,2,0,3,6],[2,-2,7,-3,0]])
x, y, z, w = symbols('x, y, z, w')
results = solve_linear_system(A_B, x, y, z, w)
print(results)"
"# 4. με τη συνάρτηση solve του sympy και τη χρήση των εξισώσεων Eq
x, y, z, w = symbols('x, y, z, w')
solve([Eq(2*x -3*y + z + w, 8), Eq(x+ y - 4*z -w, 2), Eq(3*x+2*y+3*w, 6), Eq(2*x-2*y+7*z-3*w, 0)  ], [x, y, z, w])"
"limit(1/x,x,0, dir=""+"")"
"limit(1/x,x,0, dir=""-"")"
"f = (4*x**2-9*x+1)/(x+7)
display(f)
limit(f,x,3)"
"f = (x-3)/(x**2-9)
display(f)
limit(f,x,3)"
"f = (sqrt(x)-6)/(x-36)
display(f)
limit(f,x,36)"
"f=5*x**3-2*x+1
display(f)
limit(f,x,oo)"
"f=5/(x**3+8)
display(f)
limit(f,x,-oo)"
"f=(2*x**3+x-1)/(4*x**3-x**2+2)
display(f)
limit(f,x,oo)"
"x = Symbol('x')
f = log(1+x)
f"
"df = diff(f,x)
display(df)
d2f= diff(f,x,2)
display(d2f)
d3f= diff(f,x,3)
d4f= diff(f,x,4)
ms = f.subs(x,0) + df.subs(x,0)*x/factorial(1) + d2f.subs(x,0)*x**2/factorial(2) + d3f.subs(x,0)*x**3/factorial(3) 
ms = ms + d4f.subs(x,0)*x**4/factorial(4) 
ms"
"%matplotlib inline
from sympy.plotting import plot
import warnings
p1 = plot(f, xlim=(-1,1), ylim=(-1,1), show=False)
p2 = plot(ms, xlim=(-1,1), show=False, line_color='r')
p1.extend(p2)
p1.show()"
"f=sin(x)
t5p=0
for i in range(0,6):
    df = diff(f,x,i)
    print(""όρος %d %s %s %s"" % (i,df, df.subs(x,pi/3), df.subs(x,pi/3)/factorial(i)))
    t5p+=df.subs(x,pi/3)/factorial(i)*(x-pi/3)**i
t5p"
"f = log(1+x)
ms = series(f,x,0,3)
ms"
"ms=ms.removeO()
ms"
"p1 = plot(f, xlim=(-1,1), ylim=(-1,1), show=False)
p2 = plot(ms, xlim=(-1,1), show=False, line_color='r')
p1.extend(p2)
p1.show()"
"f = sin(x)
t5p = series(f,x,pi/3,5)
t5p"
"t5p = t5p.removeO()
t5p"
"import warnings
warnings.filterwarnings('ignore')
from math import pi
r = 2**10/5**2 + pi**4
r"
"from sympy import *
init_printing()
x,y = symbols('x,y')
expand((x+y)**10)"
"from IPython.display import display
n = Symbol('n')
a_n = summation(n**2, [n,1,n])
display(a_n)
display(factor(a_n))"
"A = Matrix([[7,3],[4,1]])
display(A.det())
display(A.inv())"
"# πολλαπλασιασμός πινάκa 3x2 με πίνακα 2x3
A = Matrix([[7,3],[4,1],[4,16]])
B = Matrix([[12,3,1],[5, 8,2]])
A * B"
"A = Matrix([[1,2,3,4,5],
           [2,4,6,8,10],
           [1,1,1,1,1],
           [0,1,1,0,1],
           [2,3,4,5,6]])
display(A)
# αν εισαχθεί η εντολή A.inv() θα εμφανιστεί το σφάλμα
# ValueError: Matrix det == 0; not invertible."
"# με συμβολικούς υπολογισμούς
from sympy import *
from IPython.display import display
init_printing()
i1,i2,i3=symbols('i1,i2,i3')
A=Matrix([[1,-1,-1,0],[8,2,0,4],[0,-2,2,4]])
display(A)
results = solve_linear_system(A,i1,i2,i3)
display(results)"
"from sympy import *
from IPython.display import display
init_printing()
i1,i2,i3,r=symbols('i1,i2,i3,r')
A=Matrix([[1,-1,-1,0],[5+r,2,0,4],[0,-2,2,4]])
display(A)
results = solve_linear_system(A,i1,i2,i3,r)
display(results)"
"# ταυτοτική συνάρτηση
expr = x 
plot(expr, (x,-10,10),ylim=(-10,10), title= expr);"
"# συνάρτηση 1 προς x
expr = 1/x 
plot(expr, (x,-10,10),ylim=(-10,10), title= expr);"
"# συνάρτηση απόλυτης τιμής
expr = abs(x)
plot(expr, (x,-10,10), title= expr);"
"# τετραγωνική συνάρτηση
expr = x**2 
plot(expr, (x,-10,10),ylim=(0,20), title= expr);"
"# κυβική συνάρτηση
expr = x**3 
plot(expr, (x,-10,10),ylim=(-20,20), title= expr);"
"# συνάρτηση τετραγωνικής ρίζας
expr = sqrt(x) 
plot(expr, (x,0,10),ylim=(0,5), title= expr)"
"# εκθετική συνάρτηση
# expr = E**x
expr = exp(x)
plot(expr, (x,-10,10),ylim=(0,20), title= expr);"
"# λογαριθμική συν΄άρτηση
expr = log(x) 
plot(expr, (x,-10,10),ylim=(-5,5), title= expr);"
"# συνάρτηση ημιτόνου
expr = sin(x) 
plot(expr, (x,-10,10),ylim=(-1,1), title= expr);"
"# συνάρτηση συνημιτόνου
expr = cos(x) 
plot(expr, (x,-10,10),ylim=(-1,1), title= expr);"
"# expr = x**3 - x - 2 
# expr = (x-2)**3 - (x - 2) - 2  # μετακίνηση προς τα δεξιά κατά 2
# expr = (x-2)**3 - (x - 2) - 2 - 5 # μετακίνηση προς τα κάτω κατά 5
# expr = (-x-2)**3 - (-x - 2) - 2 - 5 # οριζόντια αντιστροφή
expr = (-x-2)**3 + x - 5
plot(expr, (x,-10,10),ylim=(-20,20))
pprint(expand(expr))"
"# εναλλακτικά 
from IPython.display import display
expr = x**3 - x - 2 
expr = expr.subs(x, x-2)
expr = expr - 5
expr = expr.subs(x,-x)
plot(expr, (x,-10,10),ylim=(-20,20))
display(expand(expr))"
"r, t = symbols('r t')
v_r = 4/3*pi*r**3
r_t = 3*sqrt(t)
f = v_r.subs(r, r_t)
display(f)
f.subs(t,60).evalf() # τιμή συνάρτησης για t=60"
"T = Matrix([[7,3],[2,2]]) # πίνακας γραμμικού μετασχηματισμού
display(T)
A = Matrix([[1],[2]]) 
Aprime = T*A
display(Latex(""$A=%s A\'=%s$"" % (latex(A), latex(Aprime))))
B = Matrix([[1],[-2]])
Bprime = T*B
display(Latex(""$B=%s B\'=%s$"" % (latex(B), latex(Bprime))))"
"fig = plt.figure()
ax = fig.add_subplot(1,1,1) # 1x1 πλέγμα υπογραφημάτων, απεικόνιση στο 1 (και μοναδικό) υπογράφημα
plt.xlim([-1,14]) # όρια x άξονα (από-έως)
plt.ylim([-3,7]) # όρια y άξονα (από-έως)
plt.axhline(0, color='black') # θέση στην οποία θα ζωγραφιστεί οριζόντια γραμμή και χρώμα
plt.axvline(0, color='black') # θέση στην οποία θα ζωγραφιστεί κατακόρυφη γραμμή και χρώμα
plt.grid('on'); # εμφάνιση πλέγματος
# point1 = (1,2)  --> (13,6)
# point2 = (1,-2) --> (1,-2)
plt.plot([1,1],[2,-2],'ro',  ms=20) # απεικόνιση των σημείων (1,2) και (1,-2) με κόκκινο χρώμα και σχήμα κύκλου
plt.plot([13,1],[6,-2],'b*', ms=20) # απεικόνιση των σημείων (13,6) και (1,-2) με μπλε χρώμα και σχήμα αστεριού
ax.annotate('A', xy=(1,2), xytext=(4, 4),
            arrowprops=dict(facecolor='black', shrink=0.1)) # υποσημείωση στο γράφημα (κείμενο και βέλος)
ax.annotate('B', xy=(1,-2), xytext=(4, -0.5),
            arrowprops=dict(facecolor='black', shrink=0.1))
ax.annotate(""A'"", xy=(13,6), xytext=(10,5),
            arrowprops=dict(facecolor='black', shrink=0.1))
ax.annotate(""B'"", xy=(1,-2), xytext=(4, -2),
            arrowprops=dict(facecolor='black', shrink=0.1));"
"fig = plt.figure()
ax = fig.add_subplot(1,1,1)
plt.xlim([-0.5,3.5]) 
plt.ylim([-0.5,2.5]) 
plt.axhline(0, color='black') 
plt.axvline(0, color='black') 
plt.xlabel('Axis x')
plt.ylabel('Axis y')
plt.arrow( 0.0, 0.0, 1.0, 0.0, head_width=0.05, head_length=0.1, linewidth=3)
ax.annotate('(1,0)', xy=(1,0), xytext=(1,-0.2))
plt.arrow( 0.0, 0.0, 0.0, 1.0, head_width=0.05, head_length=0.1, linewidth=3)
ax.annotate('(0,1)', xy=(0,1), xytext=(-0.4,1))
plt.arrow( 0.0, 0.0, 3.0, 1.0, head_width=0.05, head_length=0.1, color='red', linewidth=3)
ax.annotate('(3,1)', xy=(3,1), xytext=(3,0.8))
plt.arrow( 0.0, 0.0, 1.0, 2.0, head_width=0.05, head_length=0.1, color='red', linewidth=3)
ax.annotate('(1,2)', xy=(1,2), xytext=(1,2.2));"
"from matplotlib.path import Path
import matplotlib.patches as patches
verts = [
    (1, 0),  # P0
    (-1, 3), # P1
    (0, 3), # P2
    (1, 0),  # P0
    ]
verts2 = [
    (2, -1),  # P0
    (1, 1), # P1
    (3, 0), # P2
    (2, -1),  # P0
    ]

codes = [Path.MOVETO,
         Path.LINETO,
         Path.LINETO,
         Path.CLOSEPOLY
         ]
path = Path(verts, codes)
path2 = Path(verts2, codes)
fig = plt.figure()
ax = fig.add_subplot(1,1,1)
patch = patches.PathPatch(path, facecolor='orange', linewidth=2)
ax.add_patch(patch)
ax.annotate('A', xy=(1,0))
ax.annotate('B', xy=(-1,3))
ax.annotate('C', xy=(0,3))
patch = patches.PathPatch(path2, facecolor='cyan', linewidth=2)
ax.add_patch(patch)
ax.annotate(""A'"", xy=(2,-1), xytext=(2,-1.2))
ax.annotate(""B'"", xy=(1,1))
ax.annotate(""C'"", xy=(3,0))
ax.set_aspect('equal')
ax.set_xlim(-1.5, 3.5)
ax.set_ylim(-1.5, 3.5)
plt.axhline(0, color='black')
plt.axvline(0, color='black')
plt.title('linear transformation')

plt.grid();"
"from matplotlib.path import Path
import matplotlib.patches as patches
import matplotlib.ticker as plticker
verts = [
    (2, 1),  # P0
    (4, 3), # P1
    (3, 5), # P2
    (1, 2),  # P3
    (2, 1),  # P0
    ]

# συμμετρία ως προς την αρχή των αξόνων
verts2 = [
    (-2, -1),  # P0
    (-4, -3), # P1
    (-3, -5), # P2
    (-1, -2),  # P3
    (-2, -1),  # P0
    ]

# συμμετρία ως προς τον άξονα των τετμημένων (άξονας x) 
# verts2 = [
#     (2, -1),  # P0
#     (4, -3), # P1
#     (3, -5), # P2
#     (1, -2),  # P3
#     (2, -1),  # P0
#     ]

# συμμετρία ως προς τον άξονα των τεταγμένων (άξονας y)
# verts2 = [
#     (-2, 1),  # P0
#     (-4, 3), # P1
#     (-3, 5), # P2
#     (-1, 2),  # P3
#     (-2, 1),  # P0
#     ]

# συμμετρία ως προς την ευθεία y=x
# verts2 = [
#     (1, 2),  # P0
#     (3, 4), # P1
#     (5, 3), # P2
#     (2, 1),  # P3
#     (1, 2),  # P0
#     ]

# στροφή με κέντρο την αρχή των αξόνων και γωνία θ=90
# verts2 = [
#     (-1, 2),  # P0
#     (-3, 4), # P1
#     (-5, 3), # P2
#     (-2, 1),  # P3
#     (-1, 2),  # P0
#     ]

# Ομοιοθεσία με κέντρο την αρχή των αξόνων και λόγο λ=0.5
# verts2 = [
#     (1, .5),  # P0
#     (2, 1.5), # P1
#     (1.5, 2.5), # P2
#     (0.5, 1.0),  # P3
#     (1, 0.5),  # P0
#     ]


# Παράλληλη μεταφορά 3 αριστερά και 3 προς τα κάτω
# verts2 = [
#     (3, -1),  # P0
#     (5, 1), # P1
#     (4, 3), # P2
#     (2, 0),  # P3
#     (3, -1),  # P0
#     ]

codes = [Path.MOVETO,
         Path.LINETO,
         Path.LINETO,
         Path.LINETO,
         Path.CLOSEPOLY
         ]
path = Path(verts, codes)
path2 = Path(verts2, codes)
fig = plt.figure(figsize=(10,10))
ax = fig.add_subplot(1,1,1)
patch = patches.PathPatch(path, facecolor='orange', linewidth=2)
ax.add_patch(patch)
patch = patches.PathPatch(path2, facecolor='cyan', linewidth=2)
ax.add_patch(patch)
ax.set_xlim(-6, 6)
ax.set_ylim(-6, 6)
ax.set_aspect('equal')
loc = plticker.MultipleLocator(base=1.0) 
ax.xaxis.set_major_locator(loc)
ax.yaxis.set_major_locator(loc)
plt.axhline(0, color='black')
plt.axvline(0, color='black')
plt.title('linear transformation')
plt.grid();"
"# απόσταση δύο σημείων στο καρτεσιανό επίπεδο
from sympy.geometry import Point
from sympy import init_printing
from IPython.display import display
init_printing()
p1 = Point(3,2)
p2 = Point(8,9)
d = p1.distance(p2)
d, d.evalf()"
"# απόσταση δύο σημείων στον τρισδιάστατο χώρο
from sympy import Point3D
p1 = Point3D(3,2,1)
p2 = Point3D(1,7,2)
d = p1.distance(p2)
d, d.evalf()"
"p1=plot(2*x-x**3+5, xlim=(0,3), ylim=(0,10), show=False, line_color='b', label='2*x-x**3+5')
p2=plot(-x + 7, show=False, line_color='r')
p1.extend(p2)
p1.show()"
"f = x**3-3*x+5
print('f(5)=', f.subs(x,5))
df = diff(f,x)
t = df.subs(x,5)*(x-5) + f.subs(x,5)
p1=plot(f, xlim=(0,6), ylim=(0,200), show=False, line_color='b')
p2=plot(t, show=False, line_color='r')
p1.extend(p2)
p1.show()"
"f = log(x)
t = 1/E*x
p1=plot(f, xlim=(0,4), ylim=(-2,2), show=False, line_color='b')
p2=plot(t, show=False, line_color='r')
p1.extend(p2)
p1.show()"
"f=E**x*log(x)
Derivative(f,x)"
"diff(f,x)"
"f=sqrt(x)*sin(x)*log(x)
diff(f,x)"
"f=x**2/(5*x-1)
df = diff(f,x)
df"
simplify(df)
"f=(3*x**4+4*x**3)**(-2)
diff(f)"
simplify(diff(f))
"f = (x-1)**(2/3)
df = diff(f)
df"
nsimplify(df)
"import warnings
warnings.filterwarnings('ignore')
# σχεδίαση γραφήματος με πολύγωνο
%matplotlib inline
import matplotlib.pyplot as plt 
from matplotlib.path import Path
import matplotlib.patches as patches
verts = [
    (5, 1), # P0
    (9, 1), # P1
    (9, 2), # P2
    (7, 5), # P3
    (5, 2), # P4
    (5, 1), # P0
    ]

codes = [Path.MOVETO,
         Path.LINETO,
         Path.LINETO,
         Path.LINETO,
         Path.LINETO,
         Path.CLOSEPOLY
         ]
path = Path(verts, codes)
fig = plt.figure()
ax = fig.add_subplot(1,1,1)
patch = patches.PathPatch(path, facecolor='orange', linewidth=2)
ax.add_patch(patch)
ax.annotate('A', xy=(4.6,1))
ax.annotate('B', xy=(9,1))
ax.annotate('C', xy=(9,2))
ax.annotate('D', xy=(7,5))
ax.annotate('E', xy=(4.6,2))

ax.set_xlim(-1, 10)
ax.set_ylim(-1, 6)
plt.axhline(0, color='black')
plt.axvline(0, color='black')

plt.grid();"
"from sympy import *
from IPython.display import display
init_printing()
# απεικόνιση του πολυγώνου ως πίνακα 2x5
pentagon = Matrix([[5,9,9,7,5],[1,1,2,5,2]])
# γραμμικοί μετασχηματισμοί
T1 = Matrix([[0,1],[1,0]]) # συμμετρία ως προς y=x
T2 = Matrix([[cos(pi/3),-sin(pi/3)],[sin(pi/3),cos(pi/3)]]) # στροφή κατά 60 μοίρες
pentagon2 = T2*T1*pentagon 
display(pentagon)
display(pentagon2)
display(pentagon2.evalf())"
"# γραφική απεικόνιση του πολυγώνου και της εικόνας του μετά τους γραμμικούς μετασχηματισμούς
verts = [
    (5, 1), # P0
    (9, 1), # P1
    (9, 2), # P2
    (7, 5), # P3
    (5, 2), # P4
    (5, 1), # P0
    ]

# verts2 = [
#     (1, 5), # P0
#     (1, 9), # P1
#     (2, 9), # P2
#     (5, 7), # P3
#     (2, 5), # P4
#     (1, 5), # P0
#     ]

verts2 = [
    (-3.83, 3.37), # P0
    (-7.29, 5.37), # P1
    (-6.79, 6.23), # P2
    (-3.56, 7.83), # P3
    (-3.33, 4.23), # P4
    (-3.83, 3.37), # P0
    ]

codes = [Path.MOVETO,
         Path.LINETO,
         Path.LINETO,
         Path.LINETO,
         Path.LINETO,
         Path.CLOSEPOLY
         ]
path = Path(verts, codes)
path2 = Path(verts2, codes)
fig = plt.figure()
ax = fig.add_subplot(1,1,1)
patch = patches.PathPatch(path, facecolor='orange', linewidth=2)
ax.add_patch(patch)
patch = patches.PathPatch(path2, facecolor='cyan', linewidth=2)
ax.add_patch(patch)
ax.set_aspect('equal')
ax.set_xlim(-10, 10)
ax.set_ylim(-10, 10)
plt.axhline(0, color='black')
plt.axvline(0, color='black')
plt.title('linear transformations')
plt.grid();"
"x= Symbol('x')
f = (3*x**2+5)**9
f"
"diff(f,x)"
"f = exp(-x**2+1)
f"
"diff(f,x)"
"f = log(sqrt(x**2+1))
f"
"diff(f,x)"
"f = exp(3*x+x**2)
f"
"df= diff(f,x)
df"
"# δεύτερη παράγωγος της f
d2f=diff(df,x)
simplify(d2f)"
"# δεύτερη παράγωγος της f (απευθείας κλήση συνάρτησης diff - α' τρόπος)
diff(f,x,x)"
"# δεύτερη παράγωγος της f (απευθείας κλήση συνάρτησης diff - β' τρόπος)
diff(f,x,2)"
"# τρίτη παράγωγος της f
diff(f,x,3)"
"# δέκατη παράγωγος της f
diff(f,x,10)"
"f = -x**3+7*x**2-10*x
f"
"%matplotlib inline
from sympy.plotting import plot
import warnings
warnings.filterwarnings('ignore')
p = plot(f, xlim=(-1,6), ylim=(-10,10))"
"df=diff(f,x)
df"
"sols = solve(df,x)
sols"
"(sols[0].evalf(), sols[1].evalf())"
2<sols[0]<5
2<sols[1]<5
"f=x**3
df = diff(f,x)
rhs = (f.subs(x,4)-f.subs(x,1))/3
sols  = solve(Eq(df,rhs), x)
sols[0].evalf(), sols[1].evalf()"
"x0=sols[1]
fx0 = f.subs(x,x0)
dfx0 = df.subs(x,x0)
p1 = plot(f,xlim=(0,5),ylim=(0,100),show=False)
p2 = plot(dfx0*(x-x0)+fx0,show=False, line_color='r')
p1.extend(p2)
p1.show()"
"f = x**3-12*x**2+45*x+1
p = plot(f, xlim=(0,5), ylim=(0,100))"
"df = diff(f,x)
df"
"sols =solve(df,x)
sols"
"f.subs(x,0), f.subs(x,5), f.subs(x,sols[0]), f.subs(x,sols[1])"
"f = x**3-3*x**2+x-2
f"
"df = diff(f,x)
df"
"sols =solve(df,x)
print(""Τα κρίσιμα σημεία είναι: %.3f και %.3f"" % (sols[0].evalf() ,sols[1].evalf()))
sols"
"d2f=diff(f,x,x)
d2f"
"solve(d2f,x)"
"p1 =plot(f, xlim=(-2,4), ylim=(-10,10), show=False)
p2 = plot_implicit(Eq(x, 1), show=False)
p1.extend(p2)
p1.show()"
"f = 2*log(x)/(x-1)
limit(f,x,1)"
"df_nominator=diff(2*log(x),x)
df_denominator=diff(x-1,x)
(df_nominator/ df_denominator).subs(x,1)"
"#Τετραγωνική ρίζα αρνητικών αριθμών
sqrt(-1), sqrt(-25)"
"# Ο φανταστικός αριθμός i
I"
I*I
"# επίλυση της εξίσωσης x^2=-1
x=Symbol('x')
solve(x**2+1,x)"
"# μιγαδικοί αριθμοί
c = 3 + 4*I
display(c)
print('συντελεστής πραγματικού μέρους=%s συντελεστής φανταστικού μέρους=%s' % (re(c), im(c)))"
"# γεωμετρική αναπαράσταση μιγαδικών αριθμών
%matplotlib inline  
import matplotlib.pyplot as plt 
fig = plt.figure()
ax = fig.add_subplot(1,1,1)
plt.xlim([-0.5,3.5]) 
plt.ylim([-0.5,2.5]) 
plt.axhline(0, color='black') 
plt.axvline(0, color='black') 
plt.xlabel('real axis')
plt.ylabel('imaginary axis')
plt.arrow( 0.0, 0.0, 1.0, 2.0, head_width=0.05, head_length=0.1, linewidth=3)
ax.annotate('1+2i', xy=(1,2), xytext=(1,1.8));"
"# συζυγής
conjugate(2+3*I)"
"fig = plt.figure()
ax = fig.add_subplot(1,1,1)
plt.xlim([-0.5,5]) 
plt.ylim([-4,4]) 
plt.axhline(0, color='black') 
plt.axvline(0, color='black') 
plt.xlabel('real axis')
plt.ylabel('imaginary axis')
plt.arrow( 0.0, 0.0, 2.0, 3.0, head_width=0.05, head_length=0.1, linewidth=3)
plt.arrow( 0.0, 0.0, 2.0, -3.0, head_width=0.05, head_length=0.1, linewidth=3);"
"# γινόμενο μιγαδικού και του συζυγή του
display((2+3*I)*(2-3*I))
simplify((2+3*I)*(2-3*I))"
"# άθροισμα μιγαδικών
(2+3*I) + (3-6*I)"
"# γινόμενο μιγαδικών 
simplify((2+3*I)*(5-3*I))"
"# διαίρεση μιγαδικών
display(simplify((-6+7*I)/(2+5*I)))
# πολλαπλασιασμός αριθμητή και παρονομαστή με τον συζυγή του παρονομαστή
a = (-6+7*I)*(2-5*I)
b = (2+5*I)*(2-5*I)
display(simplify(a/b))"
"# μέτρο μιγαδικού αριθμού
c = 3-4*I
Abs(c)"
"# επ΄ίλυση δευτεροβάθμιας εξίσωσης με μιγαδικές ρίζες
x=Symbol('x')
eq = x**2 - x + 5
solve(eq)"
"# αριθμός ριζών πολυωνυμικής εξίσωσης 3ου βαθμού με πραγματικούς συντελεστές
x=Symbol('x')
eq = x**3 -3*x**2 +5*x -3
results = solve(eq)
for i in range(3):
    display(results[i].evalf())"
"# αριθμός ριζών πολυωνυμικής εξίσωσης 4ου βαθμού με πραγματικούς συντελεστές
x=Symbol('x')
eq = x**4 -x**3 -3*x**2 +5*x -3
results = solve(eq)
for i in range(4):
    display(results[i].evalf())"
"# παραφοντοποίηση με μιγαδικούς όρους
x=Symbol('x')
f_x = x**2+1
factor(f_x, extension=[I])"
"import warnings
warnings.filterwarnings('ignore')
# επίλυση του συστήματος γραμμικών εξισώσεων χρησιμοποιώντας το sympy
from sympy import *
from IPython.display import display
init_printing()
i1,i2,i3 = symbols('i1, i2, i3')
A = Matrix([[1, -1, -1, 0], [5, 3, 0, 3], [0,-3,6,4]])
display(A)
results = solve_linear_system(A, i1,i2,i3)
display(results)"
"# επίλυση του συστήματος γραμμικών εξισ΄ώσεων χρησιμοποιώντας το numpy
import numpy as np
a = np.array([[1,-1,-1], [5,3,0], [0,-3,6]])
b = np.array([0,3,4])
x = np.linalg.solve(a,b)
print(x[0], x[1], x[2])"
"# επίλυση του συστήματος γραμμικών εξισώσεων χρησιμοποιώντας το sympy
i1,i2,i3,i4,i5,i6 = symbols('i1, i2, i3, i4, i5, i6')
A = Matrix([[0, 1, -1, 0,-1, 0, 0],
            [1, 0, 0, -1, -1, 0, 0], 
            [1, -1, 0, 0, 0, 1, 0], 
            [0, 2, 0, 0, 3, 0, 5],
            [0, 0, 0, 6, -3, 0, 4],
            [0, 0, 0, -6, 0, 4, 1]])
display(A)
results = solve_linear_system(A, i1,i2,i3,i4,i5,i6)
display(results)"
"# επίλυση του συστήματος γραμμικών εξισ΄ώσεων χρησιμοποιώντας το numpy
a = np.array([[0,1,-1,0,-1,0], 
              [1,0,0,-1,-1,0], 
              [1,-1,0,0,0,1], 
              [0,2,0,0,3,0],
              [0,0,0,6,-3,0],
              [0,0,0,-6,0,4]])
b = np.array([0,0,0,5,4,1])
x = np.linalg.solve(a,b)
print(x)"
"f_x = (x+1)*(x+2)*(x+3)
f_x, expand(f_x)"
"# διαφορά τετραγώνων
factor(4*x**2 -9)"
"# άθροισμα κύβων
factor(x**3+27)"
"display(3*x**2+9*x-30)
display(factor(3*x**2+9*x-30))

from math import sqrt
a = 3
b = 9
c = -30
delta = b**2 -4*a*c
x1 = (-b+sqrt(delta))/(2*a)
x2 = (-b-sqrt(delta))/(2*a)
print('{0}*(x-{1})*(x-{2})'.format(a,x1, x2))"
"# επίλυση εξίσωσης (sympy)
solve(3*x**2+9*x-30,x)"
"expr = 10**(2*x)-17
display(expr)
solve(expr,x)"
"expr = log(x-3,10)-2
solve(expr)"
"integrate(x**3, (x,0,1))"
"Integral(x**3,(x,0,1))"
"F = integrate(x**3,x)
F.subs(x,1) - F.subs(x,0)"
"from scipy.misc import derivative

def f(x):
    return x**3 + x**2

derivative(f,1.0, dx=1e-6)"
"def f(x):
    return sqrt(x)-5*x**(1/3) + 1

derivative(f,5.0, dx=1e-6)"
"f = lambda x: sqrt(x)-5*x**(1/3) + 1
derivative(f,5.0, dx=1e-6)"
"x= Symbol('x')
f=sqrt(x)-5*x**(1/3)+1
Derivative(f,x)"
"df = diff(f,x)
df"
nsimplify(df)
"df.subs(x,5)"
"nsimplify(df.subs(x,5))"
"df.subs(x,5).evalf()"
"import warnings
warnings.filterwarnings('ignore')
from IPython.display import YouTubeVideo
YouTubeVideo(""DxVX5hPZVEM"")"
"###printing key values
for a in dict:
    print(a)"
"for a in dict.items():
    print(a)"
"for b,c in dict.items():
    print(b,c)
    print (b)
    print (c)"
"dict[9] =30
dict"
df = pd.read_csv('fire-incidents.csv')
"pd.value_counts(df['INCIDENT DESCRIPTION'], normalize=True).head(10).plot(kind='bar')"
incidents_by_year.size().plot(kind='bar')
incidents_by_year.sum()
incidents_by_type.value_counts()
incidents_by_type.value_counts(normalize=True)
"import warnings
warnings.filterwarnings('ignore')
%pylab inline
N = 50
r = 2 * random.rand(N)
theta = 2 * pi * random.rand(N)
area = 200 * r**2
ax=plt.subplot(111, projection='polar')
c=ax.scatter(theta, r, c=theta, s=area, cmap=plt.cm.hsv, alpha=0.5)"
!jupyter-nbconvert Slides.ipynb --to slides 
"# Iteration
xrange = np.arange(100000)

# Neural Network
for iter in xrange:
    
    # forward propagation
    layer_0 = X
    layer_1 = nonlin(np.dot(layer_0, weight_1))
    
    # Error Calculation for layer_1
    layer_1_err = layer_1 - y
    
    # Backpropagation when (deriv=True)
    # multiply the error with the 'derivative of activation function'
    layer_1_der = layer_1_err * nonlin(layer_1, True)
    weight_slope = np.dot(layer_0.T, layer_1_der)
    
    # update the weights
    weight_1 -= weight_slope
    
print (""Weight_1 after {} times is \n {}"".format(len(xrange), weight_1))
print (""Desired output is \n {}"".format(y))
print (""Calcualted outputs after {} times is \n {}"".format(len(xrange), layer_1))"
"# Iteration with alpha
for alpha in alphas:
    print (""\n Training with alpha {}"".format(alpha))
    np.random.seed(1)
    
    # Randomly weights initialization
    weight_1 = 2*np.random.random((3,4)) - 1
    weight_2 = 2*np.random.random((4,1)) - 1
    
    # iteration
    krange = np.arange(60000)
    
    # Neural network with different alpha effect
    for j in krange:
        
        ## Forward propagation step ##
        layer_0 = X
        layer_1 = nonlin(np.dot(layer_0, weight_1))
        layer_2 = nonlin(np.dot(layer_1, weight_2))
        
        ## Backpropagation step ## 
        # Step (1)
        layer_2_err = layer_2 - y
        
        # To output error calculation for every 10000 iterations 
        if (j% 10000) == 0:
            print(""Error after {} iterations {}"".format(j, np.mean(np.abs(layer_2_err))))
        
        # Step (2)
        layer_2_der = layer_2_err * nonlin(layer_2, True)
        
        # Step (3)
        layer_1_err = layer_2_der.dot(weight_2.T)
        
        # Step (4)
        layer_1_der = layer_1_err * nonlin(layer_1, True) 
        
        # update the weights
        weight_2 -= alpha * (layer_1.T.dot(layer_2_der))
        weight_1 -= alpha * (layer_0.T.dot(layer_1_der))"
"wineprice(95.0,3.0)

wineprice(95.0,8.0)

wineprice(99.0,1.0)

data = wineset1()

data"
"plot_3D(x,y,p_an)"
"plot_3D(x, y, p)"
"plot_3D(x,y,p)"
"pyplot.figure(figsize=(6,6))
pyplot.grid(True)
pyplot.xlabel(r'$n_x$', fontsize=18)
pyplot.ylabel(r'$L_2$-norm of the error', fontsize=18)

pyplot.loglog(nx_values, error, color='k', ls='--', lw=2, marker='o')
pyplot.axis('equal');"
"pyplot.figure(figsize=(6,6))
pyplot.grid(True)
pyplot.xlabel(r'$n_x$', fontsize=18)
pyplot.ylabel(r'$L_2$-norm of the error', fontsize=18)

pyplot.loglog(nx_values, error, color='k', ls='--', lw=2, marker='o')
pyplot.axis('equal');"
"X, Y, x, y, p_i, b, dx, dy, L = poisson_IG(nx, ny, xmax, xmin, ymax, ymin)
plot_3D(x, y, p_i)"
"plot_3D(x,y,p)"
"error = L2_rel_error(p,p_an)
error"
"pyplot.figure(figsize=(8,6))
pyplot.grid(True)
pyplot.xlabel(r'iterations', fontsize=18)
pyplot.ylabel(r'$L_2$-norm', fontsize=18)
pyplot.semilogy(numpy.arange(len(l2_conv)), l2_conv, lw=2, color='k');"
"pyplot.figure(figsize=(6,6))
pyplot.grid(True)
pyplot.xlabel(r'$n_x$', fontsize=18)
pyplot.ylabel(r'$L_2$-norm of the error', fontsize=18)
pyplot.loglog(nx_values, error, color='k', ls='--', lw=2, marker='o')
pyplot.axis('equal');"
"p, l2_conv = steepest_descent_2d(p_i.copy(), b, dx, dy, l2_target)
L2_rel_error(p, pan)"
"p, l2_conv = conjugate_gradient_2d(p_i.copy(), b, dx, dy, l2_target)
L2_rel_error(p,pan)"
"import warnings
warnings.filterwarnings('ignore')
%pylab inline
import pandas as pd
import numpy as np
from matplotlib import pyplot as plt
import os
import seaborn"
"#All symbols 2011-2015
symbols = os.listdir(''.join(('../data/',dir_,'/')))
sers, poss = get_portfolioTS(symbols, dir_)
print(poss.shape[1])
plot(sers['2011':'2015'].sum(axis = 1)*leverage/sers.shape[1])
show()
plot(sers['2011':'2015'].sum(axis = 1)*leverage/poss['2011':'2015'].sum(axis=1))
show()"
"#All symbols 2015
symbols = os.listdir(''.join(('../data/',dir_,'/')))
sers, poss = get_portfolioTS(symbols, dir_)
print(poss.shape[1])
plot(sers['2015'].sum(axis = 1)*leverage/sers.shape[1])
show()
plot(sers['2015'].sum(axis = 1)*leverage/poss['2011':'2015'].sum(axis=1))
show()"
"#Top N symbols 2011-2014
sss = pd.read_csv(''.join(('../data/performance_results/', dir_, '/test_individual.csv')), sep='\t', index_col=0)
symbols=sss.index
sers, poss = get_portfolioTS(symbols, dir_)
print(poss.shape[1])
plot(sers['2011':'2014'].sum(axis = 1)*leverage/sers.shape[1])
show()
plot(sers['2011':'2014'].sum(axis = 1)*leverage/poss['2011':'2015'].sum(axis=1))
show()"
"#The same top N over 2015
sss = pd.read_csv(''.join(('../data/performance_results/', dir_, '/test_individual.csv')), sep='\t', index_col=0)
symbols=sss.index
sers, poss = get_portfolioTS(symbols, dir_)
print(poss.shape[1])
plot(sers['2015'].sum(axis = 1)*leverage/sers.shape[1])
show()
plot(sers['2015'].sum(axis = 1)*leverage/poss['2015'].sum(axis=1))
show()"
"#Top N symbols in 2015 (as a control)
sss = pd.read_csv(''.join(('../data/performance_results/', dir_, '/control(top)_individual.csv')), sep='\t', index_col=0)
symbols=sss.index
sers, poss = get_portfolioTS(symbols, dir_)
print(poss.shape[1])
plot(sers['2015'].sum(axis = 1)*leverage/sers.shape[1])
show()
plot(sers['2015'].sum(axis = 1)*leverage/poss['2015'].sum(axis=1))
show()"
"import warnings
warnings.filterwarnings('ignore')
%pylab inline
import pandas as pd
import numpy as np
from matplotlib import pyplot as plt
import os
import seaborn"
"#All symbols 2011-2015
symbols = os.listdir(''.join(('../data/',dir_,'/')))
sers, poss = get_portfolioTS(symbols, dir_)
print(poss.shape[1])
plot(sers['2011':'2015'].sum(axis = 1)/sers.shape[1])
show()
plot(sers['2011':'2015'].sum(axis = 1)/poss['2011':'2015'].sum(axis=1))
show()"
"#All symbols 2015
symbols = os.listdir(''.join(('../data/',dir_,'/')))
sers, poss = get_portfolioTS(symbols, dir_)
print(poss.shape[1])
plot(sers['2015'].sum(axis = 1)/sers.shape[1])
show()
plot(sers['2015'].sum(axis = 1)/poss['2011':'2015'].sum(axis=1))
show()"
"#Top N symbols 2011-2014
sss = pd.read_csv(''.join(('../data/performance_results/', dir_, '/test_individual.csv')), sep='\t', index_col=0)
symbols=sss.index
sers, poss = get_portfolioTS(symbols, dir_)
print(poss.shape[1])
plot(sers['2011':'2014'].sum(axis = 1)/sers.shape[1])
show()
plot(sers['2011':'2014'].sum(axis = 1)/poss['2011':'2015'].sum(axis=1))
show()"
"#The same top N over 2015
sss = pd.read_csv(''.join(('../data/performance_results/', dir_, '/test_individual.csv')), sep='\t', index_col=0)
symbols=sss.index
sers, poss = get_portfolioTS(symbols, dir_)
print(poss.shape[1])
plot(sers['2015'].sum(axis = 1)/sers.shape[1])
show()
plot(sers['2015'].sum(axis = 1)/poss['2015'].sum(axis=1))
show()"
"#Top N symbols in 2015 (as a control)
sss = pd.read_csv(''.join(('../data/performance_results/', dir_, '/control(top)_individual.csv')), sep='\t', index_col=0)
symbols=sss.index
sers, poss = get_portfolioTS(symbols, dir_)
print(poss.shape[1])
plot(sers['2015'].sum(axis = 1)/sers.shape[1])
show()
plot(sers['2015'].sum(axis = 1)/poss['2015'].sum(axis=1))
show()"
"
data = pd.read_pickle('../data/junkout/CNX')
plt.plot(np.cumprod(data['stratret']+1))"
"ser=pd.Series()
sum(ser.add(data['stratret'], fill_value=0))"
data.columns
"ser = pd.Series()
all = pd.Series()
for symbol in os.listdir('../data/junkout/'):
    data = pd.read_pickle(''.join(('../data/junkout/', symbol)))[2011:]
    ser = ser.add(data['stratret'], fill_value=0)
    all = all.add(abs(data['pos']), fill_value=0)
plt.plot(np.cumprod(3*ser/all+1))"
plt.plot(all)
df2.as_matrix()
"b, a1, a2"
decade_cat
"# カテゴリーを表示します。
decade_cat.categories"
"# グループの数だけを指定することもできます。
pd.cut(years,2)"
poll_df.info()
np.zeros(5)
my_zeros_array
"np.ones((5,5))"
np.empty(5)
"np.empty((3,4))"
np.eye(5)
elements_dict
input_floors
"fig,ax =plt.subplots()
x = array(range(100))
ax.grid(True)
ax.axis([0,40,0,100])
fig.set_size_inches(20,10)
for i in np.arange(1,13):
    #plot(x,(np.multiply(x,i)))
    plot(x,(np.add(np.multiply(x,i),-i**2)))

#axis([0,100,0,100])
plot(x,(x/2)**2)"
"x = array(range(12))
fig,ax =plt.subplots()
axis([0,10,0,10])
ax.set_yticks([2,3,6])
ax.set_yticklabels([""1*2"",""3*1"",""3*2""])
ax.set_xticks([1,2,3,5])
ax.set_xticklabels([""1"",""2"",""3=2+1"",""5=3+2""])
def line(n):
    plot(x,np.add(np.multiply(x,n),-n**2))
    
line(1)
line(2)
line(3)
ax.grid(True)"
"sns.set(style='ticks', palette='Set2')"
"d.plot(subplots=True, layout=(2,1))
sns.despine()"
"d.diff().plot(subplots=True, layout=(2,1))
sns.despine()"
"d.diff().diff().plot(subplots=True, layout=(2,1))
sns.despine()"
"i = d.r
_ = plot_acf(i)
sns.despine()
_ = plot_pacf(i)
sns.despine()"
"i = d.y
_ = plot_acf(i)
sns.despine()
_ = plot_pacf(i)
sns.despine()"
"adfuller(d.y, regression='ct')"
r
r[-1].resols.summary()
m1.get_robustcov_results()
print(_.summary())
"question_type_result = json.loads(open(""../results/questionTypeResult.txt"", 'r').read())

for q_type, occurences in question_type_result.items():
    correct_count = occurences[0]
    total_count = occurences[1]
    occurences[2] = correct_count / total_count
    
question_type_list = list(question_type_result.items())
question_type_list = sorted(question_type_list, key=lambda x:x[1][2])

accuracy = [x[1][2] * 100 for x in question_type_list]
labels = [ x[0] if x[0] != 'none of the above' else ""Others*"" for x in question_type_list]

plt.grid()

x_pos = np.arange(len(accuracy))
plt.bar(x_pos, accuracy)

x_pos = x_pos + 0.5
plt.xticks(x_pos, labels, rotation='vertical')

plt.ylim((0, 100))
plt.xlabel('Question type (first few words)')
plt.ylabel('Accuracy (in %)')

plt.margins(0.05)

plt.show()"
"answer_type_result = json.loads(open('../results/answerTypeResult.txt', 'r').read())

for a_type, occurences in answer_type_result.items():
    correct_count = occurences[0]
    total_count = occurences[1]
    occurences[2] = correct_count / total_count
    
answer_type_list = list(answer_type_result.items())
answer_type_list = sorted(answer_type_list, key=lambda x:x[1][2])

accuracy = [x[1][2] * 100 for x in answer_type_list]
labels = [x[0].title() if x[0] != 'other' else ""Others*"" for x in answer_type_list]

x_pos = np.arange(len(accuracy))
plt.bar(x_pos, accuracy)

x_pos = x_pos + 0.5
plt.xticks(x_pos, labels)

plt.ylim((0, 100))
plt.xlabel('Answer type')
plt.ylabel('Accuracy (in %)')

plt.margins(0.05)

plt.title(""Accuracies based on different answer types"")
plt.show()"
"# Lens Parameters
diameter = 2100
focal_length = 5523

# Equation for reflector's shape
def parabola(x):
    return (x**2)/(4*focal_length)

# Number of rays to be plotted
num_rays = 5

# Incident angles to be plotted
angles = [-5, -2.5, 0, 2.5, 5]

# Setting up Figure
plt.figure(figsize=(8.5,11))

# Loop through all incident angles
for (i,angle) in enumerate(angles):
    # Create a new subplot
    plt.subplot(len(angles),1,i+1)
    
    # Incident ray angle
    angle_incident = np.deg2rad(angle) # In radians

    # Reflection Locations
    # Calculating reflection locations
    reflection_y = np.zeros(num_rays)
    for i in np.arange(num_rays):
        # Evenly space rays over the diameter of reflector
        reflection_y[i] = -diameter/2 + i*diameter/(num_rays-1)
    # Calculating mirror displacement at reflection location
    reflection_x = -parabola(reflection_y)

    # Start Locations
    # Setting X value where incident rays come from
    start_x = -focal_length*1.2
    # Calculating Y values where incident rays come from
    # Simple Y = mx+b calculation
    start_y = reflection_y+(start_x-reflection_x)*np.tan(angle_incident)

    # Calculating reflected angles of each ray using ray trace matrix eqn
    angle_reflected = angle_incident - (1/focal_length)*reflection_y

    # End locations
    # Calculating X values where reflected waves end
    end_x = -focal_length
    # Calculating Y values where incident rays come from
    # Simple Y = mx+b calculation
    end_y = reflection_y+(reflection_x-end_x)*np.tan(angle_reflected)

    # Draw Subplot
    
    # Plot parabolic lens
    y = np.arange(-diameter/2,diameter/2,diameter/1000)
    x = -parabola(y)
    plt.plot(x,y,linewidth=5.0)

    # Plot incident light rays
    for i in np.arange(num_rays):
        line = plt.Line2D((start_x,reflection_x[i]), (start_y[i],reflection_y[i]), lw=1)
        plt.gca().add_line(line)

    # Plot reflected light rays
    for i in np.arange(num_rays):
        line = plt.Line2D((end_x,reflection_x[i]), (end_y[i],reflection_y[i]), lw=1)
        plt.gca().add_line(line)

    plt.axis(""equal"")
    plt.grid(""on"")
"
"#Plots
colormap = ""gray""


plt.figure(figsize = (14,6), dpi = 100)
plt.subplot(1,2,1)
plt.imshow(aperture_screen_s, cmap=colormap, 
           extent = (-X_aperture_s_meff/2,X_aperture_s_meff/2,
                     -X_aperture_s_meff/2,X_aperture_s_meff/2))
plt.xlabel(""[m]"")
plt.ylabel(""[m]"")
plt.title(""Aperture Screen"")

plt.subplot(1,2,2)
plt.imshow(np.log10(aperture_psf), cmap=colormap)
plt.xlabel(""[pixels]"")
plt.ylabel(""[pixels]"")
plt.title(""Aperture PSF"")

plt.figure(figsize = (14,6), dpi = 100)
plt.subplot(1,2,1)
#plt.imshow(input_img, cmap=colormap, extent = (-6.513,6.513,-6.513,6.513))
plt.imshow(input_img, cmap=colormap, extent = (-nxy*platescale/2,nxy*platescale/2,
                                               -nxy*platescale/2,nxy*platescale/2))
plt.xlabel(""[arcsec]"")
plt.ylabel(""[arcsec]"")
plt.title(""Input Image"")

plt.subplot(1,2,2)
plt.imshow(sensor_img, cmap=colormap)
plt.xlabel(""[pixels]"")
plt.ylabel(""[pixels]"")
plt.title(""Sensor Image"")

# Show center row of aperture PSF and sensor's image
plt.figure(figsize = (20,7), dpi = 100)
plt.plot((aperture_psf[center,:]))
plt.title(""Aperture PSF (Center Row)"")

# List distances from center of nulls in PSF
print(""Aperture PSF Null Radii"")
minima_i = np.subtract(argrelextrema(aperture_psf[center], np.less),center)[0]
print_num = 5
minima_found = 0
for value in minima_i:
    if (value > 0) and (minima_found < print_num):
        print(value)
        minima_found = minima_found + 1"
"# FFT Sanity Check
from scipy.fftpack import fft

# Continuous Signal: y = sin(2pi*t) [Frequency = 1Hz]
# Sample this signal with sample time = 0.01s : y = sin(2pi*0.01n)
# To sample it for 2 seconds, we will have n = 0:200
n = np.arange(200)
y = np.sin(2*np.pi*0.01*n)

plt.figure()
plt.plot(n,y)
plt.xlabel(""Samples [n]"")
plt.ylabel(""y"")

# We take the FFT of this signal
# We know that all the energy of this signal should be at 1Hz
# Each index of FFT = k/T = k/2 = 0.5k Hz

y_f = fft(y)

plt.figure()
plt.plot(n,np.abs(y_f))
plt.xlabel(""Frequency [k]"")
plt.ylabel(""y"")

for i in np.arange(20):
    print(i,np.abs(y_f[i]))
    
# As expected, we see a peak at k = 2, which corresponds to f = 0.5(2) = 1Hz"
length_of_time
menu_json
print(menu_json)
menu_python.items()
"for key, value in menu_python.items():
    print(key, 'AND', value)"
myprint(menu_python)
"# create dictionary without carbon
periodic_table = {'Hydrogen': 1, 'Helium': 2}
print(periodic_table)"
"# Since carbon was not present initially, the dictionary is updated with information in the ...
# ... setdefault statement
periodic_table"
periodic_table
food_counter
"for food, count in food_counter.items():
    print(food, count)"
"for food, count in dict_counter.items():
    print(food, count)"
"from collections import Counter
breakfast = ['spam', 'spam', 'eggs', 'spam']
breakfast_counter = Counter(breakfast)
breakfast_counter"
"for video in response:
    print(video)"
response.status_code
"from scipy.optimize import minimize
from scipy.optimize import brute
import matplotlib.pyplot as plt

# related to plots
from pylab import rcParams
rcParams['figure.figsize'] = 10, 10
#from matplotlib.font_manager import FontProperties


# Constants
H_p = 20
H_c = 10
H_t_step = 0.1 # the time of the piece-wise constant inputs
int_t_step = 0.001 # the small time-steps of the integration, the lower the better
b_l = -2 # lower bound of the angular velocity vector
b_u =  2 # upper bound of the angular velocity vector
Q = 10 # Tuning parameter: scaling for the state error
weights = np.ones(H_c)*1 # Tuning parameter: weight of the input signal, the higher , the less signal is applied
xk = tuple(np.array([0,-3.0,np.pi/2]))# initial point: below curve and straight up np.pi/2 0
angle_error_weight = 0.3    # Tuning parameter: Scaling of the angle-error [-pi/2,pi/2], 
                            # the higher, the more the MPC converges to the shape of the curve

# initial guess
V = np.ones(H_c) # fixed at 1 unit/sec
W = np.ones(H_c)*0

# Constraint Functions 
bnds = generateBounds(H_c,b_l,b_u)

objfun = lambda W: J(xk,H_p,H_c,V,W,Q,weights,H_t_step,int_t_step,angle_error_weight)
#objfun(W)
#res = minimize(objfun, W, method='COBYLA',bounds=bnds,options={'maxiter':1000})
res = minimize(objfun, W, method='COBYLA',options={'maxiter':1000})
#SLSQP L-BFGS-B TNC COBYLA
#resbrute = brute(objfun, bnds, Ns=5, full_output=False, disp=True)#Ns=20,

#pdb.set_trace()
print(res)

axarr = plt.subplot(2, 1, 1)
axarr.set_title('Implicit Curve and Trajectory')
plotImplCurve(VF_phi,-3,3,-3,3,2e-3,100)

# Extend the Input Vectors if H_c<H_p
V_total = np.zeros(H_p)
W_total = np.zeros(H_p)
weights_total = np.zeros(H_p)
for i in range(0,H_p):
    if i>=H_c:
        V_total[i] = V[-1]
        W_total[i] = res.x[-1]
        weights_total[i] = 0
    else:
        V_total[i] = V[i]
        W_total[i] = res.x[i]
        weights_total[i] = weights[i]
    
plotTraj(xk,UAVmodel,V_total,W_total,H_t_step,int_t_step)
    

axarr = plt.subplot(2, 1, 2)
axarr.set_title('Cost Function along the simulated Trajectory')

plotTrajCost(xk,UAVmodel,V_total,W_total,H_t_step,int_t_step,weights_total,Q)
#plt.legend()
axarr.legend(loc='upper right', #bbox_to_anchor=(1.5, .45),
           fancybox=True, shadow=True)
plt.show()"
"import warnings
warnings.filterwarnings('ignore')
%matplotlib inline
print(__doc__)

import matplotlib.pyplot as plt
from sklearn.svm import SVC
from sklearn.model_selection import StratifiedKFold
from sklearn.feature_selection import RFECV
from sklearn.datasets import make_classification

# Build a classification task using 3 informative features
X, y = make_classification(n_samples=1000, n_features=25, n_informative=3,
                           n_redundant=2, n_repeated=0, n_classes=8,
                           n_clusters_per_class=1, random_state=0)

# Create the RFE object and compute a cross-validated score.
svc = SVC(kernel=""linear"")
# The ""accuracy"" scoring is proportional to the number of correct
# classifications
rfecv = RFECV(estimator=svc, step=1, cv=StratifiedKFold(2),
              scoring='accuracy')
rfecv.fit(X, y)

print(""Optimal number of features : %d"" % rfecv.n_features_)

# Plot number of features VS. cross-validation scores
#plt.figure()
fig = plt.figure(figsize=(12,8), dpi=300)
plt.xlabel(""Number of features selected"")
plt.ylabel(""Cross validation score (nb of correct classifications)"")
plt.plot(range(1, len(rfecv.grid_scores_) + 1), rfecv.grid_scores_)
plt.show()"
d
d
"# To understand what yield does, you must understand what generators are. 
# And before generators come iterables.

## Iterables
# When you create a list, you can read its items one by one. Reading its items one by one 
# is called iteration:
mylist = [1, 2, 3]
for i in mylist:
    print(i)
    
# mylist is an iterable. When you use a list comprehension, you create a list, 
# and so an iterable:
mylist = [x*x for x in range(3)]
for i in mylist:
    print(i)
    

# Everything you can use ""for... in..."" on is an iterable: lists, strings, files...

# These iterables are handy because you can read them as much as you wish, but you 
# store all the values in memory and this is not always what you want when you have a
# lot of values.

## Generators
# Generators are iterators, but you can only iterate over them once. It's because 
# they do not store all the values in memory, they generate the values on the fly:
mygenerator = (x*x for x in range(3))
for i in mygenerator:
    print(i)
    

# It is just the same except you used () instead of []. 
# BUT, you cannot perform for i in mygenerator a second time since generators can 
# only be used once: they calculate 0, then forget about it and calculate 1, 
# and end calculating 4, one by one.    

## Yield
# Yield is a keyword that is used like return, except the function will return a generator.
def createGenerator():
    mylist = range(3)
    for i in mylist:
        yield i*i
mygenerator = createGenerator() # create a generator
print(mygenerator) # mygenerator is an object!
for i in mygenerator:
    print(i)

# Here it's a useless example, but it's handy when you know your function will return a 
# huge set of values that you will only need to read once.

# To master yield, you must understand that when you call the function, the code 
# you have written in the function body does not run. The function only returns the 
# generator object, this is a bit tricky :-)

# Then, your code will be run each time the for uses the generator.

# The first time the for calls the generator object created from your function, 
# it will run the code in your function from the beginning until it hits yield, 
# then it'll return the first value of the loop. Then, each other call will run the loop you have written in the function one more time, and return the next value, until there is no value to return.

# The generator is considered empty once the function runs but does not hit yield anymore. 
# It can be because the loop had come to an end, or because you do not satisfy a ""if/else"" anymore.

"
"from matplotlib import pyplot

import matplotlib.pyplot as plt
plt.plot([2,4,8,16,32,64,128,256],[0.211856,0.311436,0.554385,0.960423,1.848679,3.622212,10.322792,19.588133])"
"# add 2^n random digits for n = 1 to 8
import random
import timeit

for trial in [2**_ for _ in range(1,9)]:
    numbers = [random.randint(1,9) for _ in range(trial)]
    m = timeit.timeit(stmt='sum = 0\nfor d in numbers:\n\tsum = sum + d',
                     setup='import random\nnumbers = ' + str(numbers))
    print('{0:d} {1:f}'.format(trial,m))
# plot this to see shape of graph"
"import math

print(dir(math))"
help(math.log)
"fig, ax = plt.subplots(subplot_kw={'projection': '3d'})

t = np.linspace(-4*np.pi, 4*np.pi, 100)

r = t**2 + 40
x = r * np.sin(t)
y = r * np.cos(t)
z = t

ax.plot(x, y, z)"
%run ../00_AdvancedPythonConcepts/talktools.py
"import warnings
warnings.filterwarnings('ignore')
%run ../00_AdvancedPythonConcepts/talktools.py"
"plt.plot(x,y, label='sin(x)')
plt.legend()
plt.title('Harmonic')
plt.xlabel('x')
plt.ylabel('y')

# Add one line to that plot
plt.plot(x, z, label='cos(x)')

# Make a second figure with a simple plot
plt.figure()
plt.plot(x, np.sin(2 * x), label='sin(2x)')
plt.legend()"
"f, ax0 = plt.subplots()         # we manually make a figure and axis
ax0.plot(x, y, label='sin(x)')  # it's the axis who plots
ax0.legend()
ax0.set_title('Harmonic')       # we set the title on the axis
ax0.set_xlabel('x')             # same with labels
ax0.set_ylabel('y')

# Make a second figure with a simple plot.  We can name the figure with a
# different variable name as well as its axes, and then control each
f1, ax1 = plt.subplots()
ax1.plot(x, np.sin(2 * x), label='sin(2x)')
ax1.legend()

# Since we now have variables for each axis, we can add back to the first
# figure even after making the second
ax0.plot(x, np.cos(x), label='cos(x)')
ax0.legend()"
"%matplotlib inline

x = np.linspace(0, 2 * np.pi, 400)
y = np.sin(x**2)

# Just a figure and one subplot
f, ax = plt.subplots()
ax.plot(x, y)
ax.set_title('Simple plot')

# Two subplots, unpack the output array immediately
f, (ax1, ax2) = plt.subplots(1, 2, sharey=True)
ax1.plot(x, y)
ax2.scatter(x, y)

# Put a figure-level title
f.suptitle('Sharing Y axis')"
eq1
A_sol[0]
"A_sol[1].subs([(rho_max,15),(u_max,2),(u_star,1.5)])"
B_val
rho_val
find_non_standard_sax(TRN_PATH)
find_non_standard_sax(VLD_PATH)
"df['HomeTeam'].unique() # array
df['HomeTeam'].value_counts()"
df[df['Date'] == '09/05/15']
"def ftr2(x):
    return x.value_counts()

home_ftr_df = pd.DataFrame(home_gb['FTR'].apply(ftr2).unstack())

home_ftr_df.rename(columns={'A': 'Lost','D': 'Draw' , 'H': 'Win'}, inplace=True)

#홈에서 한 번도 진적이 없는 NaN이 뜨는 첼시를 위한 특별 처리
#DataFrame의 모든 NaN값을 0으로 채운다. 

home_ftr_df = home_ftr_df.fillna(0)
home_ftr_df.head()"
"away_gb = working_df.groupby('AwayTeam')
away_goal_df = away_gb.agg({'FTHG' : sum, 'FTAG' : sum })

away_ftr_df = pd.DataFrame(away_gb['FTR'].apply(ftr2).unstack())

#Away 팀의 관점이므로 A는 승리, H는 패배
away_ftr_df.rename(columns={'A': 'Win','D': 'Draw' , 'H': 'Lost'}, inplace=True)

#away_ftr_df
#home_ftr_df

# 홈과 원정의 값들을 서로 더합니다. 
final_ftr = away_ftr_df + home_ftr_df

# 보기싫은 인덱스 이름 제거
#final_ftr.index.name = None

final_ftr

#승점 계산
final_ftr['Point']  = final_ftr['Win'] * 3 + final_ftr['Draw']

#승점 내림 차순으로 정렬
final_ftr.sort_values('Point', ascending = False)"
"mission_completed = pd.concat([final_ftr,final_goal],axis=1)
mission_completed = mission_completed.reindex_axis(['Win','Draw','Lost','GF','GA','GD','Point'], axis=1)

#승점 우선 정렬, 동률인 경우 골득실로 정렬
mission_completed.sort_values(['Point','GD'], ascending = False)"
working_df.plot()
"from datetime import datetime

pd.Series([datetime.now(),'fc-pda',20])"
"pd.Series([datetime.now(), 'fc-pad',20,[1,2,3,4,5]])"
ser.index
fruits.to_dict()
"d = fruits.to_dict()
d"
"fruits2 = pd.Series(d,index=['apple','banana','cherry','orange','plum','melon'])
fruits2"
fruits2
fruits2.sort_index()
fruits2.sort_values(ascending=False)
fruits2.sort_values(ascending=True)
box.plot(kind='bar')
box.plot(kind='barh')
box.rank(ascending=False)
"x = np.linspace(0, 2 * np.pi)
plt.plot(x, np.sin(x), label=r'$\sin(x)$')
plt.plot(x, np.cos(x), 'ro', label=r'$\cos(x)$')
plt.title(r'Two familiar functions')
plt.legend()"
"M = sym.Matrix([[1, 2, 3, 0, 0], [4, 10, 0, 0, 1]])
M.nullspace()"
"from IPython.display import YouTubeVideo
YouTubeVideo('iwVvqwLDsJo')"
"%%ruby
puts ""Hello from Ruby #{RUBY_VERSION}"""
"M = sympy.Matrix([[1, 2, 3, 0, 0], [4, 10, 0, 0, 1]])
M.nullspace()"
"sympy.plot(x, x**2, x**3, (x, -5, 5))"
"f = lambda x: x**2-2*x-6
x = np.linspace(0,5,100)
y = f(x)

plt.plot(x,y,'red')
plt.grid('off')

plt.axhline(y=2,xmin=0,xmax=0.8,linestyle=""--"")
plt.axvline(x=4,ymin=0,ymax=float(5)/9, linestyle = ""--"")

plt.axhline(-6,3.7/5,4.3/5,linewidth = 2, color = 'black')
plt.axvline(1,6.0/18,14.0/18,linewidth = 2, color = 'black')

plt.axhspan(-2,6,0,(1+np.sqrt(13))/5,alpha = 0.15, ec = 'none')
plt.axvspan((1+np.sqrt(5)),(1+np.sqrt(13)),0,1.0/3,alpha = 0.15, ec = 'none')

plt.axhspan(f(3.7),f(4.3),0,4.3/5,alpha = 0.3, ec = 'none')
plt.axvspan(3.7,4.3,0,(f(3.7)+8)/18,alpha = 0.3, ec = 'none')

plt.axis([0,5,-8,10])


plt.text(0.8,-1,r""$\epsilon$"", fontsize = 18)
plt.text(0.8,4,r""$\epsilon$"", fontsize = 18)
plt.text(3.75,-7.0,r""$\delta$"", fontsize = 18)
plt.text(4.1,-7.0,r""$\delta$"", fontsize = 18)
plt.text(3.95,-7.8,r""$a$"", fontsize = 18)
plt.text(4.5,8.5,r""$f(x)$"", fontsize = 18,color=""red"")"
"# Tips
sns.kdeplot(data['Tips'], shade=True, color='g')
graph.xlabel('Tips ($)')
graph.xlim([0, 350])
graph.show

print('N = ', len(data))
print('Median Tips ${}'.format(data['Tips'].median()))
print('Average Tips/Hour ${}/hr'.format(round(data['Tips per Hour'].median(), 2)))
print('Lowest ${} Highest ${}'.format(data['Tips'].min(), data['Tips'].max()))"
"# Time
sns.kdeplot(data['Hours'], shade=True, color='g')

print('Median Time: {}'.format(data['Hours'].median()))
print('Max {} Min {}'.format(data['Hours'].max(), data['Hours'].min()))"
"sns.jointplot(
    x='Hours', 
    y='Tips',
    data=pd.DataFrame(data, columns=['Hours', 'Tips']),
    kind='reg',
    robust=True
)
graph.xlim([0, 20])
graph.ylim([0, 400])
graph.show()"
"# Which day has the best tips per hour rate?
day_array = {0: 'Mon', 1: 'Tues', 2: 'Weds', 3: 'Thurs', 4: 'Fri', 5: 'Sat', 6: 'Sun'}

bar_graph_data = pd.DataFrame(data, columns=['Day', 'Tips per Hour'])
bar_graph_data['Day'] = bar_graph_data['Day'].apply(lambda x: day_array[x])

sns.violinplot(x='Day', y='Tips per Hour', data=bar_graph_data, inner='stick')
graph.title('Which day has the best Tips per Hour Rate?')
graph.ylabel('Tips / Hour ($/hr)')
graph.show()"
"life_expectancy = 78.73
expectancy_std = 15
age_list = np.linspace(40, 100, num=100)

life_distn = norm(life_expectancy, expectancy_std)
life_distn.mean = life_expectancy
life_distn.std = expectancy_std

death_function = lambda age: life_distn.cdf(age)

# Plot the probability of having already died given an age.
graph.plot(age_list, death_function(age_list), linewidth=3)
graph.title('CDF for Mortality')
graph.ylabel('Probability')
graph.xlabel('Age')
graph.show()

graph.plot(age_list, life_distn.pdf(age_list), linewidth=3)
graph.title('PDF for Mortality')
graph.ylabel('Probability Density')
graph.xlabel('Age')
graph.show()"
"x = linspace(0, 5*pi, 100)
y = sin(x)
plot(x, y)"
"#Remeber, plotting without axes labes or captions is a cardinal sin!
plot(x,y)
xlabel('x (radians)')
ylabel('sin(x)')
title('My first plot in Ipython Notebook!')"
params
"#You can put all this onto a JV curve plot
fig = figure()
ax = fig.add_subplot(111)
l1, = ax.plot(xi,yi,'-',color='blue')
yrange = max(yi)-min(yi)
ylim = [0, max(yi)*1.1]
ax.set_ylim(ylim)
ax.set_xlim([0,1])
ax.set_xlabel('$V$, volts')
ax.set_ylabel('$J$, mA.cm$^{2}$')
text = 'Eff: {0:.2f} (%)\nJsc: {1:.2f} (ma/cm^2)\nVoc: {2:.2f} V\nFF: {3:.2f} (%)'.format(p.get('Eff'), p.get('jsc'), p.get('voc'), p.get('FF')*100)
ax.annotate(text, xy=(0.05, 0.4), xycoords='axes fraction')"
"hist(effs, bins = 5)"
"hist(effs, bins=bin_list, facecolor='green')
xlabel('Efficiency (%)')
ylabel('Number of Cells')
title('Histogram of {0} cell efficiencies measured from a single CdTe plate (10cm x 10cm)'.format(len(cells)))
"
"plot = twinplot(headers, data)"
"#To include the legend just set 'leg=True' when calling your function
plot = twinplot(headers, data, leg=True)"
"import warnings
warnings.filterwarnings('ignore')
#LinearRegression fits a linear model with coefficients w = (w_1, ..., w_p) 
#to minimize the residual sum of squares between the observed responses 
#in the dataset, and the responses predicted by the linear approximation. 
#Mathematically it solves a problem of the form:

#{min w.r.t w} {|| X w - y||_2}^2

from sklearn import linear_model
clf = linear_model.LinearRegression()
clf.fit([[0,0],[1,1],[2,2]],[0,1,2])
clf.coef_"
"diabetes = datasets.load_diabetes()
# USe only one feature out of ten
diabetes_x = diabetes.data[:, np.newaxis, 2]

# split the data into training and test set
diabetes_x_train = diabetes_x[:-20]
diabetes_x_test = diabetes_x[-20:]

diabetes_y_train = diabetes.target[:-20]
diabetes_y_test = diabetes.target[-20:]

regr = linear_model.LinearRegression()

regr.fit(diabetes_x_train, diabetes_y_train)

print(""Coeeficients : \n"", regr.coef_)


# Mean square
mean = np.mean( (regr.predict(diabetes_x_test) - diabetes_y_test) ** 2)
### If all features were considered then cost would be
### O(n * p^2) for matrix of size (n,p)
print(""\n\n\nResidual sum of squares :"", mean)
print(""Variance : %.2f"" % regr.score(diabetes_x_test, diabetes_y_test))"
"plt.scatter(diabetes_x_test, diabetes_y_test, color='black')
plt.plot(diabetes_x_test, regr.predict(diabetes_x_test), color='blue')
plt.xticks(())
plt.yticks(())
plt.show()

"
"Edades = np.array([10,15, 16, 17, 18, 19, 20, 21, 22, 23, 24,25,26,30,32,33,34,37,38])
Frecuencia = np.array([2,1,20,12,12,20,16,17,9,4,1,2,1,2,1,1,1,1,3])
print(sum(Frecuencia))
plt.bar(Edades, Frecuencia)
plt.show()"
"x = np.random.rand(100)
sns.distplot(x,kde=False)
plt.xlim(0,1)
plt.show()"
"s = np.random.poisson(10,20000)
sns.distplot(s,kde=False)
plt.show()"
"numeros = np.random.normal(loc=2.0,scale=0.1,size=1000)
sns.distplot(numeros)
plt.xlim(0,4)
plt.show()"
"sns.pairplot(df,hue='Tipo')
plt.title('Distribuciones de Datos')
plt.show()"
"import warnings
warnings.filterwarnings('ignore')
import pandas as pd
import numpy as np # modulo de computo numerico
import matplotlib.pyplot as plt # modulo de graficas
# esta linea hace que las graficas salgan en el notebook
import seaborn as sns
%matplotlib inline"
"df = pd.read_csv('files/mini-LHC.csv')
df.head()"
df['PRI_met']
"sns.boxplot(x=""Label"", y=""DER_mass_MMC"",data=df)
plt.show()"
"sns.distplot(boson_df[""DER_mass_MMC""],label='boson')
sns.distplot(ruido_df[""DER_mass_MMC""],label='ruido')
plt.ylabel('Frecuencia')
plt.legend()
plt.title(""Distribucion de DER_mass_MMC"")
plt.show()"
"ejeX = ""DER_mass_MMC""
ejeY = ""PRI_tau_pt""
plt.scatter(df[ejeX],df[ejeY],alpha=0.5)
plt.xlabel(ejeX)
plt.ylabel(ejeY)
plt.show()"
"ejeX = ""DER_mass_MMC""
ejeY = ""PRI_tau_pt""
plt.scatter(boson_df[ejeX],boson_df[ejeY],c='r',alpha=0.9,s=20,label='boson',lw=0)
plt.scatter(ruido_df[ejeX],ruido_df[ejeY],c='g',alpha=0.1,s=10,label='ruido',lw=0)
plt.xlabel(ejeX)
plt.ylabel(ejeY)
plt.legend()
plt.show()"
"import warnings
warnings.filterwarnings('ignore')
import matplotlib.pyplot as plt
import numpy as np
%matplotlib inline

a = np.random.randint(1,10,10)
plt.hist(a)"
"import warnings
warnings.filterwarnings('ignore')
%matplotlib inline
import numpy as np
import matplotlib.pyplot as plt


N = 20
theta = np.linspace(0.0, 2 * np.pi, N, endpoint=False)
radii = 10 * np.random.rand(N)
width = np.pi / 4 * np.random.rand(N)

ax = plt.subplot(111, polar=True)
bars = ax.bar(theta, radii, width=width, bottom=0.0)

# Use custom colors and opacity
for r, bar in zip(radii, bars):
    bar.set_facecolor(plt.cm.jet(r / 10.))
    bar.set_alpha(0.5)
plt.show()"
"%matplotlib inline
from random import uniform
import matplotlib.pyplot as plt


def generate_data(n, generator_type):
    epsilon_values = [generator_type(0,1) for i in range(n)]
    return epsilon_values

data = generate_data(100, uniform)
plt.plot(data, 'b-')
plt.show()"
"from random import uniform
def binomial_rv(n,p):
    count=0
    for i in range(n):
        U=uniform(0,1)
        if U<p:
            count+=1
    return count
binomial_rv(100,0.5)"
"from random import uniform
from math import sqrt
def montepithon(n):
    count=0
    for i in range(n):
        x=uniform(0,1)
        y=uniform(0,1)
        if sqrt(((x-0.5)**2) + ((y-0.5)**2))<0.5:
            count+=1
    return 4*(count/n)
montepithon(10000)"
"from random import uniform
payoff=0
count=0
for i in range(10):
    U = uniform(0, 1)
    count = count + 1 if U < 0.5 else 0
    if count == 3:
        payoff=1
payoff"
"%matplotlib inline
from random import normalvariate
import matplotlib.pyplot as plt
alphas = [0.0, 0.8, 0.98]
ts_length = 200

for alpha in alphas:
    x_values = []
    current_x = 0
    for i in range(ts_length):
        x_values.append(current_x)
        current_x = alpha * current_x + normalvariate(0, 1)
    plt.plot(x_values, label='alpha = ' + str(alpha))
plt.legend()"
"def capscount(string):
    count=0
    a=string.upper()
    for i in range(len(string)):
        if string[i]==a[i]:
            count+=1
    return count
capscount('AlSDFqqQl')"
""
"d = {'one':[1,1,1,1,1],
    'two':[2,2,2,2,2],
    'letter':['a','a','b','b','c']}

df = pd .DataFrame(d)
df"
letterone.index
stack.index
unstack.index
"import warnings
warnings.filterwarnings('ignore')
""""""
Problem 2
=========


   Each new term in the Fibonacci sequence is generated by adding the
   previous two terms. By starting with 1 and 2, the first 10 terms will be:

                     1, 2, 3, 5, 8, 13, 21, 34, 55, 89, ...

   By considering the terms in the Fibonacci sequence whose values do not
   exceed four million, find the sum of the even-valued terms.
""""""

def project_euler_2(x):
    """"""Returns sum of even Fibonacci numbers, sequence limit is x.""""""
    fibsum = 0
    a = 1
    b = 1
    
    while(a < x):
        if a % 2 == 0:
            fibsum += a
        a, b = b, a + b
    return fibsum

# project_euler_2(4000000)

%timeit project_euler_2(4000000)

# %prun project_euler_2(4000000)"
"import warnings
warnings.filterwarnings('ignore')
from scipy.special import comb

comb(40,20)"
"import warnings
warnings.filterwarnings('ignore')
"
"import warnings
warnings.filterwarnings('ignore')
""""""
Problem 1
=========


   If we list all the natural numbers below 10 that are multiples of 3 or 5,
   we get 3, 5, 6 and 9. The sum of these multiples is 23.

   Find the sum of all the multiples of 3 or 5 below 1000.
""""""

from itertools import *

def project_euler_1(x, y, a, b):
    """"""Returns sum of multiples of a and b between x and y.""""""
    return sum(set(chain(range(x, y, a), range(x, y, b))))

# project_euler_1(0, 1000, 3, 5)
    
# timeit project_euler_1(0,1000,3,5)

# %prun project_euler_1(0,1000,3,5)"
"import warnings
warnings.filterwarnings('ignore')
import itertools
from sympy.ntheory import isprime

limit = 1000

abcombos = list(itertools.product(range(-(limit - 1),limit), repeat = 2))

n= 0
maxab = (0,0)
maxn = 0 

for a, b in abcombos: 
    n = 0      # start with n = 0
    while isprime((n+1)**2 + a*(n+1) + b): # while quadratic expression per (a,b) tuple produces primes: 
        n += 1 # increment result counter
    if n > maxn: # when the expression produces a non-prime for the first time, check if the previous run 
        maxn = n     # new n with the most primes per (a,b) tuple
        maxab = a, b # new (a,b) tuple 

print(maxab[0]*maxab[1])"
"# if there is a match, it gives back a match object
pattern.match(text)"
"# search at the beginning: ^
# search at the end: $

text = ""Python is a lot of fun""

pattern = re.compile(""^Python"")
pattern.search(text)"
"url = ""http://pythonscraping.com/pages/page1.html""

# go and check this url! use Right click -> Inspect element to discover what kind of tags
# enclose information you are looking for

response = urlopen(url)
html_text = response.read()

soup = BeautifulSoup(html_text)"
"import warnings
warnings.filterwarnings('ignore')
import numpy as np
import pandas as pd
import seaborn as sns
%pylab inline"
"pc=PowerCalc()
ds=np.logspace(start=0,stop=2,num=500)
plt.figure(figsize=(15,7))
plt.scatter(ds,[pc.PL(d) for d in ds])
plt.title('Signal attenuation PL(d)',fontsize=20)
plt.xlabel('Distance(m)',fontsize=16)
a=plt.ylabel(""Power(dBm)"",fontsize=16)"
"values=np.arange(100)

df=pd.DataFrame(data=life_CC2430(values),columns=['Anys de vida'])
df.index.name='Mesures per hora'
df.plot(kind='bar',figsize=(20,8))
plt.title(""Temps mig de vida d'un node sensor"",fontsize=20)
plt.xlabel('Mesures per hora',fontsize=16)
dump=plt.ylabel(""Anys de vida"",fontsize=16)
#plt.savefig('mesures.png')"
"delay_1_jump(payload=118,inf_address=4,ack=False,verbose=True)"
"datas_dados = [] # cria-se uma lista vazia para ser utilizada posteriormente, adicionando-se dados nela
dados_temperatura = [] # cria-se uma lista vazia para ser utilizada posteriormente, adicionando-se dados nela
media_anual = [] #cria-se uma lista vazia para ser utilizada posteriormente, adicionando-se dados nela
arquivo = open('data/23.31S-42.82W-TAVG-Trend.txt') # abrir o arquivo 'data/23.31S-42.82W-TAVG-Trend.txt' e
# nomeando de 'arquivo' o arquivo aberto (mnemômico)
for linha in arquivo: # utilizamos o comando 'for' e 'in' para percorrer com a variável 'linha' o arquivo aberto
    if linha[0] != '%' and linha[0]!= '\n' : # restringimos as linhas que terão os dados utilizados, não usaremos as
        # linhas que contem (%) e (\n = espaços vazios)
        blocos = linha.split( )  # separamos as linhas em blocos que contém as informações que queremos
        if blocos[2] != 'NaN': # restringimos as informações dos blocos que serão utilizados, não utilizaremos os
            # blocos que contem (NaN = informção perdida)
            ano = float(blocos[0]) # transformamos as informações do bloco[0] de texto para número real e atribuimos
            # esse valor a variável ano
            mes = float(blocos[1]) # transformamos as informações do bloco[1] de texto para número real e atribuimos
            # esse valor a variável mês
            temperatura = float(blocos[2]) + 24.01 # transformamos as informações do bloco[2] de texto para número
            # real e atribuimos esse valor a variável temperatura e adicionamos 24.01°C que foi a temperatura média 
            # registrada, variando +/- por esse valor
            anual = float(blocos[4]) + 24.01 # transformamos as informações do bloco[4] de texto para número
            # real e atribuimos esse valor a variável anual e adicionamos 24.01°C que foi a temperatura média registrada,
            # variando +/- por esse valor
            data = ano + mes/12 # juntamos a variável ano e mês para usarmos uma única variável 'data' assim o mês
            # passa a ser uma fração do ano
            datas_dados.append(data) # preenchemos a lista datas_dados criada anteriormente com os resultados obtidos
            # em 'data'
            dados_temperatura.append(temperatura) # preenchemos a lista dados_temperatura criada anteriormente com os
            # resultados obtidos em 'temperatura'
            media_anual.append(anual) # preenchemos a lista media_anual criada anteriormente com os resultados obtidos
            # em anual

plt.figure(figsize = [7, 4]) # comando para plotar um gráfico a ser definido
plt.plot(datas_dados, dados_temperatura, '.k', label = 'Média mensal') # plotar dados_temperatura em funcao de
# datas_dados, nomeando este resultado de Média mensal
plt.plot(datas_dados, media_anual, 'r', label = 'Média móvel anual', linewidth = 2) # plotar media_anual em funcao de datas_dados
# nomeando este resultado de Média móvel anual
plt.xlabel('Ano') # chamar a abscissa do gráfico de Ano
plt.ylabel('Temperatura Média (°C)') # chamar a ordenada do gráfico de Temperatura Média (°C)
plt.xlim(1830, 2015) # definir o intervalo dos dados estudados no eixo da abscissa
plt.grid(ls = ':', which = 'major', axis = 'both') # plotar a grade do gráfico com pontos
plt.legend(loc = 'upper left', fontsize = 'large' ) # plotar a legenda do gráfico
"
"for _ in range (10):
    print(random.random())"
"for _ in range(10):
    if(random.random() < .5):
        print(""head"")
    else:
        print(""tail"")"
"%matplotlib inline
import matplotlib.pyplot as plt
import pandas as pd

'''This script demonstrates simulations of coin flipping'''
import random

# let's create a fair coin object that can be flipped:

class Coin(object):
    '''this is a simple fair coin, can be pseudorandomly flipped'''
    sides = ('heads', 'tails')
    last_result = None

    def flip(self):
        '''call coin.flip() to flip the coin and record it as the last result'''
        self.last_result = result = random.choice(self.sides)
        return result

# let's create some auxilliary functions to manipulate the coins:

def create_coins(number):
    '''create a list of a number of coin objects'''
    return [Coin() for _ in range(number)]

def flip_coins(coins):
    '''side effect function, modifies object in place, returns None'''
    for coin in coins:
        coin.flip()

def count_heads(flipped_coins):
    return sum(coin.last_result == 'heads' for coin in flipped_coins)

def count_tails(flipped_coins):
    return sum(coin.last_result == 'tails' for coin in flipped_coins)


def main():
    coins = create_coins(1000)
    dist = []
    for i in range(100):
        flip_coins(coins)
#         print(count_heads(coins))
        n = count_heads(coins) # Mine
        dist.append(n)
    
    pd.Series(dist).hist() # Mine
    plt.show()  # Mine

if __name__ == '__main__':
    main()"
"%matplotlib inline
import matplotlib.pyplot as plt
import pandas as pd

'''This script demonstrates simulations of coin flipping'''
import random
from scipy.stats import truncnorm

# let's create a fair coin object that can be flipped:

class Coin(object):
    '''this is a simple fair coin, can be pseudorandomly flipped'''
    sides = [str(int(n)) for n in list(truncnorm(a=0, b=1, scale=100).rvs(size=100))] # Mine (non-strings don't work?!)
    last_result = None

    def flip(self):
        '''call coin.flip() to flip the coin and record it as the last result'''
        self.last_result = result = random.choice(self.sides)
        return result

# let's create some auxilliary functions to manipulate the coins:

def create_coins(number):
    '''create a list of a number of coin objects'''
    return [Coin() for _ in range(number)]

def flip_coins(coins):
    '''side effect function, modifies object in place, returns None'''
    for coin in coins:
        coin.flip()

def count_heads(flipped_coins):
    return sum(coin.last_result == '1' for coin in flipped_coins)

def count_tails(flipped_coins):
    return sum(coin.last_result == '2' for coin in flipped_coins)


def main():
    coins = create_coins(1000)
    dist = []
    for i in range(100):
        flip_coins(coins)
#         print(count_heads(coins))
        n = count_heads(coins) # Mine
        dist.append(n) # Mine
        
    pd.Series(dist).hist() # Mine
    plt.show()  # Mine

    
if __name__ == '__main__':
    main()"
"%matplotlib inline
import matplotlib.pyplot as plt
import pandas as pd

'''This script demonstrates simulations of coin flipping'''
import random
from scipy.stats import truncnorm

# let's create a fair coin object that can be flipped:

class Coin(object):
    '''this is a simple fair coin, can be pseudorandomly flipped'''
    sides = [str(int(n)) for n in list(truncnorm(a=0, b=1, scale=100).rvs(size=100))] # Mine (non-strings don't work?!)
    last_result = None

    def flip(self):
        '''call coin.flip() to flip the coin and record it as the last result'''
        self.last_result = result = random.choice(self.sides)
        return result

# let's create some auxilliary functions to manipulate the coins:

def create_coins(number):
    '''create a list of a number of coin objects'''
    return [Coin() for _ in range(number)]

def flip_coins(coins):
    '''side effect function, modifies object in place, returns None'''
    for coin in coins:
        coin.flip()

def count_heads(flipped_coins):
    return sum((coin.last_result == '1') or (coin.last_result == '100') for coin in flipped_coins)

def count_tails(flipped_coins):
    return sum(coin.last_result == '2' for coin in flipped_coins)


def main():
    coins = create_coins(1000)
    dist = []
    for i in range(100):
        flip_coins(coins)
#         print(count_heads(coins))
        n = count_heads(coins) # Mine
        dist.append(n) # Mine
        
    pd.Series(dist).hist() # Mine
    plt.show()  # Mine

    
if __name__ == '__main__':
    main()"
""
iris.target
"import matplotlib.pyplot as plt
plt.scatter(iris.data[:, 1], iris.data[:, 2], c=iris.target)
plt.xlabel(iris.feature_names[1])
plt.ylabel(iris.feature_names[2])"
"plt.scatter(iris.data[:, 1], iris.data[:, 2], c=iris.target)
plt.xlabel(iris.feature_names[1])
plt.ylabel(iris.feature_names[2])
plt.show()"
"#The first 100 observations correspond to setosa and versicolor
plt.scatter(iris.data[0:100, 1], iris.data[0:100, 2], c=iris.target[0:100])
plt.xlabel(iris.feature_names[1])
plt.ylabel(iris.feature_names[2])
plt.show()"
"from sklearn import svm
svc = svm.SVC(kernel='linear')
X = iris.data[0:100, 1:3] 
y = iris.target[0:100] # MZ: the classes
svc.fit(X, y)"
"def plot_estimator(estimator, X, y):
    estimator.fit(X, y)
    x_min, x_max = X[:, 0].min() - .1, X[:, 0].max() + .1
    y_min, y_max = X[:, 1].min() - .1, X[:, 1].max() + .1
    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 100),
                         np.linspace(y_min, y_max, 100))
    Z = estimator.predict(np.c_[xx.ravel(), yy.ravel()])

    # Put the result into a color plot
    Z = Z.reshape(xx.shape)
    plt.figure()
    plt.pcolormesh(xx, yy, Z, cmap=cmap_light)

    # Plot also the training points
    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=cmap_bold)
    plt.axis('tight')
    plt.axis('off')
    plt.tight_layout()
    plt.show()

plot_estimator(svc, X, y)"
iris.target_names
"# y = iris.target[0:100] # MZ: the classes

for y in [iris.target[0:100], np.array(list(iris.target[0:50]) + list(iris.target[100:])), iris.target[50:]]:
    for comb in [(0, 1), (0, 2), (0, 3), (1, 2), (1, 3), (2, 3)]:
        a, b = comb
        f1, f2 = iris.target_names[y[0]], iris.target_names[y[-1]]
        print(f1, '&', f2, '|', iris.feature_names[a], 'vs', iris.feature_names[b], ':')
        X = iris.data[0:100, [a,b]] 
        plot_estimator(svc, X, y)
    "
"X = iris.data[0:150, [0,1]]
y = iris.target[0:150]

plot_estimator(svc, X, y)"
"# versicolor vs virginica [50:]
svc = svm.SVC(kernel='linear', C=1)

X = iris.data[50:, [0,1]]
y = iris.target[50:]

plot_estimator(svc, X, y)


svc = svm.SVC(kernel='linear', C=2)
X = iris.data[50:, [0,1]]
y = iris.target[50:]

plot_estimator(svc, X, y)"
"# further populate tables created from nested element
json_normalize(data, 'counties', ['state', 'shortname', ['info', 'governor']])"
"# load as Pandas dataframe
sample_json_df = pd.read_json('data/world_bank_projects_less.json')
sample_json_df"
"# Steps: import data from csv file, convert units, plot
df = pd.read_csv('cricket.csv', skiprows=1, names=['tF', 'chirps'])

df['T_inv']= 1/((df.tF + 459.67) * 5/9)
df['ln_chirps'] = np.log(df.chirps)

df.plot(x='tF', y='chirps', kind='scatter')"
"# the ""formula"" api
import statsmodels.formula.api as smf

fit = smf.ols(formula=""ln_chirps ~ T_inv"", data=df).fit()

# print() suppresses html output; remove if you want html output
print(fit.summary())"
"# matplotlib
plt.scatter(df.T_inv, df.ln_chirps)
plt.plot(df.T_inv, fit.fittedvalues)
plt.xlim(df.T_inv.min(), df.T_inv.max())
plt.xlabel('1/T')
plt.ylabel('ln chirps/sec')"
"# seaborn's regression plot shows 95% probability band
sn.regplot(x = ""T_inv"", y = ""ln_chirps"", data = df)"
"sn.residplot('T_inv', 'ln_chirps', df)
sn.jointplot('T_inv', 'ln_chirps', df, kind='resid', 
                xlim=(df.T_inv.min(), df.T_inv.max()), size=8)"
"from scipy.constants import gas_constant as R

Ea = -R * fit.params.T_inv/1000
Ea"
"perr = np.sqrt(np.diag(pcov))
popt, pcov, perr"
"[ cp(298) for cp in [C_NH3, C_H2, C_N2] ] # J/mol K"
"T = Symbol('T')
p1 = plot(C_N2(T), (T, 300,1500), xlim=(0,1600), xlabel=""T/K"")"
"p2=plot(C_H2(T), (T, 300, 1500), line_color='r')"
p1.show()
"# a quick visualization to verify data entry
scatter(temp_C, pressure_torr)"
"# are the data now linear?
scatter(temp_inv, ln_P)
xlim(0.003, 0.0035)"
"# Taylor 8.10, 8.11
A = ( sum(X**2) * sum(Y) - sum(X)*sum(X*Y) ) / Delta
B = (N * sum(X*Y) - sum(X)*sum(Y)) / Delta

A, B"
"# generate an array of equally spaced x values
x = linspace(temp_inv.min(), temp_inv.max(), 100)

plot(x, B*x + A)
scatter(X, Y)
xlabel('1/T/K')
ylabel('ln(P)')"
"# compute residuals
resids = (B*X + A) - Y

plot(X, resids, 'o')
axhline(color='k')
ylabel('$y - \overline{y}$')"
"H_vap = -B * 8.314/ 1000 # kJ/mol
H_vap"
"# Need sigma_Y first; Taylor 8.15
sigma_Y = sqrt(1/(N-2) * sum((Y - A - B*X)**2))

# Taylor 8.16, 17
sigma_A = sigma_Y * sqrt( sum(X**2) / Delta )
sigma_B = sigma_Y * sqrt(N / Delta)

sigma_A, sigma_B"
"NIST_H = array([
38.56,
41.7,
39.3,
40.7,
40.5,
35.2])

H_vap, NIST_H.mean(), H_vap - NIST_H.mean()"
"# using statsmodels api for Ordinary Least Squares
import statsmodels.api as sm

#sm.OLS doesn't include the intercept by default; add one.
x = sm.add_constant(X)

model = sm.OLS(Y,x)
results = model.fit()

# don't use print() if prefer html formatted output
print(results.summary())"
"from scipy.optimize import curve_fit

def fit_function(x, m, b):
    return m*x + b

popt, pcov = curve_fit(fit_function, X, Y)

perr = np.sqrt(np.diag(pcov))
popt, perr"
"import seaborn as sn
import pandas as pd

df = pd.DataFrame({""x"":X, ""y"":Y})

sn.regplot(x = ""x"", y = ""y"", data = df)"
"sn.residplot(""x"",""y"", df)"
"sn.jointplot(""x"", ""y"", df, kind='resid', xlim=(df.x.min(), df.x.max()), size=8)"
"# Construindo o gráfico
plt.figure()
plt.plot(x1, y1, '.k', label = 'Média mensal')
plt.plot(x2, y2, '-r', linewidth = 2, label = 'Média movel anual')
#legenda
legend = plt.legend(loc='upper left', shadow=True, fontsize='large')
# Dá título ao eixo x.
plt.xlabel(""Ano"")
plt.xlim(min(x1), max(x1))
# Dá título ao eixo y.
plt.ylabel(""Temperatura Média (°C)"")
plt.grid(b=None, which='major', axis='both')"
"# Construindo o gráfico
plt.figure()
plt.plot(t1, t2, 'ok', label = 'Média Anual')
plt.plot(x2, y2, '-r', linewidth = 1, label = 'Média móvel anual')
#legenda
legend = plt.legend(loc='upper left', shadow=True, fontsize='large')
# Dá título ao eixo x.
plt.xlabel(""Ano"")
plt.xlim(min(x1), max(x1))
# Dá título ao eixo y.
plt.ylabel(""Temperatura Média (°C)"")
plt.grid(b=None, which='major', axis='both')"
"data = np.random.randn(7,4)
data"
data[:1]
data < .7
data[data < .7]
"names = np.array(['Al', 'Bo', 'Bo', 'Bo', 'Al', 'Al', 'Bo'])
names"
names != 'Al'
data
data[names != 'Al']
"np.array([False, False]) | np.array([True, True])"
"np.array([False, True]) & np.array([True, False])"
"data[data < .7] = 0
data"
"data[[1,3,5]]"
"arr = np.random.randn(4,4)
arr"
"np.where(arr > 0, 2, -2)"
"np.where(arr > 0, 2, arr)"
obj.index
obj2 + obj3
"data = {'state': ['Ohio', 'Ohio', 'Ohio', 'Nevada', 'Nevada'],
        'year': [2000, 2001, 2002, 2001, 2002],
        'pop': [1.5, 1.7, 3.6, 2.4, 2.9]}
frame = DataFrame(data)
frame"
"frame['debt'] = float('NaN')
frame"
frame.ix[0]
frame.T
frame.index
"obj.reindex(['a', 'b', 'c', 'd', 'e'])"
frame
"frame.reindex([1, 2, 3, 4, 0])"
"frame2 = frame.copy()
frame2 = frame2.drop(0)
frame2"
"frame2 = frame2.drop('year', axis=1)
frame2"
frame.ix[0]
"frame[frame['pop'] > 2] = 2
frame"
"frame.ix[0, 'pop'] = 1.49
frame"
"print(HTML(html_template.substitute({'css': css, 'js': js})).data)"
dir() # 모듈 임포트 확인
data = put_to_store(the_files)
data
"data_copy = get_from_store()
for each_athlete in data_copy:
    print(data[each_athlete].name + ' ' + data[each_athlete].dob)"
"import warnings
warnings.filterwarnings('ignore')
a = 5
print(""a=5"",hex(id(a)),type(a), ""\n"")

a = 's'
print(""a=s"",hex(id(a)),type(a), ""\n"")

a = 5 #if I changed a back to 5 Python cleverly assign it back to the memory location when it initially stored 5
print(""a=5"",hex(id(a)),type(a), ""\n"")

a = 10 #what if i changed the value but still an integer. Memory location is changes
print(""a=10"",hex(id(a)),type(a), ""\n"")

a = 5 #python still cleverly picks up the original memory location
print(""a=5"",hex(id(a)),type(a), ""\n"")

a = ""string""
print(""a=string"",hex(id(a)),type(a), ""\n"")"
"import warnings
warnings.filterwarnings('ignore')
"
"#Example functions that work with iterables and iterators.
#note iterables are the not the same as iterators

%time numbers = [1,3,2,10,19,211,4] #list is iterable
numbers = [1,3,2,10,19,211,4] #list is iterable
print(numbers, ""built in function max value"", max(numbers), ""\n"")

%time it_numbers = iter(numbers) #converted into a iterator
it_numbers = iter(numbers) #converted into a iterator 
print(it_numbers, ""built in function max value"", max(numbers), ""\n"")


#Replicating the built in function

def myMax(numbers, maximum=0):
    assert(any(numbers)) #any() tests variable is iterable
           
    for i in numbers:
        if maximum < i:
            maximum = i
    return maximum    

print(numbers,""my own function"", myMax(numbers))
print(it_numbers,""my own function"", myMax(it_numbers))
"
"#Introduction to iterators
alphabet = ['a','b','c'] #first 3 letters in the alphabet
it = iter(alphabet) #converts list into list iterator type
print(it)
print(next(it)) # 'a'
print(next(it)) # 'b'
print(next(it), ""\n"") # 'c'

#iterators can also be accessed through for loop (the most common approach)
print(it)
for i in iter(alphabet):
    print(i)"
"# in, not in function works for iterators
#And showcasing iterators do not store the values. And can only been seen once during the access

alphabet = ['a','b','c']
it = iter(alphabet)

print('a' in it) #'a'
print('b' in it) #'b'
print('b' in it) #'c'
print('c' in it) # empty iterator
"
""
"""""""Testing subtraction between between values that are next to each other

There doesn't seem to much meaning out of the visualisation, except the spikes occuring at 4000, which may be useful
for differentiating phase AB to C o

compute a moving average (i.e a dynmamic mean)
We don't have to computer every possible point, we have a frequency of the sampling so that it doesn't take too much 
computational power.

sampled the sample in order to find phase A to B. To get a more generalised shape to simplify the graph. (less datapoints)



things to do:
* merge all the graphs together with their mean value for each recording.
Reason: Less computation requirement and simplifies the problem.

* smooth curve to get be able to find the phases positions more clearly, 
although it may lose some accuracy with the position. How? By resampling (in our case we could slice) in order to remove some of the data points
Although we want to keep the original dataframe as well.



*finding the index (recording no.) for A-B, How? by finding the minimum value between 0 and 2000.

*apply index values (we should have 2 if we merge the graphs) to orginal and slice accordingly+plot.
""""""

fresh_mean = fresh_fish.mean(axis=1) #mean of all the fresh fish. #gives out series


#find the max difference in order to find the spike i.e where phase C begins
change = fresh_mean.diff()
change = change.abs() #absolute values
max_difference = change[3000:5000].max()  

max_value_location = change[change == max_difference]
max_value_location = max_value_location.index[0]
""""""
#remove the Nan in specified column and obtain index of remaining values
print(max_value_location['ID: 5'].dropna().index[0])
print(max_value_location['ID: 6'].dropna().index[0])
print(max_value_location['ID: 7'].dropna().index[0])
""""""

#find where phase B begins
change_resample = fresh_mean[::100].diff()
change_resample = change_resample.abs()
min_difference = change_resample[0:11].max() #slice the row range. 
min_value_location = change_resample[change_resample == min_difference]
min_value_location = min_value_location.index[0]


#plot phase sections
phaseA = fresh_mean[0:min_value_location]
phaseB = fresh_mean[min_value_location:max_value_location]
phaseC = fresh_mean[max_value_location:]

phaseA.plot()
phaseB.plot()
phaseC.plot()





"
"fresh_mean.diff().plot(title='Fresh fish mean difference')
fresh_fish.diff().plot(title='Fresh fish difference')"
"#
image_data['powerbox'] = dict()
image_data['powersector'] = dict()


##plt.imshow(image_data['regions']['S1'][0]+image_data['regions']['S1'][1])

#calculate every power values
for char in image_data['mag']:
    image_data['powerbox'][char] = image_data['regions'][char][0].sum() + image_data['regions'][char][1].sum()
    image_data['powersector'][char] = (image_data['regions'][char][2].sum() 
                                       + image_data['regions'][char][3].sum()
                                       + image_data['regions'][char][4].sum() 
                                       + image_data['regions'][char][5].sum()
                                       )
    #power values calc
#box_feature = box_region[0].sum()+box_region[1].sum()
#sector_feature = tr_sector.sum()+tl_sector.sum()+br_sector.sum()+bl_sector.sum()
                                    
    

#Visualise feautures plot against eachother
#x axis - sector, y axis-box
for char in image_data['powerbox']:
    #visually differentiate the characters by a colour and marker
    if 'S' in char:
        S_plot = plt.scatter(image_data['powersector'][char], image_data['powerbox'][char], c='r', marker='+',linewidth=30)
    elif 'T' in char:
        T_plot = plt.scatter(image_data['powersector'][char], image_data['powerbox'][char], c='g', marker='+', linewidth=30)
    elif 'V' in char:
        V_plot = plt.scatter(image_data['powersector'][char], image_data['powerbox'][char], c='m', marker='+', linewidth=30)

plt.legend((S_plot, T_plot, V_plot),
           ('S', 'T', 'V'),
           scatterpoints=1,
           loc='lower right',
           ncol=3,
           fontsize=30)"
"def roundd(number, i):
    """"""
    i==0 round down
    i==1 round up
    """"""
    if i==0:
        return floor(number / 10**(len(str(abs(int(number))))-2) ) * 10**(len(str(abs(int(number))))-2)
    elif i==1:
        return ceil(number / 10**(len(str(abs(int(number))))-2) ) * 10**(len(str(abs(int(number))))-2)
    else:
        print(""Error"") 
    
    
#find lower and upper limit
maxpb = max(list(image_data['powerbox'].values()))
minpb = min(list(image_data['powerbox'].values()))
minps = min(list(image_data['powersector'].values()))
maxps = max(list(image_data['powersector'].values()))

    
#create meshgrid for prediction with classifier
x = np.linspace(roundd(minps,0), roundd(maxps,1), 21)
y = np.linspace(roundd(minpb,0), roundd(maxpb,1), 13)

XX, YY = np.meshgrid(x,y)

plt.scatter(XX,YY)
        "
"#apply nearest neighbour classifier
""""""
The KNN classifer training data takes an array X (training) and a y the class labelling the training
note: each row is labelled with a class (which is contained in y)

""""""
#
neighbour = KNeighborsClassifier(n_neighbors=5)

#handle Training set with labelled class
training_data = [[image_data['powersector'][char], image_data['powerbox'][char], char[0]] for char in image_data['powersector']]

#convert to numpy array for slicing
training_data = np.array(training_data)

#convert power values into str->float
neigh_X = training_data[:,:2].astype(np.float)

neighbour.fit(neigh_X, training_data[:,2])

Z = neighbour.predict(np.c_[XX.ravel(),YY.ravel()])

#
grid = np.c_[XX.ravel(),YY.ravel()]

for char in image_data['powerbox']:
    #visually differentiate the characters by a colour and marker
    if 'S' in char:
        S_plot = plt.scatter(image_data['powersector'][char], image_data['powerbox'][char], c='r', marker='+',linewidth=30)
    elif 'T' in char:
        T_plot = plt.scatter(image_data['powersector'][char], image_data['powerbox'][char], c='g', marker='+', linewidth=30)
    elif 'V' in char:
        V_plot = plt.scatter(image_data['powersector'][char], image_data['powerbox'][char], c='m', marker='+', linewidth=30)

plt.legend((S_plot, T_plot, V_plot),
           ('S', 'T', 'V'),
           scatterpoints=1,
           loc='lower right',
           ncol=3,
           fontsize=30)


for i in range(len(grid)):
    if 'S' == Z[i]:
        plt.scatter(grid[i][0], grid[i][1], c='r', marker='x',linewidth=10)
    elif 'T' == Z[i]:
        plt.scatter(grid[i][0], grid[i][1], c='g', marker='x', linewidth=10)
    elif 'V' == Z[i]:
        plt.scatter(grid[i][0], grid[i][1], c='m', marker='x', linewidth=10)


    
"
"#Test images

#magnitude spectrum 
S = fourier_space('test/S.gif')
T = fourier_space('test/T.gif')
V = fourier_space('test/V.gif')
A = fourier_space('characters/A1.GIF')
B = fourier_space('characters/B1.GIF')

#
S_freq = extract_frequency(S)
T_freq = extract_frequency(T)
V_freq = extract_frequency(V)
A_freq = extract_frequency(A)
B_freq = extract_frequency(B)

#power values
box_S = S_freq[0].sum() + S_freq[1].sum()
sector_S = S_freq[2].sum() + S_freq[3].sum() + S_freq[4].sum() + S_freq[5].sum()

box_T = T_freq[0].sum() + T_freq[1].sum()
sector_T = T_freq[2].sum() + T_freq[3].sum() + T_freq[4].sum() + T_freq[5].sum()

box_V = V_freq[0].sum() + V_freq[1].sum()
sector_V = V_freq[2].sum() + V_freq[3].sum() + V_freq[4].sum() + V_freq[5].sum()

box_A = A_freq[0].sum() + A_freq[1].sum()
sector_A = A_freq[2].sum() + A_freq[3].sum() + A_freq[4].sum() + A_freq[5].sum()

box_B = B_freq[0].sum() + B_freq[1].sum()
sector_B = B_freq[2].sum() + B_freq[3].sum() + B_freq[4].sum() + B_freq[5].sum()

test_data = np.array([[sector_S,box_S],
             [sector_T, box_T],
             [sector_V, box_V],
             [sector_A, box_A],
             [sector_B, box_B]])
#
Z_test = neighbour.predict(test_data) 

#mean accuracy of prediction
#accuracy = neighbour.score(test_data, ['S','T','V']) * 100






#visualise points
for i in range(len(test_data)):
    if 'S' ==  Z_test[i]:
        plt.scatter(test_data[:,0], test_data[:,1],c='r',marker='x',linewidth=30)
    elif 'T' == Z_test[i]:
        plt.scatter(test_data[:,0], test_data[:,1],c='g',marker='x',linewidth=30)
    elif 'V' == Z_test[i]:
        plt.scatter(test_data[:,0], test_data[:,1],c='m',marker='x',linewidth=30)
        


for char in image_data['powerbox']:
    #visually differentiate the characters by a colour and marker
    if 'S' in char:
        S_plot = plt.scatter(image_data['powersector'][char], image_data['powerbox'][char], c='r', marker='+',linewidth=30)
    elif 'T' in char:
        T_plot = plt.scatter(image_data['powersector'][char], image_data['powerbox'][char], c='g', marker='+', linewidth=30)
    elif 'V' in char:
        V_plot = plt.scatter(image_data['powersector'][char], image_data['powerbox'][char], c='m', marker='+', linewidth=30)

plt.legend((S_plot, T_plot, V_plot),
           ('S', 'T', 'V'),
           scatterpoints=1,
           loc='lower right',
           ncol=3,
           fontsize=30)


for i in range(len(grid)):
    if 'S' == Z[i]:
        plt.scatter(grid[i][0], grid[i][1], c='r', marker='x',linewidth=10)
    elif 'T' == Z[i]:
        plt.scatter(grid[i][0], grid[i][1], c='g', marker='x',linewidth=10)
    elif 'V' == Z[i]:
        plt.scatter(grid[i][0], grid[i][1], c='m', marker='x',linewidth=10)


plt.show()

print(Z_test)
print(test_data)

"
"num_samples = 10
pi = estimate_pi(num_samples)
pi"
"num_samples = 100000000
pi = estimate_pi(num_samples)
pi"
"sigma = 100
samples, acceptance = dumb_metropolis(init, testing, 1000, sigma)
plt.plot(samples)
plt.show()
acceptance = acceptance/1000
print(str(acceptance*100) + ""% accepts"")"
"sigma = 1
samples, acceptance = dumb_metropolis(init,testing,10000, sigma)
print(samples.shape)
plt.plot(samples)
plt.show()
acceptance = acceptance/10000
print(str(acceptance*100) + ""% accepts"")"
"sigma = 0.1
samples, acceptance = dumb_metropolis(init, testing, 10000, sigma)
plt.plot(samples)
plt.show()
acceptance = acceptance/10000
print(str(acceptance*100) + ""% accepts"")"
"import warnings
warnings.filterwarnings('ignore')
from urllib.request import urlopen
import re

url = [""http://nytimes.com"",""http://google.com"",""http://cnn.com"",""http://facebook.com"",""http://youtube.com""]
regex = b""<title>(.+?)</title>""
pattern = re.compile(regex)

i = 0

while i < len(url):
    html = urlopen(url[i])
    htmltext = html.read()
    title = re.findall(pattern, htmltext)
    print(title)
    i+=1
    "
"expand(exp(2*pi*I/3), complex=True)"
"expand(exp(4*pi*I/3), complex=True)"
"plt.figure(figsize=(4,4))
roots = np.array([[1,0], [-0.5, np.sqrt(3)/2], [-0.5, -np.sqrt(3)/2]])
plt.scatter(roots[:,0], roots[:,1], s=50, c='red')
xp = np.linspace(0, 2*np.pi, 100)
plt.plot(np.cos(xp), np.sin(xp), c='blue');"
"dom = BeautifulSoup(response.content, ""html.parser"")

blog_post_elements = dom.select('li.sh_blog_top')

for blog_post_element in blog_post_elements:
    title = blog_post_element.select('a.sh_blog_title')[0].attrs.get('title')
    blog_name = blog_post_element.select('a.txt84')[0].text
    url = blog_post_element.select('a.sh_blog_title')[0].attrs.get('href')
    print((title, blog_name, url))"
"# 여러 키워드에 대해서 한꺼번에 블로그 포스트 리스트를 불러와보겠습니다.

keywords = [
    ""신사동 원룸"",
    ""신사역 원룸"",
    ""가로수길 원룸"",
    ""가로수길 오피스텔"",
    ""신사역 부동산"",
]


for keyword in keywords:
    # 우리가 작성한 키워드는 "" "" ( 띄어쓰기 ) 로 되어 있지만,
    # 네이버에서 검색을 할 떄는 "" "" => ""+"" 로 변경되서 검색되어야 합니다.
    
    query = keyword.replace(' ', '+')
    
    response = requests.get(""https://search.naver.com/search.naver?query={query}"".format(query=query))
    assert response.status_code is 200


    dom = BeautifulSoup(response.content, ""html.parser"")

    blog_post_elements = dom.select('li.sh_blog_top')

    for blog_post_element in blog_post_elements:
        title = blog_post_element.select('a.sh_blog_title')[0].attrs.get('title')
        blog_name = blog_post_element.select('a.txt84')[0].text
        url = blog_post_element.select('a.sh_blog_title')[0].attrs.get('href')
        print((keyword, title, blog_name, url))
"
"for ranking_element in ranking_elements[:-1]:
    ranking_title_element = ranking_element.select(""a"")[0]
    print(ranking_title_element.attrs.get('title'))"
"phonenumber_list = []

for item in result.get('items'):
    phonenumber = item.get('item').get('original_user_phone')
    phonenumber_list.append(phonenumber)
    
phonenumber_list"
"for phonenumber in phonenumber_list:
    response = send_sms(
        send_phone=send_phonenumber,
        dest_phone=phonenumber,
        content='(광고)같은 광고. 안녕하세요. 다방입니다. 판매자 등록 수수료 첫달 50% 할인 이벤트 진행중입니다.',
    )
    print(response.content)"
"allCats = []

allCats.append({'name' : 'Zophie', 'age': 7, 'color':'gray'})
allCats.append({'name' : 'Fooka', 'age': 5, 'color':'black'})
allCats.append({'name' : 'Fat-tail', 'age': 5, 'color':'gray'})
allCats.append({'name' : '???', 'age': -1, 'color':'orange'})

print(allCats)"
"import requests # get the Requests library

link = ""https://automatetheboringstuff.com/files/rj.txt"" # store the link where the file is at
rj = requests.get(link) # use Requests to get the link

print(rj.text) # Print out the text from Requests, and check that its stored"
print(len(rj.text))
"dotStar = re.compile(r'.*', re.DOTALL)
print(dotStar.findall(primeDirectives))"
"allDigitsRegex = re.compile(r'^\d+$') # Must start and end with a digit, with at least 1 or more digits inbetween

print(allDigitsRegex.findall('2153234623462561514')) # Matches entire string
print(allDigitsRegex.findall('21532346234letters!62561514')) # No match, doesn't end with string"
"import warnings
warnings.filterwarnings('ignore')
'hello ' + 'world!'"
"import warnings
warnings.filterwarnings('ignore')
import sys
print('Hello world')
print(sys.argv)"
"conts = sorted(set([objfun(2,y) for y in np.arange(-2,5,0.25)]))
contourplot(objfun, -3,3, -2, 5, ncontours=conts,fill=False)
plt.xlabel(""x"")
plt.ylabel(""y"")
plt.title(""Contours of $f(x,y)=100(y-x^2)^2 + (1-x)^2$"");"
"plt.figure(figsize=(17,5))
plt.subplot(1,2,1)
contourplot(objfun, -2,3,0,10)
plt.xlabel(""x"")
plt.ylabel(""y"")
plt.title(""Minimize $f(x,y)=10(y-x^2)^2 + (1-x)^2$"");
plt.scatter(p[0,0],p[0,1],marker=""*"",color=""w"")
for i in range(1,len(p)):    
        plt.plot( (p[i-1,0],p[i,0]), (p[i-1,1],p[i,1]) , ""w"");

plt.subplot(1,2,2)
plt.plot(f)
plt.xlabel(""iterations"")
plt.ylabel(""function value"");"
"plt.figure(figsize=(17,5))
plt.subplot(1,2,1)
contourplot(objfun, -1,3,0,10)
plt.xlabel(""x"")
plt.ylabel(""y"")
plt.title(""Minimize $f(x,y)=10(y-x^2)^2 + (1-x)^2$"");
plt.scatter(p[0,0],p[0,1],marker=""*"",color=""w"")
for i in range(1,len(p)):    
        plt.plot( (p[i-1,0],p[i,0]), (p[i-1,1],p[i,1]) , ""w"");

plt.subplot(1,2,2)
plt.plot(f)
plt.xlabel(""iterations"")
plt.ylabel(""function value"");"
"p, f = coordinatedescent(objfun, gradient, init=[2,6], steplength=0.01)"
f
"plt.figure(figsize=(17,5))
plt.subplot(1,2,1)
contourplot(objfun, -3,3,0,10)
plt.xlabel(""x"")
plt.ylabel(""y"")
plt.title(""Minimize $f(x,y)=10(y-x^2)^2 + (1-x)^2$"");
plt.scatter(p[0,0],p[0,1],marker=""*"",color=""w"")
for i in range(1,len(p)):    
        plt.plot( (p[i-1,0],p[i,0]), (p[i-1,1],p[i,1]) , ""w"");

plt.subplot(1,2,2)
plt.plot(f)
plt.xlabel(""iterations"")
plt.ylabel(""function value"");"
"plt.figure(figsize=(17,5))
plt.subplot(1,2,1)
contourplot(objfun, -2,3,0,10)
plt.xlabel(""x"")
plt.ylabel(""y"")
plt.title(""Minimize $f(x,y)=10(y-x^2)^2 + (1-x)^2$"");
plt.scatter(p[0,0],p[0,1],marker=""*"",color=""w"")
for i in range(1,len(p)):    
        plt.plot( (p[i-1,0],p[i,0]), (p[i-1,1],p[i,1]) , ""w"");

plt.subplot(1,2,2)
plt.plot(f)
plt.xlabel(""iterations"")
plt.ylabel(""function value"");"
"from mpl_toolkits.mplot3d import axes3d
fig = plt.figure()
ax = fig.gca(projection='3d')
ax.scatter(x,x**2,y)"
"from sklearn import linear_model
reg = linear_model.LinearRegression()
X = np.vstack((x,x**2)).transpose()
reg.fit(X,y)"
"reg.intercept_, reg.coef_[0], reg.coef_[1]"
"fig = plt.figure()
 
ax = fig.gca(projection='3d')

Xf = np.arange(-1.1, 2.2, 0.25)
Yf = np.arange(-0.2, 4.25, 0.25)
Xf, Yf = np.meshgrid(Xf, Yf)
Zf = reg.intercept_ + reg.coef_[0]*Xf + reg.coef_[1]*Yf
surf = ax.plot_surface(Xf, Yf, Zf, alpha=0.2)

for i in range(len(X)):
    ax.plot([X[i,0], X[i,0]], [X[i,1],X[i,1]], [yf[i], y[i]], linewidth=2, color='r', alpha=.5)
ax.plot(X[:,0], X[:,1], y, 'o', markersize=8, 
        markerfacecolor='none', color='r')"
"fig = plt.figure()
ax = fig.gca(projection='3d')
Xf = np.arange(-1.1, 2.2, 0.25)
Yf = np.arange(-0.2, 4.25, 0.25)
Xf, Yf = np.meshgrid(Xf, Yf)
Zf = reg.intercept_ + reg.coef_[0]*Xf + reg.coef_[1]*Yf
surf = ax.plot_surface(Xf, Yf, Zf, alpha=0.2)
ax.scatter(x,x**2,y,c=""r"")"
"MU_TRUE = 1.0

def fake_simulator (mu=MU_TRUE):
    """"""
    Pretends to simulate some process that produces
    a single output value.
    """"""
    return np.random.exponential (mu)

VAR_TRUE = MU_TRUE * MU_TRUE

# Demo
fake_simulator ()"
"def do_experiments (simulator, num_experiments):
    """"""
    This function repeatedly calls a simulator and records the outputs.
    The simulator must be a function, `simulator()`, that returns a
    single floating-point output value. This function will call the
    simulator `num_experiments` times and return all outputs.
    """"""
    assert hasattr(simulator, '__call__') # `simulator` must be a function
    Y = np.zeros (num_experiments)
    
    # @YOUSE: Run simulator and record outputs in Y[:]
    for e in range (num_experiments):
        Y[e] = simulator ()
        
    return Y

# Demo
n_e = 10000
Y = do_experiments (fake_simulator, n_e)
print (""n_e ="", n_e, ""==>"", np.mean (Y))"
"def repeat_experiments (simulator, num_experiments, num_batches):
    """"""
    This function repeats a batch of simulation experiments many times,
    return the means of each batch.
    
    It uses `do_experiments()` to run one batch of experiments, and
    repeats batch runs `num_batches` times.
    """"""
    Y_bar = np.zeros (num_batches) # Stores the means of each batch
    
    # @YOUSE: Run batches and record means
    for b in range (num_batches):
        Y_bar[b] = np.mean (do_experiments (simulator, num_experiments))
        
    return Y_bar

# Demo
n_b = 10 # Number of batches
for n_e in [10, 100, 1000]:
    print (n_e, ""=>"", repeat_experiments (fake_simulator, n_e, n_b))"
"# Another demo, which plots the means of all batches for varying
# numbers of experimental trials per batch.

fig = plt.figure (figsize=(16, 6))
ax = fig.add_subplot (111)

n_b = 100 # Number of batches
for n_e in [10, 100, 1000]:
    x = np.arange (n_b)
    y = repeat_experiments (fake_simulator, n_e, n_b)
    ax.plot (x, y, label=str (n_e))
    
ax.legend ()"
"def viz_exp (num_experiments=100, num_repetitions=100):
    """"""
    Runs many batches of ""fake"" experiments. Plots a
    histogram and adds a best-fit Gaussian to the plot.
    """"""
    Y_bar = repeat_experiments (fake_simulator,
                                num_experiments,
                                num_repetitions)
    
    fig = plt.figure (figsize=(12, 6))
    ax = fig.add_subplot (111)
    H, Bins, _ = ax.hist (Y_bar, normed=1)
    plt.xlim ([0.5, 1.5])
    plt.ylim ([0, 20])
    
    # Add best-fit Gaussian
    X_fit = np.linspace (0.5, 1.5)
    Y_fit = mlab.normpdf (X_fit, MU_TRUE, VAR_TRUE/sqrt (num_experiments))
    plt.plot (X_fit, Y_fit, 'r--', linewidth=4)
    
# Demo
x = interact (viz_exp
              , num_experiments=(100, 2000, 100)
              , num_repetitions=(10, 100, 10)
             )"
"n_e = 10
Y = do_experiments (fake_simulator, n_e)
y_bar = np.mean (Y)
s_n = np.std (Y)

# @YOUSE: Compute a 1-alpha confidence interval, y +/- dy
alpha = 0.05
dy = t.ppf (1-alpha/2, n_e-1) * s_n / (n_e-1)**(0.5)

# Test code
if MU_TRUE < (y_bar-dy) or MU_TRUE > (y_bar + dy):
    err_flag = '**'
else:
    err_flag = ''
print (n_e, ""=>"", y_bar, ""+/-"", dy, err_flag)"
"n_e = 500
for i in range (20):
    Y = do_experiments (fake_simulator, n_e)
    y_bar = np.mean (Y)
    s_n = np.std (Y)

    # @YOUSE: Compute a 1-alpha confidence interval
    alpha = 0.05
    dy = t.ppf (1.0 - 0.5*alpha, n_e-1) * s_n / np.sqrt (n_e-1)
    
    if MU_TRUE < (y_bar-dy) or MU_TRUE > (y_bar + dy):
        err_flag = '**'
    else:
        err_flag = ''

    print (i, "":"", y_bar, ""+/-"", dy, err_flag)"
"def show_grid (grid):
    plt.matshow (grid)
    
show_grid (peeps)"
"show_grid (peeps[1:-1, 1:-1])"
"new_peeps = np.copy (peeps)

# @YOUSE: Move as many unhappy campers as possible.
# Store your result in `new_peeps`.

swap_grid_random (new_peeps, locs_unhappy_A, locs_appeal_to_A)
swap_grid_random (new_peeps, locs_unhappy_B, locs_appeal_to_B)

# Sanity check: Make sure population counts have not changed!
print (""Tribe A: before ="", np.count_nonzero (peeps == TRIBE_A),
       ""and after ="", np.count_nonzero (new_peeps == TRIBE_A))
print (""Tribe B: before ="", np.count_nonzero (peeps == TRIBE_B),
       ""and after ="", np.count_nonzero (new_peeps == TRIBE_B))

show_grid (peeps)
show_grid (new_peeps)"
"# Initial conditions
i_center = int (X.shape[0]/2)
X[i_center, 0] = 1
show_grid (X.transpose ())"
"# Some test code:
def irun_ca (rule_num=90, n=100, t_max=100):
    show_grid (run_ca (rule_num, n, t_max).transpose ())
    
irun_ca (90) # Try 90, 169, and 37"
"interact (irun_ca
          , rule_num=(0, 256, 1)
          , n=(10, 100, 10)
          , t_max=(10, 100, 10))"
"# further populate tables created from nested element
json_normalize(data, 'counties', ['state', 'shortname', ['info', 'governor']])"
"# load json as string
json.load((open('data/world_bank_projects_less.json')))"
"# load as Pandas dataframe
sample_json_df = pd.read_json('data/world_bank_projects_less.json')
sample_json_df"
"# -- 1. Find the 10 countries with most projects
df = json_normalize(edata)
top10Countries = df.countryname.value_counts().head(10)
top10Countries
"
"edata = json.load((open('data/world_bank_projects.json')))
json_normalize(edata)"
"# Instantiate a graph, add vetices, initialize PageRank scores
g = Graph()
[g.add_vertex(v) for v in 'sopqrt']
g.init_pr()

# Add edges
edge_list_0 = [g.add_edge(edge[0], edge[1])
               for edge in ['so', 'sp', 'op', 'pr', 'pr', 'oq', 'qr', 'qt', 'rt', 'rs', 'tr']]
#Perform PageRank
print (""Perform PageRank for the 1st time"")
g.pagerank()

print (""\nPerform PageRank after adding edges"")

# Add some more edges
edge_list_1 = [g.add_edge(edge[0], edge[1])
               for edge in ['po', 'ro', 'so', 'to']]
# Incrementally Update PageRank Score
g.pagerank()

# Delete some edges from the graph
for i in range(0, len(edge_list_1), 2):
    g.delete_edge(edge_list_0[i])
    
print (""\nPerform PageRank after deleting edges"")

# Incrementally Update PageRank Score
g.pagerank()

"
"h = Graph()
[h.add_vertex(v) for v in 'abcde']
h.add_edge('a', 'e')
h.add_edge('b', 'e')
h.add_edge('b', 'a')
h.add_edge('d', 'b')
h.add_edge('d', 'e')
h.add_edge('c', 'd')
h.add_edge('e', 'c')

h.pagerank()"
"h = Graph()
[h.add_vertex(v) for v in 'sabt']
h.add_edge('s', 'a', 20)
h.add_edge('s', 'b', 10)
h.add_edge('a', 'b', 20)
h.add_edge('a', 't', 10)
h.add_edge('b', 't', 20)

h.reset_flow()

print (""\nNetwork before running the Ford-Fulkerson Algorithm"")
for k, v in h.vset.items():
    print (k, "":"", v)

print (""\nMax flow found from %s to %s is %s"" % ('s', 't', h.max_flow('s', 't')))
    
print (""\nNetwork after running the Ford-Fulkerson Algorithm"")
for k, v in h.vset.items():
    print (k, "":"", v)"
"g = Graph()

for v in 'sopqrt':
    g.add_vertex(v) 
g.add_edge('s', 'o', 3)
g.add_edge('s', 'p', 3)
g.add_edge('o', 'p', 2)
g.add_edge('p', 'r', 2)
g.add_edge('o', 'q', 3)
g.add_edge('q', 'r', 4)
g.add_edge('q', 't', 2)
g.add_edge('r', 't', 3)

g.reset_flow()

print (""\nNetwork before running the Ford-Fulkerson Algorithm"")
for k, v in g.vset.items():
    print (k, "":"", v)

print (""\nMax flow found from %s to %s is %s"" % ('s', 't', g.max_flow('s', 't')))
    
print (""\nNetwork after running the Ford-Fulkerson Algorithm"")
for k, v in g.vset.items():
    print (k, "":"", v)
"
"import warnings
warnings.filterwarnings('ignore')
%matplotlib inline
NUM_CLUSTERS=2
WHICH_DATASET = 'IRIS' # 'X' or 'IRIS'

from sklearn import cluster, datasets
import numpy as np
import matplotlib.pyplot as plt

# load data
X_iris = datasets.load_iris().data if WHICH_DATASET=='IRIS' else \
               np.genfromtxt('xdata.csv', delimiter=',', names=['x', 'y'])
X_iris = np.array([list(x) for x in X_iris]) #numpy likes lists

# do the clustering
k_means = cluster.KMeans(n_clusters=NUM_CLUSTERS)
k_means.fit(X_iris)
labels = k_means.labels_

# plot raw data, without clusters
# plot the clusters in color
fig = plt.figure(1, figsize=(8, 8))

plt.scatter(X_iris[:, 0], X_iris[:, 1], c=labels.astype(np.float))

plt.show()"
"from sklearn.datasets import make_blobs

X, y = make_blobs(n_samples=300, centers=4,
                  random_state=0, cluster_std=1.0)
plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='rainbow');"
"# We have some convenience functions in the repository that help 
from fig_code import visualize_tree, plot_tree_interactive

# Now using IPython's ``interact`` (available in IPython 2.0+, and requires a live kernel) we can view the decision tree splits:
plot_tree_interactive(X, y);"
"def fit_randomized_tree(random_state=0):
    X, y = make_blobs(n_samples=300, centers=4,
                      random_state=0, cluster_std=2.0)
    clf = DecisionTreeClassifier(max_depth=15)
    
    rng = np.random.RandomState(random_state)
    i = np.arange(len(y))
    rng.shuffle(i)
    visualize_tree(clf, X[i[:250]], y[i[:250]], boundaries=False,
                   xlim=(X[:, 0].min(), X[:, 0].max()),
                   ylim=(X[:, 1].min(), X[:, 1].max()))
    
from IPython.html.widgets import interact
interact(fit_randomized_tree, random_state=[0, 100]);"
"from sklearn.ensemble import RandomForestClassifier
clf = RandomForestClassifier(n_estimators=100, random_state=0, n_jobs=-1)
visualize_tree(clf, X, y, boundaries=False);"
"# plot raw data, without clusters
fig = plt.figure(1, figsize=(8, 8))
plt.clf()
ax = Axes3D(fig, rect=[0, 0, 1, 1], elev=8, azim=200)
plt.cla()

ax.scatter(X_iris[:, 3], X_iris[:, 0], X_iris[:, 2])

ax.w_xaxis.set_ticklabels([])
ax.w_yaxis.set_ticklabels([])
ax.w_zaxis.set_ticklabels([])
ax.set_xlabel('Petal width')
ax.set_ylabel('Sepal length')
ax.set_zlabel('Petal length')

plt.show()"
"# plot the clusters in color
fig = plt.figure(1, figsize=(8, 8))
plt.clf()
ax = Axes3D(fig, rect=[0, 0, 1, 1], elev=8, azim=200)
plt.cla()

ax.scatter(X_iris[:, 3], X_iris[:, 0], X_iris[:, 2], c=labels.astype(np.float))

ax.w_xaxis.set_ticklabels([])
ax.w_yaxis.set_ticklabels([])
ax.w_zaxis.set_ticklabels([])
ax.set_xlabel('Petal width')
ax.set_ylabel('Sepal length')
ax.set_zlabel('Petal length')

plt.show()"
"Y = y_lindrag(X, initial_v(100, 30), b1=1)"
"plt.plot(X, Y)
plt.xlabel(""$x$ (m)"")
plt.ylabel(""$y$ (m)"")
plt.hlines([0], X[0], X[-1], colors=""k"", linestyles=""--"");"
"plt.plot(X, Y, lw=2, label=""analytical"")
plt.plot(r[:, 1], r[:, 2], '--', label=""RK4"")
plt.legend(loc=""best"")
plt.xlabel(""$x$ (m)""); plt.ylabel(""$y$ (m)"")
plt.hlines([0], X[0], X[-1], colors=""k"", linestyles=""--"");"
"plt.plot(r[:, 1], residual)
plt.xlabel(""$x$ (m)""); plt.ylabel(""residual $r$ (m)"");"
"b1 = 1.
m = 0.5
b = b1/m
bisection(f, 0.1, v[0]/b - 1e-12, eps=1e-6)"
v[0]/b
"xvals = np.linspace(0, 7, 30)
plt.plot(xvals, np.zeros_like(xvals), 'k--')
plt.plot(xvals, g(xvals))"
"x0 = newton_raphson(g, 2)
print(x0)"
g(x0)
"newton_raphson(g, 3)"
"newton_raphson(g, 15)"
"x = {}

x0, xvals = newton_raphson_with_history(g, 1.5)
x[1.5] = xvals
print(""root x0 = {} after {} iterations"".format(x0, len(xvals)))

x0, xvals = newton_raphson_with_history(g, 5)
x[5] = xvals
print(""root x0 = {} after {} iterations"".format(x0, len(xvals)))

x0, xvals = newton_raphson_with_history(g, 10)
x[10] = xvals
print(""root x0 = {} after {} iterations"".format(x0, len(xvals)))"
"for xstart in sorted(x.keys()):
    plt.semilogx(x[xstart], label=str(xstart))
plt.legend(loc=""best"")
plt.xlabel(""iteration"")
plt.ylabel(""current guess for root $x_0$"");"
"b1 = 1.
m = 0.5
b = b1/m
v0 = 100
u = []
for theta in np.arange(1, 90):
    v = initial_v(v0, theta)
    def f(x):
        return y_lindrag(x, v, b1=b1)
    R = bisection(f, 0.1, v[0]/b - 1e-16, eps=1e-5)
    if R is not None:
        u.append((theta, R))
u = np.array(u)"
"plt.plot(u[:, 0], u[:, 1])
plt.xlabel(r""launch angle $\theta$ ($^\circ$)"")
plt.ylabel(r""range $R$ (m)"");"
"for v0 in (10, 25, 50, 75, 100):
    u = find_range(v0)
    plt.plot(u[:, 0], u[:, 1], label=""{} m/s"".format(v0))
plt.legend(loc=""best"")
plt.xlabel(r""$\theta$ (degrees)"")
plt.ylabel(r""range $R$ (m)"");"
"velocities = np.linspace(5, 100, 100)
results = []  # (v0, theta_opt)
for v0 in velocities:
    u = find_range(v0)
    thetas, ranges = u.transpose()
    # find index for the largest range and pull corresponding theta
    theta_opt = thetas[np.argmax(ranges)]
    results.append((v0, theta_opt))
results = np.array(results)

plt.plot(results[:, 0], results[:, 1]) 
plt.xlabel(r""velocity $v_0$ (m/s)"")
plt.ylabel(r""$\theta_\mathrm{best}$ (degrees)"");"
"import warnings
warnings.filterwarnings('ignore')
# Code source: Gaël Varoquaux
# Modified for documentation by Jaques Grobler
# License: BSD 3 clause

import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
from sklearn import datasets
from sklearn.decomposition import PCA
%matplotlib inline

# import some data to play with
iris = datasets.load_iris()
X = iris.data[:, :2]  # we only take the first two features.
Y = iris.target

x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5
y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5

plt.figure(2, figsize=(8, 6))
plt.clf()

# Plot the training points
plt.scatter(X[:, 0], X[:, 1], c=Y, cmap=plt.cm.Paired)
plt.xlabel('Sepal length')
plt.ylabel('Sepal width')

plt.xlim(x_min, x_max)
plt.ylim(y_min, y_max)
plt.xticks(())
plt.yticks(())
plt.show()"
"# To getter a better understanding of interaction of the dimensions
# plot the first three PCA dimensions
fig = plt.figure(1, figsize=(8, 6))
ax = Axes3D(fig, elev=-150, azim=110)
X_reduced = PCA(n_components=3).fit_transform(iris.data)
ax.scatter(X_reduced[:, 0], X_reduced[:, 1], X_reduced[:, 2], c=Y,
           cmap=plt.cm.Paired)
ax.set_title(""First three PCA directions"")
ax.set_xlabel(""1st eigenvector"")
ax.w_xaxis.set_ticklabels([])
ax.set_ylabel(""2nd eigenvector"")
ax.w_yaxis.set_ticklabels([])
ax.set_zlabel(""3rd eigenvector"")
ax.w_zaxis.set_ticklabels([])
plt.show()"
data.head()
data['index'].count()
data.groupby('gbifapi_usageKey').first().reset_index()['index'].count()
"data[data['nameMatchValidation'].str.contains('ok', na=False)].groupby('gbifapi_usageKey').first().reset_index()['index'].count()"
"def count_residues(protein_seq):
    counts_dict = {}   # create an empty dictionary to store results
    for aa in protein_seq:
        # if amino acid has already been added to the dictionary, increase its count by 1
        if aa in counts_dict:
            counts_dict[aa] += 1
        # if amino acid found for the first time, add it with count 1
        else:
            counts_dict[aa] = 1
    return counts_dict
    
protein_sequence = 'DQHTWMYAEGYLNHVYRCDKQRAEDKECNGLYAWALALESHGKGSYYCQGFKTFPNPWPMHMMTFVMADLYQYMEI'
aa_counts_dict = count_residues(protein_sequence)
# print results
for aa in aa_counts_dict:
    print(aa,'-',aa_counts_dict[aa])
"
"def residues_type_frequencies(protein_seq):
    hydrophobic = ['A','V','I','L','M','F','Y','W']
    pos_charged = ['R','H','K']
    neg_charged = ['D','E']
    polar = ['S','T','N','Q']
    other = ['C','U','G','P']
    
    prot_length = len(protein_seq)
    aa_types_dict = {'hydrophobic': 0, 'positive': 0, 'negative': 0, 'polar': 0, 'other': 0}
    # first, count amino acids in each category
    for aa in protein_seq:
        if aa in hydrophobic:
            aa_types_dict['hydrophobic'] += 1
        elif aa in pos_charged:
            aa_types_dict['positive'] += 1
        elif aa in neg_charged:
            aa_types_dict['negative'] += 1
        elif aa in polar:
            aa_types_dict['polar'] += 1
        elif aa in other:
            aa_types_dict['other'] += 1
    # now, convert to frequencies
    for aa_type in aa_types_dict:
        aa_types_dict[aa_type] = aa_types_dict[aa_type]/prot_length
    return aa_types_dict
    
    
protein_sequence = 'DQHTWMYAEGYLNHVYRCDKQRAEDKECNGLYAWALALESHGKGSYYCQGFKTFPNPWPMHMMTFVMADLYQYMEI'
aa_types_freq_dict = residues_type_frequencies(protein_sequence)
# print results
for aa_type in aa_types_freq_dict:
    print(aa_type,'-',aa_types_freq_dict[aa_type])"
"df = pd.read_csv('https://raw.githubusercontent.com/Py4Life/ebola/master/country_timeseries.csv')
df.head(10)"
"ax = df.plot('Date', 'Cases_Liberia')
df.plot('Date', 'Cases_Guinea', ax=ax)
df.plot('Date', 'Cases_SierraLeone', ax=ax)
ax.set_ylabel('# Cases')
ax.invert_xaxis()
ax.legend(['Liberia',""Guinea"",""Sierra Leone""], loc=""upper left"")
ax.grid(False)
sns.despine()"
"colors = sns.color_palette()
plt.scatter(df.Day, df.Cases_Liberia, color=colors[0], label='Liberia')
plt.scatter(df.Day, df.Cases_Guinea, color=colors[1], label='Guinea')
plt.scatter(df.Day, df.Cases_SierraLeone, color=colors[2], label='Sierra Leone')
plt.legend(loc='upper left')
plt.xlabel('Day')
plt.ylabel('# Cases')
sns.despine()"
"for col in ('Cases_Liberia', 'Cases_Guinea', 'Cases_SierraLeone', 'Cases_Nigeria'):
    idx = np.isfinite(df[col])
    plt.plot(df.Day[idx], df[col][idx], label=col[6:])
plt.legend(loc='upper left')
plt.xlabel('Day')
plt.ylabel('# Cases')
plt.yscale('log')
sns.despine()"
"def fit_poly(df, col, deg=3):    
    t = df.Day[np.isfinite(df[col])]
    y = df[col][np.isfinite(df[col])]
    
    if 0 not in t:
        t[len(t)] = 0
        y[len(y)] = 0
    
    coefs = np.polyfit(t, y, deg=deg)
    p = np.poly1d(coefs)

    plt.scatter(t, y, alpha=0.5)
    t = np.linspace(0, df.Day.max(), 1000)
    plt.plot(t, p(t), label=col[6:])
    plt.xlabel('Day')
    plt.ylabel(col)
    sns.despine()
fit_poly(df, 'Cases_Liberia', 6)"
"for col in ('Cases_Liberia', 'Cases_Guinea', 'Cases_SierraLeone', 'Cases_Nigeria'):
    fit_poly(df, col, 5)
plt.legend(loc='upper left')
plt.xlabel('Day')
plt.ylabel('# Cases')
sns.despine()
plt.ylim(10,10000)"
"def simulation(N, p0):
    p = [p0]
    while 1 > p[-1] > 0:
        p.append(np.random.binomial(N,p[-1]) / N)

    plt.plot(p)
    plt.xlabel('Generations')
    plt.ylabel('Frequency')
    sns.despine()
    return p

N = 1e4
p0 = 0.5
simulation(N, p0);"
"N = 1e4
p0 = 0.5
fixations = 0
simulations = 100

for _ in range(simulations):    
    p = simulation(N,p0)
    fixations += (p[-1]==1)    
    
print(""Fixation probability:"", fixations/simulations)"
"def simulation(N, p0, s):
    p = [p0]
    while 1 > p[-1] > 0:
        p.append(np.random.binomial(N, p[-1]/(1 - s + s * p[-1])) / N)

    plt.plot(p)
    plt.xlabel('Generations')
    plt.ylabel('Frequency')
    sns.despine()
    return p

N = 1e4
p0 = 0.5
s = 0.01
simulation(N, p0, s);"
"N = 1e4
p0 = 0.01
s = 0.01
fixations = 0
simulations = 100

for _ in range(simulations):    
    p = simulation(N, p0, s)
    fixations += (p[-1]==1)    
    
print(""Fixation probability:"", fixations/simulations)"
"nan09
i=345
for x in newnan09:
    nan09.ix[i,'purchases_Y15/M09']=x
    i+=1"
nan09
"g = sns.pairplot(iris, hue=""species"", diag_kind=""kde"")"
"g = sns.jointplot(data=iris, x=""sepal_length"", y=""sepal_width"")"
"from scipy.stats import spearmanr
g = sns.jointplot(data=iris, x=""sepal_length"", y=""sepal_width"",
                 kind=""kde"", stat_func=spearmanr)"
"sepal_lmplot = sns.lmplot(data=iris, fit_reg=False, x=""sepal_length"", y=""sepal_width"", 
           col=""species"", scatter_kws={'s': 25}, size=3.05).set_xticklabels(rotation=90)"
"petal_lmplot = sns.lmplot(data=iris, fit_reg=False, x=""petal_length"", y=""petal_width"", 
           col=""species"", scatter_kws={'s': 25}, size=3.05).set_xticklabels(rotation=90)"
"planets_factorplot = sns.factorplot(kind=""strip"", jitter=True, 
    data=planets[planets.method==""Radial Velocity""].sort_values(by=""year""),
    col=""method"", x=""year"", y=""orbital_period"", color=sns.xkcd_rgb[""warm blue""], 
    size=5, aspect=1.8)
_ = planets_factorplot.set_xticklabels(rotation=90).set(ylim=0, yscale=""log"")"
"planets_factorplot = sns.factorplot(kind=""box"",  
    data=planets[planets.method==""Radial Velocity""].sort_values(by=""year""),
    col=""method"", x=""year"", y=""orbital_period"", color=sns.xkcd_rgb[""green apple""], 
    size=5, aspect=1.8)
_ = planets_factorplot.set_xticklabels(rotation=90).set(ylim=0, yscale=""log"")"
"# Classic green and red colors for heatmap
cmap = sns.diverging_palette(133, 10, n=13, center=""dark"", as_cmap=True)

# Providing column colors. Awesome xkcd colors integration in seaborn
col_colors = [sns.xkcd_rgb[""amber""]]*4 + [sns.xkcd_rgb[""windows blue""]]*4

# Clustermap function
clustermap = sns.clustermap(tpm_filtered.sample(30), cmap=cmap, z_score=0,
                         figsize=(6,9), col_colors=col_colors)

# Just to set x axis ticks rotation to 0, by default it is 90
ax = clustermap.ax_heatmap
labels = ax.get_yticklabels()
_ = ax.set_yticklabels(labels, rotation=0)"
"sns.palplot(sns.color_palette(""Set1"", n_colors=8, desat=.5))"
"flatui = [""#9b59b6"", ""#3498db"", ""#95a5a6"", ""#e74c3c"", ""#34495e"", ""#2ecc71""]
sns.palplot(sns.color_palette(flatui))"
"step1 = Discrete_Fourier_Transform(6)
"
"step1 = np.matrix([m+0.j, 0+0.j, 0+0.j, m+0.j, 0+0.j, 0+0.j])
step2 = Discrete_Fourier_Transform(6)
print("""")
print("""")
print("""")
print("""")
step3 = step1*step2
print(step3)
"
"step1 = determine_q(15)
step2 = PeriodFinding(step1,15)
plot_prob(1500,0.15,2**step1,step1)"
"Number_factoring = 36
num_qubits = 2**8

step1 = find_x(Number_factoring,1)
step2 = PeriodFinding(step1,Number_factoring)
step3 = plot_prob(500,0.1,num_qubits,step2)"
deltatime[deltatime<60].hist(bins=60)
"ax =deltacomplete[deltacomplete<2000].hist(bins=20)
ax.set_xlabel('time to completion (s)')
ax.set_ylabel('#')
"
"C = Counter()
C.update(complete[k[9]])
print(k[9])
C"
"for x in set(complete[k[10]]):
    print(x)"
data.head()
"ax = data.realEnergy2.plot()
ax.set_ylabel('consum [kWh]')"
"ax = data.realEnergy1.resample('6H', how='sum').plot(kind='bar')
ax.set_ylabel('consum [kWh]')
"
"allPowers = data[['realPower1', 'realPower2', 'realPower3']]
ax = allPowers['2016-1-03 03:00':'2016-1-03 06:00'].plot()
ax.set_ylabel('potencia [W]')"
"import numpy
print(numpy.random.randint(low=0, high=1000000, size=10))"
"# calculating Eigen vector
egv=la.eigh(v)
egval=egv[0]
# Detemining maximum Eigen value
egval_max=max(egval)
egval_max"
"knjige.reset_index()[:100].plot(y=['Depository','Amazon'], style=['rx','o'])"
"knjige\
    .reset_index()\
    .sort_values(by='Depository',ascending=True)[:270]\
    .plot(x='Depository',y='razlika',style='o')\
    .plot([0,22.33],[7.13,7.13],'r--')"
"knjige[(knjige['leto']>1970)]\
    .groupby('leto')[['razlika']]\
    .mean()\
    .plot()"
"colsum=ct.sum(axis=0)
colpct=ct/colsum
print(colpct)"
"cs= scipy.stats.chi2_contingency(ct)
print (cs)"
"recode1 = {""high"": ""high"", ""low"": ""low""}
chi2ByCat(recode1)"
"recode1 = {""high"": ""high"", ""med"": ""med""}
chi2ByCat(recode1)"
"recode1 = {""low"": ""low"", ""med"": ""med""}
chi2ByCat(recode1)"
"cs= scipy.stats.chi2_contingency(ct)
print (cs)"
"recode1 = {""high"": ""high"", ""low"": ""low""}
chi2ByCat(expensive,recode1)"
"recode1 = recode1 = {""high"": ""high"", ""med"": ""med""}
chi2ByCat(expensive,recode1)"
"recode1 = {""low"": ""low"", ""med"": ""med""}
chi2ByCat(expensive,recode1)"
"ct=pandas.crosstab(reasonable['recommend'],reasonable['safety'])
print (ct)
cs= scipy.stats.chi2_contingency(ct)
print (cs)
"
"recode1 = {""high"": ""high"", ""low"": ""low""}
chi2ByCat(reasonable,recode1)"
"recode1 = recode1 = {""high"": ""high"", ""med"": ""med""}
chi2ByCat(reasonable,recode1)"
"recode1 = {""low"": ""low"", ""med"": ""med""}
chi2ByCat(reasonable,recode1)"
"plt.plot([1,2,3,4])
plt.ylabel('some numbers')
plt.show()
"
"plt.figure(figsize=(12,4))
plt.subplot(121)

plt.contour(X,Y,Z);
plt.title(""Steepest Descent"");
step=-0.25
X0 = 10.0
Y0 = 1.0

Ngrad=Hinv.dot(DQuad(X0,Y0))

sgrad = step*DQuad(X0,Y0)
plt.quiver(X0,Y0,sgrad[0],sgrad[1],color='red',angles='xy',scale_units='xy',scale=1);
X1 = X0 + sgrad[0]
Y1 = Y0 + sgrad[1]
sgrad = step*DQuad(X1,Y1)
plt.quiver(X1,Y1,sgrad[0],sgrad[1],color='green',angles='xy',scale_units='xy',scale=1);
X2 = X1 + sgrad[0]
Y2 = Y1 + sgrad[1]
sgrad = step*DQuad(X2,Y2)
plt.quiver(X2,Y2,sgrad[0],sgrad[1],color='purple',angles='xy',scale_units='xy',scale=1);


plt.subplot(122)
plt.contour(X,Y,Z);
plt.title(""Newton's Method"")
plt.quiver(X0,Y0,Ngrad[0],Ngrad[1],color='purple',angles='xy',scale_units='xy',scale=1);

#Compute Hessian and plot again.
"
"import warnings
warnings.filterwarnings('ignore')
([x for x in dir(__builtin__)  if x.islower() and not x.startswith('__')])"
"# Initial condition set and display
u0 = numpy.ones(nx)
u0[int(.5/dx):int(1/dx)+1] = 2
plot(x,u0)"
"# Explicit forward time-step
u = numpy.copy(u0)
for it in range(nt):
    un = numpy.copy(u)
    u[1:] = un[1:] - c*dt/dx*(u[1:] - u[:-1])
    
# Display the transported solution
plot(x,u)"
"# Robot parameters
dflt_v = 7.5                 # Initial velocity of the ball, in m / s
dflt_aim = 0.5
dflt_pos = 0.25

robot_width = 0.0      # Robot width in meters. 1 foot = 0.3048 meters
robot_height = 0.5     # Shooter height in meters

# Plot the results
from IPython.html.widgets import interact, interactive, FloatSlider
from IPython.display import display
import matplotlib.pyplot as plt
%matplotlib inline

plt.rcParams['mathtext.fontset'] = 'stixsans'

def plot_trajectories(v, r_pos, aim_frac):
    r_pos *= field_length

    # Create the robot reference frame
    robo_frame = ReferenceFrame(r_pos, shooter_dx=robot_width, shooter_dy=robot_height)

    gx, gy = goal_in_robot_frame(robo_frame, aim=aim_frac)
    glx, gly = robo_frame.from_frame(gx, gy)                # Same thing, in the lab frame
    glx, (gly_min, gly_max) = goal_limits()

    # Calculate the optimal value for theta with and without drag
    croot = -1
    th_n = get_theta(v, gx, gy, sgn=croot)
    th_d = get_theta(v, gx, gy, vt=ball_vt, sgn=croot)

    # Calculate the trajectory of the ball
    get_traj = lambda *args, **kwargs: robo_frame.from_frame(*get_trajectory(*args, **kwargs))

    xtn, ytn = get_traj(v, th_n)
    xte, yte = get_traj(v, th_n, vt=ball_vt)   # Use the ideal angle in the wind resistance case
    xtd, ytd = get_traj(v, th_d, vt=ball_vt)   # The wind resistance case with the correct angle

    # Start plotting
    fig = plt.figure(figsize=(10, 6))
    ax = fig.gca()
    
    # Draw the goal that we're aiming for
    ax.scatter(glx, gly, c='k', s=30, alpha=0.5)
    ax.plot([glx, glx], [gly_min, gly_max], 'k-', lw=1.5)

    # Plot the trajectory
    to_deg = lambda x: x * 180 / np.pi
    
    n_line, = ax.plot(xtn, ytn, label='No resistance ($\\theta = {:0.3f}^\\circ$)'.format(to_deg(th_n)))
    e_line, = ax.plot(xte, yte, '--', label='Real trajectory ($\\theta = {:0.3f}^\\circ$)'.format(to_deg(th_n)))
    d_line, = ax.plot(xtd, ytd, label='Adjusted trajectory ($\\theta = {:0.3f}^\\circ$)'.format(to_deg(th_d)))
    
    # Adjust the plots now
    ax.set_xlim((0, glx * 1.05))
    ax.set_ylim((0, pit_ceiling))
    print((0, glx * 1.05))

    ax.legend(loc='best', fontsize=16)
    #plt.tight_layout()
    fig.canvas.draw()
    plt.show()


v_widget = FloatSlider(min=6.0, max=25, step=0.05, value=dflt_v)
pos_widget = FloatSlider(min=0, max=1, step=0.01, value=dflt_pos)
aim_widget = FloatSlider(min=0, max=1, step=0.01, value=dflt_aim)

w = interact(plot_trajectories, v=v_widget, r_pos=pos_widget, aim_frac=aim_widget)
"
"x = [2,4,6,8,10]
s = 0
for i in x:
    s += (1/i)
    
print(""The sum of the reciprocals of x is:"", s)"
"plt.hist(popn, bins=50, color='gray', alpha=0.75, histtype='stepfilled')
plt.xlabel(""X"")
plt.ylabel(""Frequency"")
pass"
"plt.hist(popn, normed=True, bins=50, label='population',
         color='gray', alpha=0.75, histtype='stepfilled')
plt.hist(sample, normed=True, label='sample',
         color='steelblue', alpha=0.75, histtype='stepfilled')
plt.xlabel(""X"")
plt.ylabel(""Density"")
plt.legend(loc=""best"")
pass"
"plt.hist(sample, color='steelblue', alpha=0.75, 
         histtype='stepfilled',label='sample')
plt.xlabel(""X"")
plt.ylabel(""Frequency"")
pass"
"plt.hist(smeans, normed=True, bins=30, label='simulated\ndistn of\n means',
         color='firebrick', alpha=0.75, histtype='stepfilled')
plt.xlabel(""mean(X)"")
plt.ylabel(""Frequency"")
pass"
"plt.hist(popn, normed=True, bins=50, label='population',
         color='gray', alpha=0.75, histtype='stepfilled')
plt.hist(sample, normed=True, label='sample',
         color='steelblue', alpha=0.75, histtype='stepfilled')
plt.hist(smeans, normed=True, bins=50, label='simulated\ndistn of\n means',
         color='firebrick', alpha=0.75, histtype='stepfilled')
plt.xlabel(""X"")
plt.ylabel(""Density"")
plt.legend(loc=""best"")
plt.ylim(0,0.06)  # comment out this line to remove truncation
pass"
"# make a pair of plots
ssmin, ssmax = min(ssizes), max(ssizes)
theoryss = np.linspace(ssmin, ssmax, 250)

fig, (ax1, ax2) = plt.subplots(1,2)  # 1 x 2 grid of plots
fig.set_size_inches(12,4)

# plot histograms of sampling distributions
for (ss,mean) in zip(ssizes, means):
    ax1.hist(mean, normed=True, histtype='stepfilled', alpha=0.75, label=""n = %d"" % ss)

ax1.set_xlabel(""X"")
ax1.set_ylabel(""Density"")
ax1.legend()
ax1.set_title(""Sampling Distributions of Mean\nFor Different Sample Sizes"")

# plot simulation SE of mean vs theory SE of mean
ax2.plot(ssizes, se, 'ko', label='simulation')
ax2.plot(theoryss, sigma/np.sqrt(theoryss), color='red', label=""theory"")
ax2.set_xlim(0, ssmax*1.1)
ax2.set_ylim(0, max(se)*1.1)
ax2.set_xlabel(""sample size ($n$)"")
ax2.set_ylabel(""SE of mean"")
ax2.legend()
ax2.set_title(""Standard Error of Mean\nTheoretical Expectation vs. Simulation"")

pass"
"N = 1000
samples50 = popn.rvs(size=(50, N)) # N samples of size 50
means50 = np.mean(samples50, axis=0) # sample means
std50 = np.std(samples50, axis=0, ddof=1) # sample std devs
se50 = std50/np.sqrt(50) # sample standard errors

frac_overlap_mu = []
zs = np.arange(1,3,step=0.05)
for z in zs:
    lowCI = means50 - z*se50
    highCI = means50 + z*se50 
    overlap_mu = np.logical_and(lowCI <= mu, highCI >= mu)
    frac = np.count_nonzero(overlap_mu)/N
    frac_overlap_mu.append(frac)
    
frac_overlap_mu = np.array(frac_overlap_mu)

plt.plot(zs, frac_overlap_mu * 100, 'k-', label=""simulation"")
plt.ylim(60, 104)
plt.xlim(1, 3)
plt.xlabel(""z in CI =  sample mean ± z × SE"")
plt.ylabel(u""% of CIs that include popn mean"")

# plot theoretical expectation
stdnorm = stats.norm(loc=0, scale=1)
plt.plot(zs, (1 - (2* stdnorm.sf(zs)))*100, 'r-', alpha=0.5, label=""theory"")
plt.legend(loc='lower right')

pass"
"ndraw = 100
x = means50[:ndraw]
y = range(0,ndraw)
plt.errorbar(x, y, xerr=1.96*se50[:ndraw], fmt='o')
plt.vlines(mu, 0, ndraw, linestyle='dashed', color='#D55E00', linewidth=3, zorder=5)
plt.ylim(-1,101)
plt.yticks([])
plt.title(""95% CI: mean ± 1.96×SE\nfor 100 samples of size 50"")
fig = plt.gcf()
fig.set_size_inches(4,8)"
"# draw a figure
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10,4))
clrs = sbn.color_palette(""Set1"", n_colors=k)

for i, sample in enumerate(samples):
    sbn.kdeplot(sample, color=clrs[i], ax=ax1)
ax1_ymax = ax1.get_ylim()[1]

for i, sample in enumerate(samples):
    ax2.vlines(np.mean(sample), 0, ax1_ymax, linestyle=""dashed"", color=clrs[i])
ax2.set_xlim(ax1.get_xlim())
ax2.set_ylim(ax1.get_ylim())    

ax1.set_title(""Group Sample Distributions"")
ax2.set_title(""Group Means"")

ax1.set_xlabel(""X"")
ax1.set_ylabel(""Density"")

ax2.set_xlabel(""mean(X)"")
ax2.set_ylabel(""Density"")
pass"
"# Between-group and within-group estimates of variance
sample_group_means = [np.mean(s) for s in samples]
sample_group_var = [np.var(s, ddof=1) for s in samples]

Vbtw = n * np.var(sample_group_means, ddof=1)
Vwin = np.mean(sample_group_var)
Fstat = Vbtw/Vwin

print(""Between group estimate of population variance:"", Vbtw)
print(""Within group estimate of population variance:"", Vwin)
print(""Fstat = Vbtw/Vwin = "", Fstat)"
"fig, ax = plt.subplots()
sbn.distplot(Fstats, ax=ax, label=""Simulation"",
             kde_kws=dict(alpha=0.5, linewidth=2))

# plot the theoretical F-distribution for
# corresponding degrees of freedom
df1 = k - 1
df2 = n*k - k
x = np.linspace(0,9,500)
Ftheory = stats.f.pdf(x, df1, df2)
plt.plot(x,Ftheory, linestyle='dashed', linewidth=2, label=""Theory"")

# axes, legends, title
ax.set_xlim(0, )
ax.set_xlabel(""F-statistic"")
ax.set_ylabel(""Density"")
ax.legend()
title = \
""""""Comparison of Simulated and Theoretical
F-distribution for F(df1={}, df2={})""""""
ax.set_title(title.format(df1, df2))

pass"
"# draw F distribution
x = np.linspace(0,9,500)
Ftheory = stats.f.pdf(x, df1, df2)
plt.plot(x, Ftheory, linestyle='solid', linewidth=2, label=""Theoretical\nExpectation"")

# draw vertical line at threshold
threshold = stats.f.ppf(0.95, df1, df2)
plt.vlines(threshold, 0, stats.f.pdf(threshold, df1, df2), linestyle='solid')

# shade area under curve to right of threshold
areax = np.linspace(threshold, 9, 250)
plt.fill_between(areax, stats.f.pdf(areax, df1, df2), color='gray', alpha=0.75)

# axes, legends, title
plt.xlim(0, )
plt.xlabel(""F-statistic"")
plt.ylabel(""Density"")
plt.legend()
title = \
r"""""" $\alpha$ = 0.05 threshold for 
F-distribution with df1 = {}, df2={}""""""
plt.title(title.format(df1, df2))

print(""The α =0.05 significance threshold is:"", threshold)

pass
"
"fig, ax = plt.subplots()
sbn.distplot(Fstats, ax=ax, label=""Simulated $H_A$"",
             kde_kws=dict(alpha=0.5, linewidth=2))

# plot the theoretical F-distribution for
# corresponding degrees of freedom
df1 = k - 1
df2 = n*k - k
x = np.linspace(0,9,500)
Ftheory = stats.f.pdf(x, df1, df2)
plt.plot(x,Ftheory, linestyle='dashed', linewidth=2, label=""Theory"")

ymin, ymax = ax.get_ylim()
# Draw threshold alpha = 0.05
ax.vlines(stats.f.ppf(0.95, df1, df2), 0, ymax, linestyle='dotted', 
          color='k', label=r""Threshold for $\alpha=0.05$"")

# axes, legends, title
ax.set_xlim(0, )
ax.set_ylim(0, ymax)
ax.set_xlabel(""F-statistic"")
ax.set_ylabel(""Density"")
ax.legend()
title = \
""""""Comparison of Theoretical F-distribution
and F-distribution when $H_A$ is true""""""
ax.set_title(title.format(df1, df2))

pass"
"# create a histogram of our sample from the standard normal
plt.hist(y,alpha=0.5,normed=True,  # note the used of normed=True to generate
         bins=25,label='sample')   # a density histogram

# plot the pdf for the standard normal
plt.plot(xs, fx, color='red', 
         linestyle='dashed', linewidth=2,
         label='population')

# note the use of the label arguments above to create
# labels for the legend; by default the legend() fxn
# tries to find the best placement of the legend so as
# not to minimally interfere with the plotted data
plt.legend()

plt.xlabel(""Random Variate"")
plt.ylabel(""Density"")
pass"
"# let's generate the pdf for the SAT example
# we covered in lecture, which was assumed to be
# N(mu=1500,sigma=300)
SAT = stats.norm(loc=1500, scale=300)
xsat = np.linspace(600,2400,500)
fsat = SAT.pdf(xsat)
plt.plot(xsat,fsat,'black')
plt.xlabel(""SAT scores"")
plt.ylabel(""Density"")
pass"
"# In the example from the slides, Pam had a score of 1800
# on her SATs.  We can use the scipy.stats.norm.cdf fxn to 
# calculate her percentile. cdf returns the cumulative probability to
# the left of the given point. Strictly speaking, to turn
# this into a percentile we need to multiply by hundred
pamcdf = SAT.cdf(1800)
print(""Pam's percentile is"", pamcdf * 100)"
"# Let's redraw the PDF, illustrating how to draw a line at
# Pam's score (1800) and shade the area to the left of it

# draw entire pdf first
# note xsat and fsat were defined a couple cells above
plt.plot(xsat, fsat, color='black')  

# plot vertical line at 1800 extending from 0 to pdf(1800)
plt.vlines(1800, 0, SAT.pdf(1800), linestyle='dashed', color='k')

# draw area under curve from 600 to 1800
xtoleft = np.linspace(600,1800,500)
ftoleft = SAT.pdf(xtoleft)
plt.fill_between(xtoleft, np.zeros_like(ftoleft), ftoleft, color='gray', alpha=0.75)

plt.xlabel(""SAT scores"")
plt.ylabel(""Density"")

pass"
"# Another example we looked at in class involved
# quality control at a ketchup factory.  The volume
# of ketchup in bottles was ~ N(36 oz,0.11 oz).  We first
# asked what the probability was that a sampled bottle
# had <= 35.8 oz of ketchup 
kmu, kstd = 36, 0.11
ketchup = stats.norm(loc=kmu, scale=kstd)
ketchup.cdf(35.8)"
"# then we asked what fraction of bottles had between
# 35.8 and 36.2 oz of ketchup. Here's one way to calculate
# this value.  Note the introduction of the scipy.stats.norm.sf 
# function (also called the ""survival function"" = 1 - cdf)
1 - ketchup.cdf(35.8) - ketchup.sf(36.2)"
"# let's illustrate this last example with one more figure.
# We'll make this figure slightly fancier by changes its aspect
# ratio, customizing the axes, etc.

# set figure size -- (length, width) in inches
fig = plt.figure(figsize=(10,4))  

xvol = np.linspace(kmu - 4*kstd, kmu + 4*kstd, 500)
fvol = ketchup.pdf(xvol)
plt.plot(xvol, fvol, color='black')

# shade area representing fraction of bottles that
# are expected to pass inspection (btw 35.8 and 36.2)
xpass = np.linspace(35.8,36.2,500)
fpass = ketchup.pdf(xpass)
plt.fill_between(xpass, np.zeros_like(fpass), fpass, color='gray', alpha=0.75)

plt.xlabel(""Volume (oz)"")
plt.ylabel(""Density"")

#---------------------------------------------------------
# Below here we're just customizing the look of the figure

# get rid of yticks
plt.yticks([])

# get current axes
ax = plt.gca() 

# draw xsticks only at the bottom
ax.xaxis.set_ticks_position('bottom')

# remove left, right, and top ""spines"" surrounding the plot
ax.spines[""left""].set_visible(False)
ax.spines[""right""].set_visible(False)
ax.spines[""top""].set_visible(False)


pass"
"# here's how we can use our two fxns defined above to quickly create 
# some nice figures with a minimum of code. We'll illustrate it by drawing
# a figure showing the fraction of bottles that are expected NOT to pass
# inspection (the inverse of the previous figure above)

mu, sigma = 36, 0.11

plt.figure(figsize=(10,4))
distn, ax = norm_plot(mu, sigma, color='k')
area_under_distn(distn, mu-4*sigma, 35.8, color='firebrick', alpha=0.5)
area_under_distn(distn, 36.2, mu+4*sigma, color='firebrick', alpha=0.5)

# tweak the plot with labels and additional text
plt.xlabel(""Volume (oz)"")

# draw some text on the plot. We use the markup language 
# LaTeX to draw a nicely formatted shorthand formula
plt.text(36.2, 2, ""$X \sim N(36,0.11)$"", fontsize=18)
pass"
births.head()
"premieAndSmoke = births.query('(premature == ""premie"") and (smoke == ""smoker"")')
premieAndSmoke"
"# apply an summary function w/respect to the grouping
termgroup.describe()"
type(termgroup)
"# get a specific group
premies = termgroup.get_group('premie')
type(premies),premies.shape"
term_and_smoke_group.groups.keys()
term_and_smoke_group.weight.describe()
"plt.scatter(births.mAge, births.fAge)
plt.xlabel(""Age of Mother"")
plt.ylabel(""Age of Father"")
plt.title(""Relationship between Age of Parents\nBased on 150 births from NC"")
pass"
"# create just the figure and axis objects
fig = plt.figure()
axes = fig.add_axes([0.1, 0.1, 0.8, 0.8]) # left, bottom, width, height"
"# create our figure and axis objects
fig = plt.figure()
axes = fig.add_axes([0.1, 0.1, 0.8, 0.8]) # left, bottom, width, height

# change/add features of axis
axes.scatter(births.mAge, births.fAge)
axes.set_xlabel(""Age of Mother"")
axes.set_ylabel(""Age of Father"")
axes.set_title(""Relationship between Age of Parents\nBased on 150 births from NC"")

pass"
"# create our figure and axis objects
fig = plt.figure(figsize=(6,6))

# note that we've made the main axis take up less of the total figure 
axes = fig.add_axes([0.1, 0.1, 0.5, 0.5]) # left, bottom, width, height

# change/add features of axis
axes.scatter(births.mAge, births.fAge)
axes.set_xlabel(""Age of Mother"")
axes.set_ylabel(""Age of Father"")
axes.set_xlim(10,50)
axes.set_ylim(10,50)

# add new axis on right of figure to draw histogram of father's age
# I figured these out the  coordinates and width by sketching and trial and error
right = fig.add_axes([0.7, 0.1, 0.15, 0.5])  
right.hist(births.fAge[births.fAge.notnull()], orientation=""horizontal"", normed=True)
right.set_xticks([])
right.set_ylim(10,50)  # for figure to be accurate, these limits must match main figure limits

# add new axis on top of figure to draw histogram of mother's age
above = fig.add_axes([0.1, 0.7, 0.5, 0.15])
above.hist(births.fAge[births.fAge.notnull()], orientation=""vertical"", normed=True)
above.set_yticks([])
above.set_xlim(10,50)

pass"
"sbn.distplot(births.weight, color='r')
pass"
"# Plot a histogram with a kernel density estimate (kde)
ax = sns.distplot(births.weight, color=""r"")
ax.set_ylabel(""Density"")
pass"
"# Plot a kde with a rug plot
ax = sns.distplot(births.weight, hist=False, rug=True, color=""r"")
ax.set_ylabel(""Density"")
pass"
"sns.kdeplot(births.weight, bw=0.1, label='bw = 0.1')
sns.kdeplot(births.weight, bw=0.25, label='bw = 0.25')
sns.kdeplot(births.weight, bw=0.5, label='bw = 0.5')
sns.rugplot(births.weight, color='black')
plt.xlabel(""Weight"")
plt.ylabel(""Density"")
pass"
"sns.set(style=""whitegrid"", palette=""pastel"", color_codes=True)
sns.violinplot(x=""smoke"", y=""weight"", hue=""premature"", data=births, split=True,
               inner='quartile',
               palette={""full term"": ""b"", ""premie"": ""y""})
plt.legend(loc='upper center')
pass"
"sbn.set_palette(""deep"")

bins = np.linspace(-3, 3, 10)
fig, axes = plt.subplots(1, 5, figsize=(20,5), sharex=True, sharey=True)

for i, sample in enumerate(samples):
    axes[i].hist(sample, bins=bins, histtype='stepfilled', 
                 linewidth=1.5, label='sample {}'.format(i+1))
    axes[i].legend()

axes[-1].hist(allobs, bins=bins, histtype='stepfilled', linewidth=2, label='all data')
axes[-1].legend()
axes[-1].set_title(""All data combined"", fontsize=20)

axes[0].set_ylabel(""Frequency"", fontsize=20)
axes[2].set_xlabel(""X"", fontsize=20)

fig.tight_layout()
pass"
"SSbtw = sum_squares_between(samples)
SSwin = sum_squares_within(samples)
SStot = sum_squares_total(samples)

print(""SS between:"", SSbtw)
print(""SS within:"", SSwin)
print(""SS total:"", SStot)"
"sbn.set_palette(""deep"")

bins = np.linspace(-3, 3, 10)
fig, axes = plt.subplots(1, 5, figsize=(20,5), sharex=True, sharey=True)

for i, sample in enumerate(samples):
    axes[i].hist(sample, bins=bins, histtype='stepfilled', 
                 linewidth=1.5, label='sample {}'.format(i+1))
    axes[i].legend()

axes[-1].hist(allobs, bins=bins, histtype='stepfilled', linewidth=2, label='all data')
axes[-1].legend()
axes[-1].set_title(""All data combined"", fontsize=20)

axes[0].set_ylabel(""Frequency"", fontsize=20)
axes[2].set_xlabel(""X"", fontsize=20)

fig.tight_layout()
pass"
"SSbtw = sum_squares_between(samples)
SSwin = sum_squares_within(samples)
SStot = sum_squares_total(samples)

print(""SS between:"", SSbtw)
print(""SS within:"", SSwin)
print(""SS total:"", SStot)"
"sbn.distplot(iris[""Sepal.Length""])
pass"
"sbn.violinplot(x=""Species"", y=""Sepal.Length"", data=iris)
pass"
"expected_stds = np.ones_like(ssizes)  # all expected standard deviations are 1

mean_stds = [np.mean(i) for i in sstds]

plt.plot(ssizes, expected_stds, linestyle='dashed', color='black',
        label=""Expected"")

plt.plot(ssizes, mean_stds, marker='o', color='red', 
         label=""Observed"")


plt.xlabel(""Sample Size"")
plt.ylabel(""Mean of Sampling Distribution of Std Dev"")
plt.xlim(0, max(ssizes)*1.1)
plt.ylim(0, sigma * 1.1)
plt.legend(loc='best')
pass"
"fig, plotmtx = plt.subplots(nrows=4, ncols=2)
fig.set_size_inches(12,20)

x = np.linspace(-10, 10, 200)
expected_pdf = stats.norm.pdf(x)

# plot first 8 distributions of z-scores
ct = 0
for row in plotmtx:
    for subplot in row:
        subplot.hist(zscores[ct], bins=np.arange(-10,10,0.5),
                     normed=True, color='gray', alpha=0.5, label=""Observed Z-scores"")
        subplot.plot(x, expected_pdf, color='firebrick',label=""Expected Z-scores"")
        subplot.legend(loc='best', fontsize=9)
        subplot.set_title(""Z-scores, n = {:d}"".format(ssizes[ct]))
        ct += 1"
"fig, (ax1, ax2, ax3) = plt.subplots(nrows=1, ncols=3)
fig.set_size_inches(15,4)

x = np.linspace(-10, 10, 200)
norm_pdf = stats.norm.pdf(x)

z2 = zscores[ssizes.index(2)]
z3 = zscores[ssizes.index(3)]
z10 = zscores[ssizes.index(10)]

ax1.hist(z2, bins=np.arange(-10,10,0.5), 
         normed=True, color='gray', alpha=0.5, label=""Observed"")
ax1.plot(x, norm_pdf, color='firebrick', label=""normal pdf"")
ax1.plot(x, stats.t.pdf(x, df=2), color='steelblue',
         linewidth=3, label=""$t_2$ pdf"")
ax1.legend(loc='best', fontsize=9)
ax1.set_title(""Z-scores, n = 2"")

ax2.hist(z3, bins=np.arange(-10,10,0.5), 
         normed=True, color='gray', alpha=0.5, label=""Observed"")
ax2.plot(x, norm_pdf, color='firebrick',label=""normal pdf"")
ax2.plot(x, stats.t.pdf(x, df=3), color='steelblue',
         linewidth=3, label=""$t_{3}$ pdf"")
ax2.legend(loc='best', fontsize=9)
ax2.set_title(""Z-scores, n = 3"")

ax3.hist(z10, bins=np.arange(-10,10,0.5), 
         normed=True, color='gray', alpha=0.5, label=""Observed"")
ax3.plot(x, norm_pdf, color='firebrick',label=""normal pdf"")
ax3.plot(x, stats.t.pdf(x, df=10), color='steelblue',
         linewidth=3, label=""$t_{10}$ pdf"")
ax3.legend(loc='best', fontsize=9)
ax3.set_title(""Z-scores, n = 10"")


pass"
"fig = plt.figure()
fig.set_size_inches(10,4)

df = [2, 4, 30]
x = np.linspace(-6, 6, 200)

for i in df:
    plt.plot(x, stats.t.pdf(x, df=i), alpha=0.5, label='t({:d})'.format(i))
    
plt.plot(x, stats.norm.pdf(x), alpha=0.5, linestyle='dashed', label='normal',
        color='orange', linewidth=3)

plt.xlabel(""Z-score"")
plt.ylabel(""Density"")
plt.legend(loc='best')
pass"
"import pandas as pd
d = pd.DataFrame()
d[""sample_size""] = ssizes
d[""mean_of_means""] = [np.mean(i) for i in smeans]
d"
"hdrfmt = ""{:^7}\t{:^18}""
print(hdrfmt.format(""sample"", ""Mean of Sampling""))
print(hdrfmt.format(""size"",  ""Distn of the Mean""))
print(hdrfmt.format(""=""*7, ""=""*18))

fmt = ""{:>7d}\t{:>18.3f}""
for i in range(len(ssizes)):
    print(fmt.format(ssizes[i], np.mean(smeans[i])))
    "
"for i, size in enumerate(ssizes):
    plt.hist(smeans[i], alpha=0.5, bins=50, normed=True,
             histtype='stepfilled',label = ""n = %d"" % size)

plt.xlabel(""Sample means"")
plt.ylabel(""Density"")
plt.legend(loc='best')
plt.title(""Sampling Distributions of the Mean\nfor samples of size n"")
pass"
"fig, (ax1, ax2) = plt.subplots(1,2)
fig.set_size_inches(12,4)

x = np.linspace(-6, 6, 200)
leftx = np.linspace(-6, -2, 100)

tProbLess2 = stats.t.cdf(-2, df=5)
nProbLess2 = stats.norm.cdf(-2)

ax1.plot(x, stats.t.pdf(x, df=5), alpha=0.5, linewidth=3, label='t({:d})'.format(i))
ax1.fill_between(leftx, stats.t.pdf(leftx, df=5), color='gray', alpha=0.75)
ax1.text(-5.5, 0.3, ""$P(z \leq 2) = {:.3f}$"".format(tProbLess2), fontsize=16)
ax1.set_xlabel(""Z-score"")
ax1.set_ylabel(""Density"")

    
ax2.plot(x, stats.norm.pdf(x), alpha=0.5, label='normal', linewidth=3)
ax2.fill_between(leftx, stats.norm.pdf(leftx), color='gray', alpha=0.75)
ax2.text(-5.5, 0.3, ""$P(z \leq 2) = {:.3f}$"".format(nProbLess2), fontsize=16)
ax2.set_xlabel(""Z-score"")
ax2.set_ylabel(""Density"")

pass"
"idx3 = ssizes.index(3)  # do you know what index does? if not, look it up!
idx10 = ssizes.index(10)
idx30 = ssizes.index(30)

ss = [3,5,30]
idxs = [idx3, idx10, idx30]

fig, axes = plt.subplots(1, 3)
fig.set_size_inches(15,6)

for i in range(len(ss)):
    ax = axes[i]
    ax.hist(sstds[idxs[i]], alpha=0.5, bins=21, normed=True,
            histtype='stepfilled',label = ""n = %d"" % ss[i])
    ax.set_xlabel(""Sample standard deviations"")
    ax.set_ylabel(""Density"")
    ax.legend(loc='best')

fig.suptitle(""Sampling Distributions of the Std Dev\nfor samples of size n"", 
             fontsize=14)
pass"
"mean = [0,0]
cov = [[1,0],
       [0,1]]
ssize = 30
nsims = 2500

cors = []
for n in range(nsims):
    sample = stats.multivariate_normal.rvs(mean=mean, cov=cov, size=ssize)
    r = np.corrcoef(sample, rowvar=False, ddof=1)[0,1]
    cors.append(r)
sbn.distplot(cors)
pass"
"mean = [0,0]
cov = [[1,0.9],
       [0.9,1]]
ssize = 30
nsims = 2500

cors090 = []
for n in range(nsims):
    sample = stats.multivariate_normal.rvs(mean=mean, cov=cov, size=ssize)
    r = np.corrcoef(sample, rowvar=False, ddof=1)[0,1]
    cors090.append(r)
    
sbn.distplot(cors090)
pass"
"# plot for sampling distribution when pho = 0.9
# using Fisher's transformation
sbn.distplot(np.arctanh(cors090))
print("""")
pass"
"ssizes = np.arange(10,250,step=10)
cis = []
for i in ssizes:
    cis.append(correlationCI(0, i)[0])

plt.plot(ssizes, cis, '-o')
plt.xlabel(""Sample size"")
plt.ylabel(""Half-width of CI"")
plt.title(r""""""Half-width of CIs for $\rho=0$
as as function of sample size"""""")


pass"
"n = 50
x = np.linspace(1,10,n) + stats.norm.rvs(size=n)
y = x + stats.norm.rvs(loc=1, scale=1.5, size=n)
plt.scatter(x,y)

print(""Pearson r: "", stats.pearsonr(x, y)[0])
print(""Spearman's rho: "", stats.spearmanr(x, y)[0])
print(""Kendall's tau: "", stats.kendalltau(x, y)[0])

pass"
"pollute_X = np.concatenate([x, stats.norm.rvs(loc=14, size=1), stats.norm.rvs(loc=-1, size=1)])
pollute_Y = np.concatenate([y, stats.norm.rvs(loc=6, size=1), stats.norm.rvs(loc=8, size=1)])

plt.scatter(pollute_X, pollute_Y)

print(""Pearson r: "", stats.pearsonr(pollute_X, pollute_Y)[0])
print(""Spearman's rho: "", stats.spearmanr(pollute_X,pollute_Y)[0])
print(""Kendall's tau: "", stats.kendalltau(pollute_X,pollute_Y)[0])"
"nobs = observed.ix['All','All']

prob_female = observed.ix['All','f']/nobs
prob_male = observed.ix['All', 'm']/nobs

prob_surv = observed.ix['T', 'All']/nobs
prob_died = observed.ix['F', 'All']/nobs

expected_counts = []
for i in (prob_died, prob_surv):
    row = []
    for j in (prob_female, prob_male):
        row.append(i * j * nobs)
    expected_counts.append(row + [np.sum(row)])
expected_counts.append(np.sum(expected_counts,axis=0).tolist())

expected = pd.DataFrame(expected_counts, index=observed.index, columns=observed.columns)

print(""Table of Expected Counts"")
expected"
"Z2 = (observed-expected)**2/expected
Z2"
"chi2 = np.sum(Z2.values)
chi2"
"Chi2, Pval, Dof"
bumpus.survived.unique()
"# generate bivariate normal data for uncorrelated variables
# See the docs on scipy.stats.multivariate_normal

# bivariate mean
mean = [0,0]  

# covariance matrix
cov = np.array([[1,0],
                [0,1]])

sample = stats.multivariate_normal.rvs(mean=mean, cov=cov, size=30) 
"
 
"sbn.jointplot(sample[:,0], sample[:,1])
print(""Correlation matrix:\n"", np.corrcoef(sample, rowvar=False, ddof=1))"
"stats.pearsonr(sample[:,0], sample[:,1])"
"iris.groupby(""Species"").describe()"
"iris.hist()
pass"
"iris.groupby(""Species"").Petal_Length.hist(alpha=0.5)
plt.xlabel(""Petal Length (mm)"")
plt.ylabel(""Frequency"")
plt.title(""Distribution of Petal Length"")
pass"
"iris.groupby(""Species"").Sepal_Length.hist()"
"iris.boxplot()
pass"
"matplotlib.style.use(""ggplot"")
iris.boxplot(""Petal_Length"", by=""Species"")
plt.ylabel(""Petal Length (mm)"")"
iris
"import dis
dis.dis(f1)"
"timeit.timeit(stmt=""f1(old_list)"", setup=""from __main__ import f1; from __main__ import old_list"", number=20000)"
"timeit.timeit(stmt=""f2(old_list)"", setup=""from __main__ import f2; from __main__ import old_list"", number=20000)"
dis.dis(f2)
"timeit.timeit(stmt=""f3(old_list)"", setup=""from __main__ import f3; from __main__ import old_list"", number=20000)"
dis.dis(f3)
poem_counter.most_common(10)
"[(k, v) for (k, v) in poem_counter.items() if v==2]"
"df = df.sort_values(['score'], ascending=False)
df.head(n=10)"
